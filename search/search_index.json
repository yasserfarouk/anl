{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ANL Documentation This repository is the official platform for running ANAC Automated Negotiation Leagues (starting 2024). It will contain a package called anlXXXX for the competition run in year XXXX. For example anl2024 will contain all files related to the 2024's version of the competition. This package is a thin-wrapper around the NegMAS library for automated negotiation. Its main goal is to provide the following functionalities: A set of utility functions to run tournaments in the same settings as in the ANL competition. These functions are always called anl20XX_tournament for year 20XX . A CLI for running tournaments called anl . A place to hold the official implementation of every strategy submitted to the ANL competition after each year. These can be found in the module anl.anl20XX.negotiators for year 20XX . The official website for the ANL competition is: https://scml.cs.brown.edu/anl Installation pip install anl You can also install the in-development version with:: pip install https://github.com/autoneg/anl/archive/master.zip Documentation Documentation for the ANL package: https://yasserfarouk.github.io/anl/ Documentation for the NegMAS library: https://negmas.readthedocs.io CLI After installation, you can try running a tournament using the CLI: anl tournament2024 To find all the parameters you can customize for running tournaments run: anl tournament2024 --help You can run the following command to check the versions of ANL and NegMAS on your machine: anl version You should get at least these versions: anl: 0 .1.5 ( NegMAS: 0 .10.9 ) Other than the two commands mentioned above (tournament2024, version), you can use the CLI to generate and save scenarios which you can later reuse with the tournament2024 command using --scenarios-path. As an example: anl make-scenarios myscenarios --scenarios = 5 anl tournament2024 --scenarios-path = myscenarios --scenarios = 0 The first command will create 5 scenarios and save them under myscenarios . The second command will use these scenarios without generating any new scenarios to run a tournament. Visualizer ANL comes with a simple visualizer that can be used to visualize logs from tournaments after the fact. To start the visualizer type: anlv show This will allow you to select any tournament that is stored in the default location (~/negmas/anl2024/tournaments). You can also show the tournament stored in a specific location 'your-tournament-path' using: anlv show your-tournament-path","title":"Index"},{"location":"#anl-documentation","text":"This repository is the official platform for running ANAC Automated Negotiation Leagues (starting 2024). It will contain a package called anlXXXX for the competition run in year XXXX. For example anl2024 will contain all files related to the 2024's version of the competition. This package is a thin-wrapper around the NegMAS library for automated negotiation. Its main goal is to provide the following functionalities: A set of utility functions to run tournaments in the same settings as in the ANL competition. These functions are always called anl20XX_tournament for year 20XX . A CLI for running tournaments called anl . A place to hold the official implementation of every strategy submitted to the ANL competition after each year. These can be found in the module anl.anl20XX.negotiators for year 20XX . The official website for the ANL competition is: https://scml.cs.brown.edu/anl","title":"ANL Documentation"},{"location":"#installation","text":"pip install anl You can also install the in-development version with:: pip install https://github.com/autoneg/anl/archive/master.zip","title":"Installation"},{"location":"#documentation","text":"Documentation for the ANL package: https://yasserfarouk.github.io/anl/ Documentation for the NegMAS library: https://negmas.readthedocs.io","title":"Documentation"},{"location":"#cli","text":"After installation, you can try running a tournament using the CLI: anl tournament2024 To find all the parameters you can customize for running tournaments run: anl tournament2024 --help You can run the following command to check the versions of ANL and NegMAS on your machine: anl version You should get at least these versions: anl: 0 .1.5 ( NegMAS: 0 .10.9 ) Other than the two commands mentioned above (tournament2024, version), you can use the CLI to generate and save scenarios which you can later reuse with the tournament2024 command using --scenarios-path. As an example: anl make-scenarios myscenarios --scenarios = 5 anl tournament2024 --scenarios-path = myscenarios --scenarios = 0 The first command will create 5 scenarios and save them under myscenarios . The second command will use these scenarios without generating any new scenarios to run a tournament.","title":"CLI"},{"location":"#visualizer","text":"ANL comes with a simple visualizer that can be used to visualize logs from tournaments after the fact. To start the visualizer type: anlv show This will allow you to select any tournament that is stored in the default location (~/negmas/anl2024/tournaments). You can also show the tournament stored in a specific location 'your-tournament-path' using: anlv show your-tournament-path","title":"Visualizer"},{"location":"faq/","text":"FAQ How can I access a data file in my package? When your agent is submitted, it is run in an environment different from that in which the tournament will be run. This means that you cannot use hardcoded paths in your agent. Moreover, you (and we) do not know in advance what will be the current directory when the tournament is run. For this reason, it is required that if you access any files in your agent, you should use a path relative to the file in which the code accessing these files is located. Please note that accessing ANY FILES outside the directory of your agent is prohibited and will lead to immediate disqualification for obvious security reasons. There are no second chances in this one. Let's assume that your file structure is something like that: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 myagent.py \u2502 \u2514\u2500\u2500 otherfiles.py \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 myfile.csv \u2514\u2500\u2500 tests Now you want to access the file myfile.csv when you are inside myagent.py . To do so you can use the following code:: import pathlib path_2_myfile = pathlib.Path(__file__).parent.parent / \"data\" / \"myfile.csv\" Can my agent pass data to my other agents between negotiations? NO Passing data to your agents between negotiations will lead to disqualification. Can my agent read data from the HDD outside my agent's folder? NO Your agent can only read files that you submitted to us in your zip file. It cannot modify these files in anyway during the competition. It cannot read from anywhere else in secondary storage. Trying to do so will lead to disqualification. Can my agent write data to the HDD during the negotiation? NO The agent is not allowed to write anything to the hard disk during the competition. Can I print to the screen to debug my agent? PLEASE DO NOT Printing to the screen in your agent will prevent us from monitoring the progress of tournament runs and will slow down the process. Moreover, it is not useful anyway because the tournaments are run in parallel. If you really need to print something (e.g. for debugging purposes), please remove all print statements before submission. We will never touch your code after submission so we cannot remove them. Can I write arbitrary code in my module besides the negotiator class definition? When python imports your module, it runs everything in it so the top level code should be only one of these: - Class definitions - Function definitions - Variable definitions - Arbitrary code that runs in few milliseconds and prints nothing Any other code must be protected inside:: if __name__ == \"__main__\" For example, if you want to run a simulation to test your agent. DO NOT USE SOMETHING LIKE THIS :: anl2024_tournament(....) But something like this:: def main(): anl2024_tournament(....) if __name__ == \"__main__\": main() This way, importing your module will not run the world simulation. I ran a simulation using \"anl tournament2024\" command. Where are my log files? If you did not pass \"--no-log\", you will find the log files at ~/negmas/anl2024/[date-time-uuid] I implement my agent using multiple files. How should I import them? Assume that you have the following file structure base \u251c\u2500\u2500 subfolder \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py In your agent.py file, you want to import your other files:: import component1 import subfolder.component2 This will not work because in the actual competition component1.py and component2.py will not be in python path. There are two ways to solve it: The clean way is to use relative imports. You will need to turn your agent int a package by adding empty __init__.py files to every folder you want to import from:: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py You can now change your import to:: import .component1 import .subfolder.component2 Notice the extra dot ( . ) Another way that does not require any modification of your file structure is to add the following lines before your imports:: import os, sys sys.path.append(os.path.dirname(__file__)) Note that the later method has the disadvantage of putting your components at the end of the path which means that if you have any classes, functions, etc with a name that is defined in any module that appears earlier in the path, yours will be hidden.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-can-i-access-a-data-file-in-my-package","text":"When your agent is submitted, it is run in an environment different from that in which the tournament will be run. This means that you cannot use hardcoded paths in your agent. Moreover, you (and we) do not know in advance what will be the current directory when the tournament is run. For this reason, it is required that if you access any files in your agent, you should use a path relative to the file in which the code accessing these files is located. Please note that accessing ANY FILES outside the directory of your agent is prohibited and will lead to immediate disqualification for obvious security reasons. There are no second chances in this one. Let's assume that your file structure is something like that: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 myagent.py \u2502 \u2514\u2500\u2500 otherfiles.py \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 myfile.csv \u2514\u2500\u2500 tests Now you want to access the file myfile.csv when you are inside myagent.py . To do so you can use the following code:: import pathlib path_2_myfile = pathlib.Path(__file__).parent.parent / \"data\" / \"myfile.csv\"","title":"How can I access a data file in my package?"},{"location":"faq/#can-my-agent-pass-data-to-my-other-agents-between-negotiations","text":"NO Passing data to your agents between negotiations will lead to disqualification.","title":"Can my agent pass data to my other agents between negotiations?"},{"location":"faq/#can-my-agent-read-data-from-the-hdd-outside-my-agents-folder","text":"NO Your agent can only read files that you submitted to us in your zip file. It cannot modify these files in anyway during the competition. It cannot read from anywhere else in secondary storage. Trying to do so will lead to disqualification.","title":"Can my agent read data from the HDD outside my agent's folder?"},{"location":"faq/#can-my-agent-write-data-to-the-hdd-during-the-negotiation","text":"NO The agent is not allowed to write anything to the hard disk during the competition.","title":"Can my agent write data to the HDD during the negotiation?"},{"location":"faq/#can-i-print-to-the-screen-to-debug-my-agent","text":"PLEASE DO NOT Printing to the screen in your agent will prevent us from monitoring the progress of tournament runs and will slow down the process. Moreover, it is not useful anyway because the tournaments are run in parallel. If you really need to print something (e.g. for debugging purposes), please remove all print statements before submission. We will never touch your code after submission so we cannot remove them.","title":"Can I print to the screen to debug my agent?"},{"location":"faq/#can-i-write-arbitrary-code-in-my-module-besides-the-negotiator-class-definition","text":"When python imports your module, it runs everything in it so the top level code should be only one of these: - Class definitions - Function definitions - Variable definitions - Arbitrary code that runs in few milliseconds and prints nothing Any other code must be protected inside:: if __name__ == \"__main__\" For example, if you want to run a simulation to test your agent. DO NOT USE SOMETHING LIKE THIS :: anl2024_tournament(....) But something like this:: def main(): anl2024_tournament(....) if __name__ == \"__main__\": main() This way, importing your module will not run the world simulation.","title":"Can I write arbitrary code in my module besides the negotiator class definition?"},{"location":"faq/#i-ran-a-simulation-using-anl-tournament2024-command-where-are-my-log-files","text":"If you did not pass \"--no-log\", you will find the log files at ~/negmas/anl2024/[date-time-uuid]","title":"I ran a simulation using \"anl tournament2024\" command. Where are my log files?"},{"location":"faq/#i-implement-my-agent-using-multiple-files-how-should-i-import-them","text":"Assume that you have the following file structure base \u251c\u2500\u2500 subfolder \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py In your agent.py file, you want to import your other files:: import component1 import subfolder.component2 This will not work because in the actual competition component1.py and component2.py will not be in python path. There are two ways to solve it: The clean way is to use relative imports. You will need to turn your agent int a package by adding empty __init__.py files to every folder you want to import from:: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py You can now change your import to:: import .component1 import .subfolder.component2 Notice the extra dot ( . ) Another way that does not require any modification of your file structure is to add the following lines before your imports:: import os, sys sys.path.append(os.path.dirname(__file__)) Note that the later method has the disadvantage of putting your components at the end of the path which means that if you have any classes, functions, etc with a name that is defined in any module that appears earlier in the path, yours will be hidden.","title":"I implement my agent using multiple files. How should I import them?"},{"location":"reference/","text":"ANL Reference This package provides a wrapper around NegMAS functionality to generate and run tournaments a la ANL 2024 competition. You mostly only need to use anl2024_tournament in your code. The other helpers are provided to allow for a finer control over the scenarios used. Example Negotiators The package provides few example negotiators. Of special importance is the MiCRO negotiator which provides a full implementation of a recently proposed behavioral strategy. Other negotiators are just wrappers over negotiators provided by NegMAS. class: Boulware Bases: BoulwareTBNegotiator Time-based boulware negotiation strategy Source code in anl/anl2024/negotiators/builtins/wrappers.py 83 84 85 86 class Boulware ( BoulwareTBNegotiator ): \"\"\" Time-based boulware negotiation strategy \"\"\" class: Linear Bases: LinearTBNegotiator Time-based linear negotiation strategy Source code in anl/anl2024/negotiators/builtins/wrappers.py 69 70 71 72 73 74 class Linear ( LinearTBNegotiator ): \"\"\" Time-based linear negotiation strategy \"\"\" ... class: Conceder Bases: ConcederTBNegotiator Time-based conceder negotiation strategy Source code in anl/anl2024/negotiators/builtins/wrappers.py 77 78 79 80 class Conceder ( ConcederTBNegotiator ): \"\"\" Time-based conceder negotiation strategy \"\"\" class: NaiveTitForTat Bases: NaiveTitForTatNegotiator A simple behavioral strategy that assumes a zero-sum game Source code in anl/anl2024/negotiators/builtins/wrappers.py 12 13 14 15 class NaiveTitForTat ( NaiveTitForTatNegotiator ): \"\"\" A simple behavioral strategy that assumes a zero-sum game \"\"\" class: MiCRO Bases: SAONegotiator A simple implementation of the MiCRO negotiation strategy Remarks This is a simplified implementation of the MiCRO strategy. It is not guaranteed to exactly match the published work. MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. Note that MiCRO works optimally if both negotiators can concede all the way to agreement. If one of them has a high reservation value preventing it from doing so, or if the allowed number of steps is small, MiCRO will not reach agreement (even against itself). Source code in anl/anl2024/negotiators/builtins/micro.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class MiCRO ( SAONegotiator ): \"\"\" A simple implementation of the MiCRO negotiation strategy Remarks: - This is a simplified implementation of the MiCRO strategy. - It is not guaranteed to exactly match the published work. - MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. - Note that MiCRO works optimally if both negotiators can concede all the way to agreement. If one of them has a high reservation value preventing it from doing so, or if the allowed number of steps is small, MiCRO will not reach agreement (even against itself). \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # initialize local variables self . worst_offer_utility : float = float ( \"inf\" ) self . sorter = None self . _received , self . _sent = set (), set () def __call__ ( self , state : SAOState ) -> SAOResponse : # The main implementation of the MiCRO strategy assert self . ufun # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL) if self . sorter is None : # A sorter, sorts a ufun and can be used to get outcomes using their utiility self . sorter = PresortingInverseUtilityFunction ( self . ufun , rational_only = True , eps =- 1 , rel_eps =- 1 ) # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes self . sorter . init () # get the current offer and prepare for rejecting it offer = state . current_offer # If I received something, check for acceptance if offer is not None : self . _received . add ( offer ) # Find out my next offer and the acceptable offer will_concede = len ( self . _sent ) <= len ( self . _received ) # My next offer is either a conceding outcome if will_concede or sampled randomly from my past offers next_offer = ( self . sample_sent () if not will_concede else self . sorter . next_worse () ) # If I exhausted all my rational offers, do not concede if next_offer is None : will_concede , next_offer = False , self . sample_sent () else : next_utility = float ( self . ufun ( next_offer )) if next_utility < self . ufun . reserved_value : will_concede , next_offer = False , self . sample_sent () next_utility = float ( self . ufun ( next_offer )) # Find my acceptable outcome. It will None if I did not offer anything yet. acceptable_utility = ( self . worst_offer_utility if not will_concede else next_utility ) # The Acceptance Policy of MiCRO # accept if the offer is not worse than my acceptable offer if I am conceding or the best so far if I am not offer_utility = float ( self . ufun ( offer )) if ( offer is not None and offer_utility >= acceptable_utility and offer_utility >= self . ufun . reserved_value ): return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # If I cannot find any offers, I know that there are NO rational outcomes in this negotiation for me and will end it. if next_offer is None : return SAOResponse ( ResponseType . END_NEGOTIATION , None ) # Offer my next-offer and record it self . _sent . add ( next_offer ) self . worst_offer_utility = next_utility return SAOResponse ( ResponseType . REJECT_OFFER , next_offer ) def sample_sent ( self ) -> Outcome | None : # Get an outcome from the set I sent so far (or my best if I sent nothing) if not len ( self . _sent ): return None return random . choice ( list ( self . _sent )) class: NashSeeker Bases: SAONegotiator Assumes that the opponent has a fixed reserved value and seeks the Nash equilibrium. Parameters: opponent_reserved_value ( float , default: 0.25 ) \u2013 Assumed reserved value for the opponent nash_factor \u2013 Fraction (or multiple) of the agent utility at the Nash Point (assuming the opponent_reserved_value ) that is acceptable Source code in anl/anl2024/negotiators/builtins/nash_seeker.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class NashSeeker ( SAONegotiator ): \"\"\"Assumes that the opponent has a fixed reserved value and seeks the Nash equilibrium. Args: opponent_reserved_value: Assumed reserved value for the opponent nash_factor: Fraction (or multiple) of the agent utility at the Nash Point (assuming the `opponent_reserved_value`) that is acceptable \"\"\" def __init__ ( self , * args , opponent_reserved_value : float = 0.25 , nash_factor = 0.9 , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _opponent_r = opponent_reserved_value self . _outcomes : list [ Outcome ] = [] self . _min_acceptable = float ( \"inf\" ) self . _nash_factor = nash_factor self . _best : Outcome = None # type: ignore def on_preferences_changed ( self , changes ): _ = changes # silenting a typing warning # This callback is called at the start of the negotiation after the ufun is set assert self . ufun is not None and self . ufun . outcome_space is not None # save my best outcome for later use self . _best = self . ufun . best () # check that I have access to the opponent ufun assert self . opponent_ufun is not None # set the reserved value of the opponent self . opponent_ufun . reserved_value = self . _opponent_r # consider my and my parther's ufuns ufuns = ( self . ufun , self . opponent_ufun ) # list all outcomes outcomes = list ( self . ufun . outcome_space . enumerate_or_sample ()) # find the pareto-front and the nash point frontier_utils , frontier_indices = pareto_frontier ( ufuns , outcomes ) frontier_outcomes = [ outcomes [ _ ] for _ in frontier_indices ] my_frontier_utils = [ _ [ 0 ] for _ in frontier_utils ] nash = nash_points ( ufuns , frontier_utils ) # type: ignore if nash : # find my utility at the Nash Bargaining Solution. my_nash_utility = nash [ 0 ][ 0 ][ 0 ] else : my_nash_utility = 0.5 * ( float ( self . ufun . max ()) + self . ufun . reserved_value ) # Set the acceptable utility limit self . _min_acceptable = my_nash_utility * self . _nash_factor # Set the set of outcomes to offer from self . _outcomes = [ w for u , w in zip ( my_frontier_utils , frontier_outcomes ) if u >= self . _min_acceptable ] def __call__ ( self , state : SAOState ) -> SAOResponse : # just assert that I have a ufun and I know the outcome space. assert self . ufun is not None and self . ufun . outcome_space is not None # read the current offer from the state. None means I am starting the negotiation offer = state . current_offer # Accept the offer if its utility is higher than my utility at the Nash Bargaining Solution with the assumed opponent ufun if offer and float ( self . ufun ( offer )) >= self . _min_acceptable : return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # If I could not find the Nash Bargaining Solution, just offer my best outcome forever. if not self . _outcomes : return SAOResponse ( ResponseType . REJECT_OFFER , self . _best ) # Offer some outcome with high utility relative to the Nash Bargaining Solution return SAOResponse ( ResponseType . REJECT_OFFER , random . choice ( self . _outcomes )) class: RVFitter Bases: SAONegotiator A simple negotiator that uses curve fitting to learn the reserved value. Parameters: min_unique_utilities ( int , default: 10 ) \u2013 Number of different offers from the opponent before starting to attempt learning their reserved value. e ( float , default: 5.0 ) \u2013 The concession exponent used for the agent's offering strategy stochasticity ( float , default: 0.1 ) \u2013 The level of stochasticity in the offers. enable_logging ( bool , default: False ) \u2013 If given, a log will be stored for the estimates. Remarks: - Assumes that the opponent is using a time-based offering strategy that offers the outcome at utility $u(t) = (u_0 - r) - r \\exp(t^e)$ where $u_0$ is the utility of the first offer (directly read from the opponent ufun), $e$ is an exponent that controls the concession rate and $r$ is the reserved value we want to learn. - After it receives offers with enough different utilities, it starts finding the optimal values for $e$ and $r$. - When it is time to respond, RVFitter, calculates the set of rational outcomes **for both agents** based on its knowledge of the opponent ufun (given) and reserved value (learned). It then applies the same concession curve defined above to concede over an ordered list of these outcomes. - Is this better than using the same concession curve on the outcome space without even trying to learn the opponent reserved value? Maybe sometimes but empirical evaluation shows that it is not in general. - Note that the way we check for availability of enough data for training is based on the uniqueness of the utility of offers from the opponent (for the opponent). Given that these are real values, this approach is suspect because of rounding errors. If two outcomes have the same utility they may appear to have different but very close utilities because or rounding errors (or genuine very small differences). Such differences should be ignored. - Note also that we start assuming that the opponent reserved value is 0.0 which means that we are only restricted with our own reserved values when calculating the rational outcomes. This is the best case scenario for us because we have MORE negotiation power when the partner has LOWER utility. Source code in anl/anl2024/negotiators/builtins/rv_fitter.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 class RVFitter ( SAONegotiator ): \"\"\"A simple negotiator that uses curve fitting to learn the reserved value. Args: min_unique_utilities: Number of different offers from the opponent before starting to attempt learning their reserved value. e: The concession exponent used for the agent's offering strategy stochasticity: The level of stochasticity in the offers. enable_logging: If given, a log will be stored for the estimates. Remarks: - Assumes that the opponent is using a time-based offering strategy that offers the outcome at utility $u(t) = (u_0 - r) - r \\\\exp(t^e)$ where $u_0$ is the utility of the first offer (directly read from the opponent ufun), $e$ is an exponent that controls the concession rate and $r$ is the reserved value we want to learn. - After it receives offers with enough different utilities, it starts finding the optimal values for $e$ and $r$. - When it is time to respond, RVFitter, calculates the set of rational outcomes **for both agents** based on its knowledge of the opponent ufun (given) and reserved value (learned). It then applies the same concession curve defined above to concede over an ordered list of these outcomes. - Is this better than using the same concession curve on the outcome space without even trying to learn the opponent reserved value? Maybe sometimes but empirical evaluation shows that it is not in general. - Note that the way we check for availability of enough data for training is based on the uniqueness of the utility of offers from the opponent (for the opponent). Given that these are real values, this approach is suspect because of rounding errors. If two outcomes have the same utility they may appear to have different but very close utilities because or rounding errors (or genuine very small differences). Such differences should be ignored. - Note also that we start assuming that the opponent reserved value is 0.0 which means that we are only restricted with our own reserved values when calculating the rational outcomes. This is the best case scenario for us because we have MORE negotiation power when the partner has LOWER utility. \"\"\" def __init__ ( self , * args , min_unique_utilities : int = 10 , e : float = 5.0 , stochasticity : float = 0.1 , enable_logging : bool = False , ** kwargs , ): super () . __init__ ( * args , ** kwargs ) self . min_unique_utilities = min_unique_utilities self . e = e self . stochasticity = stochasticity # keeps track of times at which the opponent offers self . opponent_times : list [ float ] = [] # keeps track of opponent utilities of its offers self . opponent_utilities : list [ float ] = [] # keeps track of the our last estimate of the opponent reserved value self . _past_oppnent_rv = 0.0 # keeps track of the rational outcome set given our estimate of the # opponent reserved value and our knowledge of ours self . _rational : list [ tuple [ float , float , Outcome ]] = [] self . _enable_logging = enable_logging def __call__ ( self , state : SAOState ) -> SAOResponse : assert self . ufun and self . opponent_ufun # update the opponent reserved value in self.opponent_ufun self . update_reserved_value ( state ) # rune the acceptance strategy and if the offer received is acceptable, accept it if self . is_acceptable ( state ): return SAOResponse ( ResponseType . ACCEPT_OFFER , state . current_offer ) # The offering strategy # We only update our estimate of the rational list of outcomes if it is not set or # there is a change in estimated reserved value if ( not self . _rational or abs ( self . opponent_ufun . reserved_value - self . _past_oppnent_rv ) > 1e-3 ): # The rational set of outcomes sorted dependingly according to our utility function # and the opponent utility function (in that order). self . _rational = sorted ( [ ( my_util , opp_util , _ ) for _ in self . nmi . outcome_space . enumerate_or_sample ( levels = 10 , max_cardinality = 100_000 ) if ( my_util := float ( self . ufun ( _ ))) > self . ufun . reserved_value and ( opp_util := float ( self . opponent_ufun ( _ ))) > self . opponent_ufun . reserved_value ], ) # If there are no rational outcomes (i.e. our estimate of the opponent rv is very wrogn), # then just revert to offering our top offer if not self . _rational : return SAOResponse ( ResponseType . REJECT_OFFER , self . ufun . best ()) # find our aspiration level (value between 0 and 1) the higher the higher utility we require asp = aspiration_function ( state . relative_time , 1.0 , 0.0 , self . e ) # find the index of the rational outcome at the aspiration level (in the rational set of outcomes) n_rational = len ( self . _rational ) max_rational = n_rational - 1 min_indx = max ( 0 , min ( max_rational , int ( asp * max_rational ))) # find current stochasticity which goes down from the set level to zero linearly s = aspiration_function ( state . relative_time , self . stochasticity , 0.0 , 1.0 ) # find the index of the maximum utility we require based on stochasticity (going down over time) max_indx = max ( 0 , min ( int ( min_indx + s * n_rational ), max_rational )) # offer an outcome in the selected range indx = random . randint ( min_indx , max_indx ) if min_indx != max_indx else min_indx outcome = self . _rational [ indx ][ - 1 ] return SAOResponse ( ResponseType . REJECT_OFFER , outcome ) def is_acceptable ( self , state : SAOState ) -> bool : # The acceptance strategy assert self . ufun and self . opponent_ufun # get the offer from the mechanism state offer = state . current_offer # If there is no offer, there is nothing to accept if offer is None : return False # Find the current aspiration level asp = aspiration_function ( state . relative_time , 1.0 , self . ufun . reserved_value , self . e ) # accept if the utility of the received offer is higher than # the current aspiration return float ( self . ufun ( offer )) >= asp def update_reserved_value ( self , state : SAOState ): # Learns the reserved value of the partner assert self . opponent_ufun is not None # extract the current offer from the state offer = state . current_offer if offer is None : return # save to the list of utilities received from the opponent and their times self . opponent_utilities . append ( float ( self . opponent_ufun ( offer ))) self . opponent_times . append ( state . relative_time ) # If we do not have enough data, just assume that the opponent # reserved value is zero n_unique = len ( set ( self . opponent_utilities )) if n_unique < self . min_unique_utilities : self . _past_oppnent_rv = 0.0 self . opponent_ufun . reserved_value = 0.0 return # Use curve fitting to estimate the opponent reserved value # We assume the following: # - The opponent is using a concession strategy with an exponent between 0.2, 5.0 # - The opponent never offers outcomes lower than their reserved value which means # that their rv must be no higher than the worst outcome they offered for themselves. bounds = (( 0.2 , 0.0 ), ( 5.0 , min ( self . opponent_utilities ))) err = \"\" try : optimal_vals , _ = curve_fit ( lambda x , e , rv : aspiration_function ( x , self . opponent_utilities [ 0 ], rv , e ), self . opponent_times , self . opponent_utilities , bounds = bounds , ) self . _past_oppnent_rv = self . opponent_ufun . reserved_value self . opponent_ufun . reserved_value = optimal_vals [ 1 ] except Exception as e : err , optimal_vals = f \" { str ( e ) } \" , [ None , None ] # log my estimate if self . _enable_logging : self . nmi . log_info ( self . id , dict ( estimated_rv = self . opponent_ufun . reserved_value , n_unique = n_unique , opponent_utility = self . opponent_utilities [ - 1 ], estimated_exponent = optimal_vals [ 0 ], estimated_max = self . opponent_utilities [ 0 ], error = err , ), ) Tournaments function: anl2024_tournament Runs an ANL 2024 tournament Parameters: scenarios ( tuple [ Scenario , ...] | list [ Scenario ] , default: tuple () ) \u2013 A list of predefined scenarios to use for the tournament n_scenarios ( int , default: DEFAULT2024SETTINGS ['n_scenarios'] ) \u2013 Number of negotiation scenarios to generate specifically for this tournament n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: DEFAULT2024SETTINGS ['n_outcomes'] ) \u2013 Number of outcomes (or a min/max tuple of n. outcomes) for each scenario competitors ( tuple [ type [ Negotiator ] | str , ...] | list [ type [ Negotiator ] | str ] , default: DEFAULT_AN2024_COMPETITORS ) \u2013 list of competitor agents competitor_params ( Sequence [ dict | None] | None , default: None ) \u2013 If given, parameters to construct each competitor rotate_ufuns ( bool , default: DEFAULT2024SETTINGS ['rotate_ufuns'] ) \u2013 If given, each scenario will be tried with both orders of the ufuns. n_repetitions ( int , default: DEFAULT2024SETTINGS ['n_repetitions'] ) \u2013 Number of times to repeat each negotiation n_steps ( int | tuple [ int , int ] | None , default: DEFAULT2024SETTINGS ['n_steps'] ) \u2013 Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) time_limit ( float | tuple [ float , float ] | None , default: DEFAULT2024SETTINGS ['time_limit'] ) \u2013 Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) pend ( float | tuple [ float , float ] , default: DEFAULT2024SETTINGS ['pend'] ) \u2013 Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) pend_per_second ( float | tuple [ float , float ] , default: DEFAULT2024SETTINGS ['pend_per_second'] ) \u2013 Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) step_time_limit ( float | tuple [ float , float ] | None , default: DEFAULT2024SETTINGS ['step_time_limit'] ) \u2013 Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) negotiator_time_limit ( float | tuple [ float , float ] | None , default: DEFAULT2024SETTINGS ['negotiator_time_limit'] ) \u2013 Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) name ( str | None , default: None ) \u2013 Name of the tournament nologs ( bool , default: False ) \u2013 If given, no logs will be saved njobs ( int , default: 0 ) \u2013 Number of parallel jobs to use. -1 for serial and 0 for all cores plot_fraction ( float , default: 0.2 ) \u2013 Fraction of negotiations to plot. Only used if not nologs verbosity ( int , default: 1 ) \u2013 Verbosity level. The higher the more verbose self_play ( bool , default: DEFAULT2024SETTINGS ['self_play'] ) \u2013 Allow negotiators to run against themselves. randomize_runs ( bool , default: DEFAULT2024SETTINGS ['randomize_runs'] ) \u2013 Randomize the order of negotiations save_every ( int , default: 0 ) \u2013 Save logs every this number of negotiations save_stats ( bool , default: True ) \u2013 Save statistics for scenarios known_partner ( bool , default: DEFAULT2024SETTINGS ['known_partner'] ) \u2013 Allow negotiators to know the type of their partner (through their ID) final_score ( tuple [ str , str ] , default: DEFAULT2024SETTINGS ['final_score'] ) \u2013 The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max base_path ( Path | None , default: None ) \u2013 Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments scenario_generator ( str | ScenarioGenerator , default: DEFAULT2024SETTINGS ['scenario_generator'] ) \u2013 An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. generator_params ( dict [ str , Any ] | None , default: DEFAULT2024SETTINGS ['generator_params'] ) \u2013 Parameters passed to the scenario generator plot_params ( dict [ str , Any ] | None , default: None ) \u2013 If given, overrides plotting parameters. See nemgas.sao.SAOMechanism.plot() for all parameters Returns: SimpleTournamentResults \u2013 Tournament results as a SimpleTournamentResults object. Source code in anl/anl2024/runner.py 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 def anl2024_tournament ( scenarios : tuple [ Scenario , ... ] | list [ Scenario ] = tuple (), n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore competitors : tuple [ type [ Negotiator ] | str , ... ] | list [ type [ Negotiator ] | str ] = DEFAULT_AN2024_COMPETITORS , rotate_ufuns : bool = DEFAULT2024SETTINGS [ \"rotate_ufuns\" ], # type: ignore n_repetitions : int = DEFAULT2024SETTINGS [ \"n_repetitions\" ], # type: ignore n_steps : int | tuple [ int , int ] | None = DEFAULT2024SETTINGS [ \"n_steps\" ], # type: ignore time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"time_limit\" ], # type: ignore pend : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend\" ], # type: ignore pend_per_second : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend_per_second\" ], # type: ignore step_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"step_time_limit\" ], # type: ignore negotiator_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"negotiator_time_limit\" ], # type: ignore self_play : bool = DEFAULT2024SETTINGS [ \"self_play\" ], # type: ignore randomize_runs : bool = DEFAULT2024SETTINGS [ \"randomize_runs\" ], # type: ignore known_partner : bool = DEFAULT2024SETTINGS [ \"known_partner\" ], # type: ignore final_score : tuple [ str , str ] = DEFAULT2024SETTINGS [ \"final_score\" ], # type: ignore scenario_generator : str | ScenarioGenerator = DEFAULT2024SETTINGS [ \"scenario_generator\" ], # type: ignore generator_params : dict [ str , Any ] | None = DEFAULT2024SETTINGS [ \"generator_params\" ], # type: ignore competitor_params : Sequence [ dict | None ] | None = None , name : str | None = None , nologs : bool = False , njobs : int = 0 , plot_fraction : float = 0.2 , verbosity : int = 1 , save_every : int = 0 , save_stats : bool = True , base_path : Path | None = None , plot_params : dict [ str , Any ] | None = None , raise_exceptions : bool = True , ) -> SimpleTournamentResults : \"\"\"Runs an ANL 2024 tournament Args: scenarios: A list of predefined scenarios to use for the tournament n_scenarios: Number of negotiation scenarios to generate specifically for this tournament n_outcomes: Number of outcomes (or a min/max tuple of n. outcomes) for each scenario competitors: list of competitor agents competitor_params: If given, parameters to construct each competitor rotate_ufuns: If given, each scenario will be tried with both orders of the ufuns. n_repetitions: Number of times to repeat each negotiation n_steps: Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) time_limit: Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) pend: Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) pend_per_second: Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) step_time_limit: Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) negotiator_time_limit: Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) name: Name of the tournament nologs: If given, no logs will be saved njobs: Number of parallel jobs to use. -1 for serial and 0 for all cores plot_fraction: Fraction of negotiations to plot. Only used if not nologs verbosity: Verbosity level. The higher the more verbose self_play: Allow negotiators to run against themselves. randomize_runs: Randomize the order of negotiations save_every: Save logs every this number of negotiations save_stats: Save statistics for scenarios known_partner: Allow negotiators to know the type of their partner (through their ID) final_score: The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max base_path: Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments scenario_generator: An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. generator_params: Parameters passed to the scenario generator plot_params: If given, overrides plotting parameters. See `nemgas.sao.SAOMechanism.plot()` for all parameters Returns: Tournament results as a `SimpleTournamentResults` object. \"\"\" if generator_params is None : generator_params = dict () if isinstance ( scenario_generator , str ): scenario_generator = GENERAROR_MAP [ scenario_generator ] all_outcomes = not scenario_generator == zerosum_pie_scenarios if nologs : path = None elif base_path is not None : path = Path ( base_path ) / ( name if name else unique_name ( \"anl\" )) else : path = DEFAULT_TOURNAMENT_PATH / ( name if name else unique_name ( \"anl\" )) params = dict ( ylimits = ( 0 , 1 ), mark_offers_view = True , mark_pareto_points = all_outcomes , mark_all_outcomes = all_outcomes , mark_nash_points = True , mark_kalai_points = all_outcomes , mark_max_welfare_points = all_outcomes , show_agreement = True , show_pareto_distance = False , show_nash_distance = True , show_kalai_distance = False , show_max_welfare_distance = False , show_max_relative_welfare_distance = False , show_end_reason = True , show_annotations = not all_outcomes , show_reserved = True , show_total_time = True , show_relative_time = True , show_n_steps = True , ) if plot_params : params = params . update ( plot_params ) scenarios = list ( scenarios ) + list ( scenario_generator ( n_scenarios , n_outcomes , ** generator_params ) ) private_infos = [ tuple ( dict ( opponent_ufun = U ( values = _ . values , weights = _ . weights , bias = _ . _bias , reserved_value = 0 , outcome_space = _ . outcome_space )) # type: ignore for _ in s . ufuns [:: - 1 ] ) for s in scenarios ] return cartesian_tournament ( competitors = tuple ( competitors ), scenarios = scenarios , competitor_params = competitor_params , private_infos = private_infos , # type: ignore rotate_ufuns = rotate_ufuns , n_repetitions = n_repetitions , path = path , njobs = njobs , mechanism_type = SAOMechanism , n_steps = n_steps , time_limit = time_limit , pend = pend , pend_per_second = pend_per_second , step_time_limit = step_time_limit , negotiator_time_limit = negotiator_time_limit , mechanism_params = None , plot_fraction = plot_fraction , verbosity = verbosity , self_play = self_play , randomize_runs = randomize_runs , save_every = save_every , save_stats = save_stats , final_score = final_score , id_reveals_type = known_partner , name_reveals_type = True , plot_params = params , raise_exceptions = raise_exceptions , ) constant: DEFAULT_AN2024_COMPETITORS Default set of negotiators (agents) used as competitors constant: DEFAULT_TOURNAMENT_PATH Default location to store tournament logs constant: DEFAULT2024SETTINGS Default settings for ANL 2024 Helpers (Scenario Generation) type: ScenarioGenerator Type of callable that can be used for generating scenarios. It must receive the number of scenarios and number of outcomes (as int, tuple or list) and return a list of Scenario s function: mixed_scenarios Generates a mix of zero-sum, monotonic and general scenarios Parameters: n_scenarios ( int , default: DEFAULT2024SETTINGS ['n_scenarios'] ) \u2013 Number of scenarios to genearate n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: DEFAULT2024SETTINGS ['n_outcomes'] ) \u2013 Number of outcomes (or a list of range thereof). reserved_ranges ( ReservedRanges , default: DEFAULT2024SETTINGS ['reserved_ranges'] ) \u2013 the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome log_uniform ( bool , default: DEFAULT2024SETTINGS ['outcomes_log_uniform'] ) \u2013 Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. zerosum_fraction ( float , default: DEFAULT2024SETTINGS ['generator_params']['zerosum_fraction'] ) \u2013 Fraction of zero-sum scenarios. These are original DivideThePie scenarios monotonic_fraction ( float , default: DEFAULT2024SETTINGS ['generator_params']['monotonic_fraction'] ) \u2013 Fraction of scenarios where each ufun is a monotonic function of the received pie. curve_fraction ( float , default: DEFAULT2024SETTINGS ['generator_params']['curve_fraction'] ) \u2013 Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. pareto_first ( bool , default: DEFAULT2024SETTINGS ['generator_params']['pareto_first'] ) \u2013 If given, the Pareto frontier will always be in the first set of outcomes n_ufuns ( int , default: DEFAULT2024SETTINGS ['n_ufuns'] ) \u2013 Number of ufuns to generate per scenario n_pareto ( int | float | tuple [ float | int , float | int ] | list [ int | float ] , default: DEFAULT2024SETTINGS ['generator_params']['n_pareto'] ) \u2013 Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. pareto_log_uniform ( bool , default: False ) \u2013 Use log-uniform instead of uniform sampling if n_pareto is a tuple n_trials \u2013 Number of times to retry generating each scenario if failures occures Returns: list [ Scenario ] \u2013 A list Scenario s Source code in anl/anl2024/runner.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def mixed_scenarios ( n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore * , reserved_ranges : ReservedRanges = DEFAULT2024SETTINGS [ \"reserved_ranges\" ], # type: ignore log_uniform : bool = DEFAULT2024SETTINGS [ \"outcomes_log_uniform\" ], # type: ignore zerosum_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"zerosum_fraction\" ], # type: ignore monotonic_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"monotonic_fraction\" ], # type: ignore curve_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"curve_fraction\" ], # type: ignore pareto_first : bool = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"pareto_first\" ], # type: ignore n_ufuns : int = DEFAULT2024SETTINGS [ \"n_ufuns\" ], # type: ignore n_pareto : int | float | tuple [ float | int , float | int ] | list [ int | float ] = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"n_pareto\" ], # type: ignore pareto_log_uniform : bool = False , n_trials = 10 , ) -> list [ Scenario ]: \"\"\"Generates a mix of zero-sum, monotonic and general scenarios Args: n_scenarios: Number of scenarios to genearate n_outcomes: Number of outcomes (or a list of range thereof). reserved_ranges: the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome log_uniform: Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. zerosum_fraction: Fraction of zero-sum scenarios. These are original DivideThePie scenarios monotonic_fraction: Fraction of scenarios where each ufun is a monotonic function of the received pie. curve_fraction: Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. pareto_first: If given, the Pareto frontier will always be in the first set of outcomes n_ufuns: Number of ufuns to generate per scenario n_pareto: Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. pareto_log_uniform: Use log-uniform instead of uniform sampling if n_pareto is a tuple n_trials: Number of times to retry generating each scenario if failures occures Returns: A list `Scenario` s \"\"\" assert zerosum_fraction + monotonic_fraction <= 1.0 nongeneral_fraction = zerosum_fraction + monotonic_fraction ufun_sets = [] for i in range ( n_scenarios ): r = random . random () n = intin ( n_outcomes , log_uniform ) name = \"S\" if r < nongeneral_fraction : n_pareto_selected = n name = \"DivideThePieGen\" else : if isinstance ( n_pareto , Iterable ): n_pareto = type ( n_pareto )( int ( _ * n + 0.5 ) if _ < 1 else int ( _ ) for _ in n_pareto # type: ignore ) else : n_pareto = int ( 0.5 + n_pareto * n ) if n_pareto < 1 else int ( n_pareto ) n_pareto_selected = intin ( n_pareto , log_uniform = pareto_log_uniform ) # type: ignore for _ in range ( n_trials ): try : if r < zerosum_fraction : vals = generate_utility_values ( n_pareto = n_pareto_selected , n_outcomes = n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"zero_sum\" , ) name = \"DivideThePie\" else : if n_pareto_selected < 2 : n_pareto_selected = 2 vals = generate_utility_values ( n_pareto = n_pareto_selected , n_outcomes = n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"curve\" if random . random () < curve_fraction else \"piecewise_linear\" , ) break except : continue else : continue issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( vals [ i ][ k ]) for i , _ in enumerate ( issues [ 0 ] . all )} ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \" { name }{ i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ] function: pie_scenarios Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Parameters: n_scenarios ( int , default: 20 ) \u2013 Number of scenarios to create n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: 100 ) \u2013 Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). reserved_ranges ( ReservedRanges , default: ((0.0, 0.999999), (0.0, 0.999999)) ) \u2013 Ranges of reserved values for first and second negotiators log_uniform ( bool , default: True ) \u2013 If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). monotonic \u2013 If true all ufuns are monotonically increasing in the portion of the pie Remarks When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. Source code in anl/anl2024/runner.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , monotonic = False , ) -> list [ Scenario ]: \"\"\"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). monotonic: If true all ufuns are monotonically increasing in the portion of the pie Remarks: - When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. \"\"\" ufun_sets = [] base_name = \"DivideTyePie\" if monotonic else \"S\" def normalize ( x ): mn , mx = x . min (), x . max () return (( x - mn ) / ( mx - mn )) . tolist () def make_monotonic ( x , i ): x = np . sort ( np . asarray ( x ), axis = None ) if i : x = x [:: - 1 ] r = random . random () if r < 0.33 : x = np . exp ( x ) elif r < 0.67 : x = np . log ( x ) else : pass return normalize ( x ) max_jitter_level = 0.8 for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ( [ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" if not monotonic else \"i1\" , ), ) # funs = [ # dict( # zip( # issues[0].all, # # adjust(np.asarray([random.random() for _ in range(n)])), # generate(n, i), # ) # ) # for i in range(2) # ] os = make_os ( issues , name = f \" { base_name }{ i } \" ) outcomes = list ( os . enumerate_or_sample ()) ufuns = U . generate_bilateral ( outcomes , conflict_level = 0.5 + 0.5 * random . random (), conflict_delta = random . random (), ) jitter_level = random . random () * max_jitter_level funs = [ np . asarray ([ float ( u ( _ )) for _ in outcomes ]) + np . random . random () * jitter_level for u in ufuns ] if monotonic : funs = [ make_monotonic ( x , i ) for i , x in enumerate ( funs )] else : funs = [ normalize ( x ) for x in funs ] ufuns = tuple ( U ( values = ( TableFun ( dict ( zip ( issues [ 0 ] . all , vals ))),), name = f \" { uname }{ i } \" , outcome_space = os , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), ) for ( uname , vals ) in zip (( \"First\" , \"Second\" ), funs ) # for (uname, r, vals) in zip((\"First\", \"Second\"), reserved_ranges, funs) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ] function: arbitrary_pie_scenarios Source code in anl/anl2024/runner.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def arbitrary_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = False , ) function: monotonic_pie_scenarios Source code in anl/anl2024/runner.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def monotonic_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = True , ) function: zerosum_pie_scenarios Creates scenarios all of the DivideThePie variety with proportions giving utility Parameters: n_scenarios ( int , default: 20 ) \u2013 Number of scenarios to create n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: 100 ) \u2013 Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). reserved_ranges ( ReservedRanges , default: ((0.0, 0.499999), (0.0, 0.499999)) ) \u2013 Ranges of reserved values for first and second negotiators log_uniform ( bool , default: True ) \u2013 If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). Remarks When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently Source code in anl/anl2024/runner.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def zerosum_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.499999 ), ( 0.0 , 0.499999 )), log_uniform : bool = True , ) -> list [ Scenario ]: \"\"\"Creates scenarios all of the DivideThePie variety with proportions giving utility Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). Remarks: - When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently \"\"\" ufun_sets = [] for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( int ( str ( _ ) . split ( \"_\" )[ k ]) / ( n - 1 )) for _ in issues [ 0 ] . all } ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \"DivideTyePie { i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , pareto = tuple ( tuple ( u ( _ ) for u in ufuns ) for _ in make_os ( issues ) . enumerate_or_sample () ), reserved_ranges = reserved_ranges , ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"Reference"},{"location":"reference/#anl-reference","text":"This package provides a wrapper around NegMAS functionality to generate and run tournaments a la ANL 2024 competition. You mostly only need to use anl2024_tournament in your code. The other helpers are provided to allow for a finer control over the scenarios used.","title":"ANL Reference"},{"location":"reference/#example-negotiators","text":"The package provides few example negotiators. Of special importance is the MiCRO negotiator which provides a full implementation of a recently proposed behavioral strategy. Other negotiators are just wrappers over negotiators provided by NegMAS.","title":"Example Negotiators"},{"location":"reference/#class-boulware","text":"Bases: BoulwareTBNegotiator Time-based boulware negotiation strategy Source code in anl/anl2024/negotiators/builtins/wrappers.py 83 84 85 86 class Boulware ( BoulwareTBNegotiator ): \"\"\" Time-based boulware negotiation strategy \"\"\"","title":"class: Boulware"},{"location":"reference/#class-linear","text":"Bases: LinearTBNegotiator Time-based linear negotiation strategy Source code in anl/anl2024/negotiators/builtins/wrappers.py 69 70 71 72 73 74 class Linear ( LinearTBNegotiator ): \"\"\" Time-based linear negotiation strategy \"\"\" ...","title":"class: Linear"},{"location":"reference/#class-conceder","text":"Bases: ConcederTBNegotiator Time-based conceder negotiation strategy Source code in anl/anl2024/negotiators/builtins/wrappers.py 77 78 79 80 class Conceder ( ConcederTBNegotiator ): \"\"\" Time-based conceder negotiation strategy \"\"\"","title":"class: Conceder"},{"location":"reference/#class-naivetitfortat","text":"Bases: NaiveTitForTatNegotiator A simple behavioral strategy that assumes a zero-sum game Source code in anl/anl2024/negotiators/builtins/wrappers.py 12 13 14 15 class NaiveTitForTat ( NaiveTitForTatNegotiator ): \"\"\" A simple behavioral strategy that assumes a zero-sum game \"\"\"","title":"class: NaiveTitForTat"},{"location":"reference/#class-micro","text":"Bases: SAONegotiator A simple implementation of the MiCRO negotiation strategy Remarks This is a simplified implementation of the MiCRO strategy. It is not guaranteed to exactly match the published work. MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. Note that MiCRO works optimally if both negotiators can concede all the way to agreement. If one of them has a high reservation value preventing it from doing so, or if the allowed number of steps is small, MiCRO will not reach agreement (even against itself). Source code in anl/anl2024/negotiators/builtins/micro.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class MiCRO ( SAONegotiator ): \"\"\" A simple implementation of the MiCRO negotiation strategy Remarks: - This is a simplified implementation of the MiCRO strategy. - It is not guaranteed to exactly match the published work. - MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. - Note that MiCRO works optimally if both negotiators can concede all the way to agreement. If one of them has a high reservation value preventing it from doing so, or if the allowed number of steps is small, MiCRO will not reach agreement (even against itself). \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # initialize local variables self . worst_offer_utility : float = float ( \"inf\" ) self . sorter = None self . _received , self . _sent = set (), set () def __call__ ( self , state : SAOState ) -> SAOResponse : # The main implementation of the MiCRO strategy assert self . ufun # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL) if self . sorter is None : # A sorter, sorts a ufun and can be used to get outcomes using their utiility self . sorter = PresortingInverseUtilityFunction ( self . ufun , rational_only = True , eps =- 1 , rel_eps =- 1 ) # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes self . sorter . init () # get the current offer and prepare for rejecting it offer = state . current_offer # If I received something, check for acceptance if offer is not None : self . _received . add ( offer ) # Find out my next offer and the acceptable offer will_concede = len ( self . _sent ) <= len ( self . _received ) # My next offer is either a conceding outcome if will_concede or sampled randomly from my past offers next_offer = ( self . sample_sent () if not will_concede else self . sorter . next_worse () ) # If I exhausted all my rational offers, do not concede if next_offer is None : will_concede , next_offer = False , self . sample_sent () else : next_utility = float ( self . ufun ( next_offer )) if next_utility < self . ufun . reserved_value : will_concede , next_offer = False , self . sample_sent () next_utility = float ( self . ufun ( next_offer )) # Find my acceptable outcome. It will None if I did not offer anything yet. acceptable_utility = ( self . worst_offer_utility if not will_concede else next_utility ) # The Acceptance Policy of MiCRO # accept if the offer is not worse than my acceptable offer if I am conceding or the best so far if I am not offer_utility = float ( self . ufun ( offer )) if ( offer is not None and offer_utility >= acceptable_utility and offer_utility >= self . ufun . reserved_value ): return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # If I cannot find any offers, I know that there are NO rational outcomes in this negotiation for me and will end it. if next_offer is None : return SAOResponse ( ResponseType . END_NEGOTIATION , None ) # Offer my next-offer and record it self . _sent . add ( next_offer ) self . worst_offer_utility = next_utility return SAOResponse ( ResponseType . REJECT_OFFER , next_offer ) def sample_sent ( self ) -> Outcome | None : # Get an outcome from the set I sent so far (or my best if I sent nothing) if not len ( self . _sent ): return None return random . choice ( list ( self . _sent ))","title":"class: MiCRO"},{"location":"reference/#class-nashseeker","text":"Bases: SAONegotiator Assumes that the opponent has a fixed reserved value and seeks the Nash equilibrium. Parameters: opponent_reserved_value ( float , default: 0.25 ) \u2013 Assumed reserved value for the opponent nash_factor \u2013 Fraction (or multiple) of the agent utility at the Nash Point (assuming the opponent_reserved_value ) that is acceptable Source code in anl/anl2024/negotiators/builtins/nash_seeker.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class NashSeeker ( SAONegotiator ): \"\"\"Assumes that the opponent has a fixed reserved value and seeks the Nash equilibrium. Args: opponent_reserved_value: Assumed reserved value for the opponent nash_factor: Fraction (or multiple) of the agent utility at the Nash Point (assuming the `opponent_reserved_value`) that is acceptable \"\"\" def __init__ ( self , * args , opponent_reserved_value : float = 0.25 , nash_factor = 0.9 , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . _opponent_r = opponent_reserved_value self . _outcomes : list [ Outcome ] = [] self . _min_acceptable = float ( \"inf\" ) self . _nash_factor = nash_factor self . _best : Outcome = None # type: ignore def on_preferences_changed ( self , changes ): _ = changes # silenting a typing warning # This callback is called at the start of the negotiation after the ufun is set assert self . ufun is not None and self . ufun . outcome_space is not None # save my best outcome for later use self . _best = self . ufun . best () # check that I have access to the opponent ufun assert self . opponent_ufun is not None # set the reserved value of the opponent self . opponent_ufun . reserved_value = self . _opponent_r # consider my and my parther's ufuns ufuns = ( self . ufun , self . opponent_ufun ) # list all outcomes outcomes = list ( self . ufun . outcome_space . enumerate_or_sample ()) # find the pareto-front and the nash point frontier_utils , frontier_indices = pareto_frontier ( ufuns , outcomes ) frontier_outcomes = [ outcomes [ _ ] for _ in frontier_indices ] my_frontier_utils = [ _ [ 0 ] for _ in frontier_utils ] nash = nash_points ( ufuns , frontier_utils ) # type: ignore if nash : # find my utility at the Nash Bargaining Solution. my_nash_utility = nash [ 0 ][ 0 ][ 0 ] else : my_nash_utility = 0.5 * ( float ( self . ufun . max ()) + self . ufun . reserved_value ) # Set the acceptable utility limit self . _min_acceptable = my_nash_utility * self . _nash_factor # Set the set of outcomes to offer from self . _outcomes = [ w for u , w in zip ( my_frontier_utils , frontier_outcomes ) if u >= self . _min_acceptable ] def __call__ ( self , state : SAOState ) -> SAOResponse : # just assert that I have a ufun and I know the outcome space. assert self . ufun is not None and self . ufun . outcome_space is not None # read the current offer from the state. None means I am starting the negotiation offer = state . current_offer # Accept the offer if its utility is higher than my utility at the Nash Bargaining Solution with the assumed opponent ufun if offer and float ( self . ufun ( offer )) >= self . _min_acceptable : return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # If I could not find the Nash Bargaining Solution, just offer my best outcome forever. if not self . _outcomes : return SAOResponse ( ResponseType . REJECT_OFFER , self . _best ) # Offer some outcome with high utility relative to the Nash Bargaining Solution return SAOResponse ( ResponseType . REJECT_OFFER , random . choice ( self . _outcomes ))","title":"class: NashSeeker"},{"location":"reference/#class-rvfitter","text":"Bases: SAONegotiator A simple negotiator that uses curve fitting to learn the reserved value. Parameters: min_unique_utilities ( int , default: 10 ) \u2013 Number of different offers from the opponent before starting to attempt learning their reserved value. e ( float , default: 5.0 ) \u2013 The concession exponent used for the agent's offering strategy stochasticity ( float , default: 0.1 ) \u2013 The level of stochasticity in the offers. enable_logging ( bool , default: False ) \u2013 If given, a log will be stored for the estimates. Remarks: - Assumes that the opponent is using a time-based offering strategy that offers the outcome at utility $u(t) = (u_0 - r) - r \\exp(t^e)$ where $u_0$ is the utility of the first offer (directly read from the opponent ufun), $e$ is an exponent that controls the concession rate and $r$ is the reserved value we want to learn. - After it receives offers with enough different utilities, it starts finding the optimal values for $e$ and $r$. - When it is time to respond, RVFitter, calculates the set of rational outcomes **for both agents** based on its knowledge of the opponent ufun (given) and reserved value (learned). It then applies the same concession curve defined above to concede over an ordered list of these outcomes. - Is this better than using the same concession curve on the outcome space without even trying to learn the opponent reserved value? Maybe sometimes but empirical evaluation shows that it is not in general. - Note that the way we check for availability of enough data for training is based on the uniqueness of the utility of offers from the opponent (for the opponent). Given that these are real values, this approach is suspect because of rounding errors. If two outcomes have the same utility they may appear to have different but very close utilities because or rounding errors (or genuine very small differences). Such differences should be ignored. - Note also that we start assuming that the opponent reserved value is 0.0 which means that we are only restricted with our own reserved values when calculating the rational outcomes. This is the best case scenario for us because we have MORE negotiation power when the partner has LOWER utility. Source code in anl/anl2024/negotiators/builtins/rv_fitter.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 class RVFitter ( SAONegotiator ): \"\"\"A simple negotiator that uses curve fitting to learn the reserved value. Args: min_unique_utilities: Number of different offers from the opponent before starting to attempt learning their reserved value. e: The concession exponent used for the agent's offering strategy stochasticity: The level of stochasticity in the offers. enable_logging: If given, a log will be stored for the estimates. Remarks: - Assumes that the opponent is using a time-based offering strategy that offers the outcome at utility $u(t) = (u_0 - r) - r \\\\exp(t^e)$ where $u_0$ is the utility of the first offer (directly read from the opponent ufun), $e$ is an exponent that controls the concession rate and $r$ is the reserved value we want to learn. - After it receives offers with enough different utilities, it starts finding the optimal values for $e$ and $r$. - When it is time to respond, RVFitter, calculates the set of rational outcomes **for both agents** based on its knowledge of the opponent ufun (given) and reserved value (learned). It then applies the same concession curve defined above to concede over an ordered list of these outcomes. - Is this better than using the same concession curve on the outcome space without even trying to learn the opponent reserved value? Maybe sometimes but empirical evaluation shows that it is not in general. - Note that the way we check for availability of enough data for training is based on the uniqueness of the utility of offers from the opponent (for the opponent). Given that these are real values, this approach is suspect because of rounding errors. If two outcomes have the same utility they may appear to have different but very close utilities because or rounding errors (or genuine very small differences). Such differences should be ignored. - Note also that we start assuming that the opponent reserved value is 0.0 which means that we are only restricted with our own reserved values when calculating the rational outcomes. This is the best case scenario for us because we have MORE negotiation power when the partner has LOWER utility. \"\"\" def __init__ ( self , * args , min_unique_utilities : int = 10 , e : float = 5.0 , stochasticity : float = 0.1 , enable_logging : bool = False , ** kwargs , ): super () . __init__ ( * args , ** kwargs ) self . min_unique_utilities = min_unique_utilities self . e = e self . stochasticity = stochasticity # keeps track of times at which the opponent offers self . opponent_times : list [ float ] = [] # keeps track of opponent utilities of its offers self . opponent_utilities : list [ float ] = [] # keeps track of the our last estimate of the opponent reserved value self . _past_oppnent_rv = 0.0 # keeps track of the rational outcome set given our estimate of the # opponent reserved value and our knowledge of ours self . _rational : list [ tuple [ float , float , Outcome ]] = [] self . _enable_logging = enable_logging def __call__ ( self , state : SAOState ) -> SAOResponse : assert self . ufun and self . opponent_ufun # update the opponent reserved value in self.opponent_ufun self . update_reserved_value ( state ) # rune the acceptance strategy and if the offer received is acceptable, accept it if self . is_acceptable ( state ): return SAOResponse ( ResponseType . ACCEPT_OFFER , state . current_offer ) # The offering strategy # We only update our estimate of the rational list of outcomes if it is not set or # there is a change in estimated reserved value if ( not self . _rational or abs ( self . opponent_ufun . reserved_value - self . _past_oppnent_rv ) > 1e-3 ): # The rational set of outcomes sorted dependingly according to our utility function # and the opponent utility function (in that order). self . _rational = sorted ( [ ( my_util , opp_util , _ ) for _ in self . nmi . outcome_space . enumerate_or_sample ( levels = 10 , max_cardinality = 100_000 ) if ( my_util := float ( self . ufun ( _ ))) > self . ufun . reserved_value and ( opp_util := float ( self . opponent_ufun ( _ ))) > self . opponent_ufun . reserved_value ], ) # If there are no rational outcomes (i.e. our estimate of the opponent rv is very wrogn), # then just revert to offering our top offer if not self . _rational : return SAOResponse ( ResponseType . REJECT_OFFER , self . ufun . best ()) # find our aspiration level (value between 0 and 1) the higher the higher utility we require asp = aspiration_function ( state . relative_time , 1.0 , 0.0 , self . e ) # find the index of the rational outcome at the aspiration level (in the rational set of outcomes) n_rational = len ( self . _rational ) max_rational = n_rational - 1 min_indx = max ( 0 , min ( max_rational , int ( asp * max_rational ))) # find current stochasticity which goes down from the set level to zero linearly s = aspiration_function ( state . relative_time , self . stochasticity , 0.0 , 1.0 ) # find the index of the maximum utility we require based on stochasticity (going down over time) max_indx = max ( 0 , min ( int ( min_indx + s * n_rational ), max_rational )) # offer an outcome in the selected range indx = random . randint ( min_indx , max_indx ) if min_indx != max_indx else min_indx outcome = self . _rational [ indx ][ - 1 ] return SAOResponse ( ResponseType . REJECT_OFFER , outcome ) def is_acceptable ( self , state : SAOState ) -> bool : # The acceptance strategy assert self . ufun and self . opponent_ufun # get the offer from the mechanism state offer = state . current_offer # If there is no offer, there is nothing to accept if offer is None : return False # Find the current aspiration level asp = aspiration_function ( state . relative_time , 1.0 , self . ufun . reserved_value , self . e ) # accept if the utility of the received offer is higher than # the current aspiration return float ( self . ufun ( offer )) >= asp def update_reserved_value ( self , state : SAOState ): # Learns the reserved value of the partner assert self . opponent_ufun is not None # extract the current offer from the state offer = state . current_offer if offer is None : return # save to the list of utilities received from the opponent and their times self . opponent_utilities . append ( float ( self . opponent_ufun ( offer ))) self . opponent_times . append ( state . relative_time ) # If we do not have enough data, just assume that the opponent # reserved value is zero n_unique = len ( set ( self . opponent_utilities )) if n_unique < self . min_unique_utilities : self . _past_oppnent_rv = 0.0 self . opponent_ufun . reserved_value = 0.0 return # Use curve fitting to estimate the opponent reserved value # We assume the following: # - The opponent is using a concession strategy with an exponent between 0.2, 5.0 # - The opponent never offers outcomes lower than their reserved value which means # that their rv must be no higher than the worst outcome they offered for themselves. bounds = (( 0.2 , 0.0 ), ( 5.0 , min ( self . opponent_utilities ))) err = \"\" try : optimal_vals , _ = curve_fit ( lambda x , e , rv : aspiration_function ( x , self . opponent_utilities [ 0 ], rv , e ), self . opponent_times , self . opponent_utilities , bounds = bounds , ) self . _past_oppnent_rv = self . opponent_ufun . reserved_value self . opponent_ufun . reserved_value = optimal_vals [ 1 ] except Exception as e : err , optimal_vals = f \" { str ( e ) } \" , [ None , None ] # log my estimate if self . _enable_logging : self . nmi . log_info ( self . id , dict ( estimated_rv = self . opponent_ufun . reserved_value , n_unique = n_unique , opponent_utility = self . opponent_utilities [ - 1 ], estimated_exponent = optimal_vals [ 0 ], estimated_max = self . opponent_utilities [ 0 ], error = err , ), )","title":"class: RVFitter"},{"location":"reference/#tournaments","text":"","title":"Tournaments"},{"location":"reference/#function-anl2024_tournament","text":"Runs an ANL 2024 tournament Parameters: scenarios ( tuple [ Scenario , ...] | list [ Scenario ] , default: tuple () ) \u2013 A list of predefined scenarios to use for the tournament n_scenarios ( int , default: DEFAULT2024SETTINGS ['n_scenarios'] ) \u2013 Number of negotiation scenarios to generate specifically for this tournament n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: DEFAULT2024SETTINGS ['n_outcomes'] ) \u2013 Number of outcomes (or a min/max tuple of n. outcomes) for each scenario competitors ( tuple [ type [ Negotiator ] | str , ...] | list [ type [ Negotiator ] | str ] , default: DEFAULT_AN2024_COMPETITORS ) \u2013 list of competitor agents competitor_params ( Sequence [ dict | None] | None , default: None ) \u2013 If given, parameters to construct each competitor rotate_ufuns ( bool , default: DEFAULT2024SETTINGS ['rotate_ufuns'] ) \u2013 If given, each scenario will be tried with both orders of the ufuns. n_repetitions ( int , default: DEFAULT2024SETTINGS ['n_repetitions'] ) \u2013 Number of times to repeat each negotiation n_steps ( int | tuple [ int , int ] | None , default: DEFAULT2024SETTINGS ['n_steps'] ) \u2013 Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) time_limit ( float | tuple [ float , float ] | None , default: DEFAULT2024SETTINGS ['time_limit'] ) \u2013 Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) pend ( float | tuple [ float , float ] , default: DEFAULT2024SETTINGS ['pend'] ) \u2013 Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) pend_per_second ( float | tuple [ float , float ] , default: DEFAULT2024SETTINGS ['pend_per_second'] ) \u2013 Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) step_time_limit ( float | tuple [ float , float ] | None , default: DEFAULT2024SETTINGS ['step_time_limit'] ) \u2013 Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) negotiator_time_limit ( float | tuple [ float , float ] | None , default: DEFAULT2024SETTINGS ['negotiator_time_limit'] ) \u2013 Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) name ( str | None , default: None ) \u2013 Name of the tournament nologs ( bool , default: False ) \u2013 If given, no logs will be saved njobs ( int , default: 0 ) \u2013 Number of parallel jobs to use. -1 for serial and 0 for all cores plot_fraction ( float , default: 0.2 ) \u2013 Fraction of negotiations to plot. Only used if not nologs verbosity ( int , default: 1 ) \u2013 Verbosity level. The higher the more verbose self_play ( bool , default: DEFAULT2024SETTINGS ['self_play'] ) \u2013 Allow negotiators to run against themselves. randomize_runs ( bool , default: DEFAULT2024SETTINGS ['randomize_runs'] ) \u2013 Randomize the order of negotiations save_every ( int , default: 0 ) \u2013 Save logs every this number of negotiations save_stats ( bool , default: True ) \u2013 Save statistics for scenarios known_partner ( bool , default: DEFAULT2024SETTINGS ['known_partner'] ) \u2013 Allow negotiators to know the type of their partner (through their ID) final_score ( tuple [ str , str ] , default: DEFAULT2024SETTINGS ['final_score'] ) \u2013 The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max base_path ( Path | None , default: None ) \u2013 Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments scenario_generator ( str | ScenarioGenerator , default: DEFAULT2024SETTINGS ['scenario_generator'] ) \u2013 An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. generator_params ( dict [ str , Any ] | None , default: DEFAULT2024SETTINGS ['generator_params'] ) \u2013 Parameters passed to the scenario generator plot_params ( dict [ str , Any ] | None , default: None ) \u2013 If given, overrides plotting parameters. See nemgas.sao.SAOMechanism.plot() for all parameters Returns: SimpleTournamentResults \u2013 Tournament results as a SimpleTournamentResults object. Source code in anl/anl2024/runner.py 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 def anl2024_tournament ( scenarios : tuple [ Scenario , ... ] | list [ Scenario ] = tuple (), n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore competitors : tuple [ type [ Negotiator ] | str , ... ] | list [ type [ Negotiator ] | str ] = DEFAULT_AN2024_COMPETITORS , rotate_ufuns : bool = DEFAULT2024SETTINGS [ \"rotate_ufuns\" ], # type: ignore n_repetitions : int = DEFAULT2024SETTINGS [ \"n_repetitions\" ], # type: ignore n_steps : int | tuple [ int , int ] | None = DEFAULT2024SETTINGS [ \"n_steps\" ], # type: ignore time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"time_limit\" ], # type: ignore pend : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend\" ], # type: ignore pend_per_second : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend_per_second\" ], # type: ignore step_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"step_time_limit\" ], # type: ignore negotiator_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"negotiator_time_limit\" ], # type: ignore self_play : bool = DEFAULT2024SETTINGS [ \"self_play\" ], # type: ignore randomize_runs : bool = DEFAULT2024SETTINGS [ \"randomize_runs\" ], # type: ignore known_partner : bool = DEFAULT2024SETTINGS [ \"known_partner\" ], # type: ignore final_score : tuple [ str , str ] = DEFAULT2024SETTINGS [ \"final_score\" ], # type: ignore scenario_generator : str | ScenarioGenerator = DEFAULT2024SETTINGS [ \"scenario_generator\" ], # type: ignore generator_params : dict [ str , Any ] | None = DEFAULT2024SETTINGS [ \"generator_params\" ], # type: ignore competitor_params : Sequence [ dict | None ] | None = None , name : str | None = None , nologs : bool = False , njobs : int = 0 , plot_fraction : float = 0.2 , verbosity : int = 1 , save_every : int = 0 , save_stats : bool = True , base_path : Path | None = None , plot_params : dict [ str , Any ] | None = None , raise_exceptions : bool = True , ) -> SimpleTournamentResults : \"\"\"Runs an ANL 2024 tournament Args: scenarios: A list of predefined scenarios to use for the tournament n_scenarios: Number of negotiation scenarios to generate specifically for this tournament n_outcomes: Number of outcomes (or a min/max tuple of n. outcomes) for each scenario competitors: list of competitor agents competitor_params: If given, parameters to construct each competitor rotate_ufuns: If given, each scenario will be tried with both orders of the ufuns. n_repetitions: Number of times to repeat each negotiation n_steps: Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) time_limit: Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) pend: Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) pend_per_second: Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) step_time_limit: Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) negotiator_time_limit: Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) name: Name of the tournament nologs: If given, no logs will be saved njobs: Number of parallel jobs to use. -1 for serial and 0 for all cores plot_fraction: Fraction of negotiations to plot. Only used if not nologs verbosity: Verbosity level. The higher the more verbose self_play: Allow negotiators to run against themselves. randomize_runs: Randomize the order of negotiations save_every: Save logs every this number of negotiations save_stats: Save statistics for scenarios known_partner: Allow negotiators to know the type of their partner (through their ID) final_score: The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max base_path: Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments scenario_generator: An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. generator_params: Parameters passed to the scenario generator plot_params: If given, overrides plotting parameters. See `nemgas.sao.SAOMechanism.plot()` for all parameters Returns: Tournament results as a `SimpleTournamentResults` object. \"\"\" if generator_params is None : generator_params = dict () if isinstance ( scenario_generator , str ): scenario_generator = GENERAROR_MAP [ scenario_generator ] all_outcomes = not scenario_generator == zerosum_pie_scenarios if nologs : path = None elif base_path is not None : path = Path ( base_path ) / ( name if name else unique_name ( \"anl\" )) else : path = DEFAULT_TOURNAMENT_PATH / ( name if name else unique_name ( \"anl\" )) params = dict ( ylimits = ( 0 , 1 ), mark_offers_view = True , mark_pareto_points = all_outcomes , mark_all_outcomes = all_outcomes , mark_nash_points = True , mark_kalai_points = all_outcomes , mark_max_welfare_points = all_outcomes , show_agreement = True , show_pareto_distance = False , show_nash_distance = True , show_kalai_distance = False , show_max_welfare_distance = False , show_max_relative_welfare_distance = False , show_end_reason = True , show_annotations = not all_outcomes , show_reserved = True , show_total_time = True , show_relative_time = True , show_n_steps = True , ) if plot_params : params = params . update ( plot_params ) scenarios = list ( scenarios ) + list ( scenario_generator ( n_scenarios , n_outcomes , ** generator_params ) ) private_infos = [ tuple ( dict ( opponent_ufun = U ( values = _ . values , weights = _ . weights , bias = _ . _bias , reserved_value = 0 , outcome_space = _ . outcome_space )) # type: ignore for _ in s . ufuns [:: - 1 ] ) for s in scenarios ] return cartesian_tournament ( competitors = tuple ( competitors ), scenarios = scenarios , competitor_params = competitor_params , private_infos = private_infos , # type: ignore rotate_ufuns = rotate_ufuns , n_repetitions = n_repetitions , path = path , njobs = njobs , mechanism_type = SAOMechanism , n_steps = n_steps , time_limit = time_limit , pend = pend , pend_per_second = pend_per_second , step_time_limit = step_time_limit , negotiator_time_limit = negotiator_time_limit , mechanism_params = None , plot_fraction = plot_fraction , verbosity = verbosity , self_play = self_play , randomize_runs = randomize_runs , save_every = save_every , save_stats = save_stats , final_score = final_score , id_reveals_type = known_partner , name_reveals_type = True , plot_params = params , raise_exceptions = raise_exceptions , )","title":"function: anl2024_tournament"},{"location":"reference/#constant-default_an2024_competitors","text":"Default set of negotiators (agents) used as competitors","title":"constant: DEFAULT_AN2024_COMPETITORS"},{"location":"reference/#constant-default_tournament_path","text":"Default location to store tournament logs","title":"constant: DEFAULT_TOURNAMENT_PATH"},{"location":"reference/#constant-default2024settings","text":"Default settings for ANL 2024","title":"constant: DEFAULT2024SETTINGS"},{"location":"reference/#helpers-scenario-generation","text":"","title":"Helpers (Scenario Generation)"},{"location":"reference/#type-scenariogenerator","text":"Type of callable that can be used for generating scenarios. It must receive the number of scenarios and number of outcomes (as int, tuple or list) and return a list of Scenario s","title":"type: ScenarioGenerator"},{"location":"reference/#function-mixed_scenarios","text":"Generates a mix of zero-sum, monotonic and general scenarios Parameters: n_scenarios ( int , default: DEFAULT2024SETTINGS ['n_scenarios'] ) \u2013 Number of scenarios to genearate n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: DEFAULT2024SETTINGS ['n_outcomes'] ) \u2013 Number of outcomes (or a list of range thereof). reserved_ranges ( ReservedRanges , default: DEFAULT2024SETTINGS ['reserved_ranges'] ) \u2013 the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome log_uniform ( bool , default: DEFAULT2024SETTINGS ['outcomes_log_uniform'] ) \u2013 Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. zerosum_fraction ( float , default: DEFAULT2024SETTINGS ['generator_params']['zerosum_fraction'] ) \u2013 Fraction of zero-sum scenarios. These are original DivideThePie scenarios monotonic_fraction ( float , default: DEFAULT2024SETTINGS ['generator_params']['monotonic_fraction'] ) \u2013 Fraction of scenarios where each ufun is a monotonic function of the received pie. curve_fraction ( float , default: DEFAULT2024SETTINGS ['generator_params']['curve_fraction'] ) \u2013 Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. pareto_first ( bool , default: DEFAULT2024SETTINGS ['generator_params']['pareto_first'] ) \u2013 If given, the Pareto frontier will always be in the first set of outcomes n_ufuns ( int , default: DEFAULT2024SETTINGS ['n_ufuns'] ) \u2013 Number of ufuns to generate per scenario n_pareto ( int | float | tuple [ float | int , float | int ] | list [ int | float ] , default: DEFAULT2024SETTINGS ['generator_params']['n_pareto'] ) \u2013 Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. pareto_log_uniform ( bool , default: False ) \u2013 Use log-uniform instead of uniform sampling if n_pareto is a tuple n_trials \u2013 Number of times to retry generating each scenario if failures occures Returns: list [ Scenario ] \u2013 A list Scenario s Source code in anl/anl2024/runner.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def mixed_scenarios ( n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore * , reserved_ranges : ReservedRanges = DEFAULT2024SETTINGS [ \"reserved_ranges\" ], # type: ignore log_uniform : bool = DEFAULT2024SETTINGS [ \"outcomes_log_uniform\" ], # type: ignore zerosum_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"zerosum_fraction\" ], # type: ignore monotonic_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"monotonic_fraction\" ], # type: ignore curve_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"curve_fraction\" ], # type: ignore pareto_first : bool = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"pareto_first\" ], # type: ignore n_ufuns : int = DEFAULT2024SETTINGS [ \"n_ufuns\" ], # type: ignore n_pareto : int | float | tuple [ float | int , float | int ] | list [ int | float ] = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"n_pareto\" ], # type: ignore pareto_log_uniform : bool = False , n_trials = 10 , ) -> list [ Scenario ]: \"\"\"Generates a mix of zero-sum, monotonic and general scenarios Args: n_scenarios: Number of scenarios to genearate n_outcomes: Number of outcomes (or a list of range thereof). reserved_ranges: the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome log_uniform: Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. zerosum_fraction: Fraction of zero-sum scenarios. These are original DivideThePie scenarios monotonic_fraction: Fraction of scenarios where each ufun is a monotonic function of the received pie. curve_fraction: Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. pareto_first: If given, the Pareto frontier will always be in the first set of outcomes n_ufuns: Number of ufuns to generate per scenario n_pareto: Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. pareto_log_uniform: Use log-uniform instead of uniform sampling if n_pareto is a tuple n_trials: Number of times to retry generating each scenario if failures occures Returns: A list `Scenario` s \"\"\" assert zerosum_fraction + monotonic_fraction <= 1.0 nongeneral_fraction = zerosum_fraction + monotonic_fraction ufun_sets = [] for i in range ( n_scenarios ): r = random . random () n = intin ( n_outcomes , log_uniform ) name = \"S\" if r < nongeneral_fraction : n_pareto_selected = n name = \"DivideThePieGen\" else : if isinstance ( n_pareto , Iterable ): n_pareto = type ( n_pareto )( int ( _ * n + 0.5 ) if _ < 1 else int ( _ ) for _ in n_pareto # type: ignore ) else : n_pareto = int ( 0.5 + n_pareto * n ) if n_pareto < 1 else int ( n_pareto ) n_pareto_selected = intin ( n_pareto , log_uniform = pareto_log_uniform ) # type: ignore for _ in range ( n_trials ): try : if r < zerosum_fraction : vals = generate_utility_values ( n_pareto = n_pareto_selected , n_outcomes = n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"zero_sum\" , ) name = \"DivideThePie\" else : if n_pareto_selected < 2 : n_pareto_selected = 2 vals = generate_utility_values ( n_pareto = n_pareto_selected , n_outcomes = n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"curve\" if random . random () < curve_fraction else \"piecewise_linear\" , ) break except : continue else : continue issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( vals [ i ][ k ]) for i , _ in enumerate ( issues [ 0 ] . all )} ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \" { name }{ i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"function: mixed_scenarios"},{"location":"reference/#function-pie_scenarios","text":"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Parameters: n_scenarios ( int , default: 20 ) \u2013 Number of scenarios to create n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: 100 ) \u2013 Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). reserved_ranges ( ReservedRanges , default: ((0.0, 0.999999), (0.0, 0.999999)) ) \u2013 Ranges of reserved values for first and second negotiators log_uniform ( bool , default: True ) \u2013 If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). monotonic \u2013 If true all ufuns are monotonically increasing in the portion of the pie Remarks When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. Source code in anl/anl2024/runner.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , monotonic = False , ) -> list [ Scenario ]: \"\"\"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). monotonic: If true all ufuns are monotonically increasing in the portion of the pie Remarks: - When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. \"\"\" ufun_sets = [] base_name = \"DivideTyePie\" if monotonic else \"S\" def normalize ( x ): mn , mx = x . min (), x . max () return (( x - mn ) / ( mx - mn )) . tolist () def make_monotonic ( x , i ): x = np . sort ( np . asarray ( x ), axis = None ) if i : x = x [:: - 1 ] r = random . random () if r < 0.33 : x = np . exp ( x ) elif r < 0.67 : x = np . log ( x ) else : pass return normalize ( x ) max_jitter_level = 0.8 for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ( [ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" if not monotonic else \"i1\" , ), ) # funs = [ # dict( # zip( # issues[0].all, # # adjust(np.asarray([random.random() for _ in range(n)])), # generate(n, i), # ) # ) # for i in range(2) # ] os = make_os ( issues , name = f \" { base_name }{ i } \" ) outcomes = list ( os . enumerate_or_sample ()) ufuns = U . generate_bilateral ( outcomes , conflict_level = 0.5 + 0.5 * random . random (), conflict_delta = random . random (), ) jitter_level = random . random () * max_jitter_level funs = [ np . asarray ([ float ( u ( _ )) for _ in outcomes ]) + np . random . random () * jitter_level for u in ufuns ] if monotonic : funs = [ make_monotonic ( x , i ) for i , x in enumerate ( funs )] else : funs = [ normalize ( x ) for x in funs ] ufuns = tuple ( U ( values = ( TableFun ( dict ( zip ( issues [ 0 ] . all , vals ))),), name = f \" { uname }{ i } \" , outcome_space = os , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), ) for ( uname , vals ) in zip (( \"First\" , \"Second\" ), funs ) # for (uname, r, vals) in zip((\"First\", \"Second\"), reserved_ranges, funs) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"function: pie_scenarios"},{"location":"reference/#function-arbitrary_pie_scenarios","text":"Source code in anl/anl2024/runner.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def arbitrary_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = False , )","title":"function: arbitrary_pie_scenarios"},{"location":"reference/#function-monotonic_pie_scenarios","text":"Source code in anl/anl2024/runner.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def monotonic_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = True , )","title":"function: monotonic_pie_scenarios"},{"location":"reference/#function-zerosum_pie_scenarios","text":"Creates scenarios all of the DivideThePie variety with proportions giving utility Parameters: n_scenarios ( int , default: 20 ) \u2013 Number of scenarios to create n_outcomes ( int | tuple [ int , int ] | list [ int ] , default: 100 ) \u2013 Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). reserved_ranges ( ReservedRanges , default: ((0.0, 0.499999), (0.0, 0.499999)) ) \u2013 Ranges of reserved values for first and second negotiators log_uniform ( bool , default: True ) \u2013 If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). Remarks When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently Source code in anl/anl2024/runner.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def zerosum_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.499999 ), ( 0.0 , 0.499999 )), log_uniform : bool = True , ) -> list [ Scenario ]: \"\"\"Creates scenarios all of the DivideThePie variety with proportions giving utility Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). Remarks: - When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently \"\"\" ufun_sets = [] for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( int ( str ( _ ) . split ( \"_\" )[ k ]) / ( n - 1 )) for _ in issues [ 0 ] . all } ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \"DivideTyePie { i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , pareto = tuple ( tuple ( u ( _ ) for u in ufuns ) for _ in make_os ( issues ) . enumerate_or_sample () ), reserved_ranges = reserved_ranges , ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"function: zerosum_pie_scenarios"},{"location":"tutorials/04.ideas/","text":"Ideas for developing your agent This section of the tutorials will discuss some possible ideas for developing your agent. It is completely optional to read this but it may provide some directions that help you in your quest. We will assume that you are using the component based approach discussed in the second tutorial. Let\u2019s start by reminding ourselves of the agent decomposition used by built in agents (check this video <https://youtu.be/3xwR-aPZSb0> __ explains the main components in details). The three main components of an agent in this decomposition are the trading strategy, negotiation control strategy and production strategy. .. image:: anatomy.png The trading strategy decides what should the agent buy and sell (the trading schedule ) and the negotiation control strategy takes that as input and uses it to drive negotiations in order to carry out this plan. The production strategy controls the factory by deciding how many items to produce at every time step (based on existing inventory and the trading schedule). We will discuss ideas for improving each one of these three components separately. Before diving into these ideas, it is important to note that the overall performance of the agent does not come from having one perfect component but from harmony between all the components constituting it. For example, a trading strategy that generates a perfect trading schedule is useless without a negotiation control strategy capable of achieving that schedule. Trading Strategy ~~~~~~~~~~~~~~~~ Representing the planning department of a company, the trading strategy seems like the obvious target of improvement. This figure shows the outputs of the trading strategy and the three examples implemented in the scml package. .. image:: trading.png The best trading strategy used by the built-in agents is the PredictionBasedTradingStrategy and we will focus on it as it seems the most amenable to improvement. This trading strategy uses two components, a TradePredictionStrategy that predicts the amount of trade on the input and output products of the agent as a function of the simulation step, and an ERPredictionStrategy predicting the quantity that will actually be executed from a contract. These predictions are both set to constants for the built-in component. This immediately suggests the following ideas IDEA 1: Improve trade prediction ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The only TradePredictionStrategy implemented in scml is the FixedTradePredictionStrategy which predicts trade at a fixed amount in every product at every step (currently set to half the number of lines: :math: 5 ). This can definitely be improved. Train a regressor (e.g. a scikit-learn <https://scikit-learn.org/stable/user_guide.html> regressor) on many worlds to receive the product number and the fraction of the simulation steps passed and predict the amount of trade and use this regressor in real time (or store its results in a table that you can load in real time <https://scml.readthedocs.io/en/latest/faq.html#how-can-i-access-a-data-file-in-my-package> ). Improve the regressor using incremental learning in real time during world simulation. This may not be very effective in short simulations but we will simulate up to :math: 200 steps so it may improve performance. IDEA 2: Improve execution rate prediction ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The only ERPredictionStrategy implemented in the system is the FixedERPredictionStrategy which will expect that half of the quantity in any contract will be executed. This can easily be improved using several approaches. Use the financial reports of your suppliers and consumers to predict the possibility that they will breach contracts in the future. Again you can train a regressor that receives few past financial reports and predicts future behavior using simulations against a variety of agents (including your own!) and then load it in real time. Use more general market conditions for prediction of actual trade amount and base your prediction of the contract execution rate on that. IDEA3: Improve the logic of the trading strategy ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The PredictionBasedTradingStrategy just uses the TradePredictionStrategy and ERPredictionStrategy directly for deciding trade but that need not be the optimal thing to do. It may be possible to change that logic of the trading strategy itself to add a higher level of control over the outputs of these base prediction strategies. Negotiation Manager ~~~~~~~~~~~~~~~~~~~ This is a negotiation competition and it seems fit to focus our efforts on negotiation. Moreover, as we indicated earlier, having the perfect trade schedule coming out from the trading strategy is useless for the agent if it cannot negotiate effectively to achieve that schedule. The negotiation control strategy consists of two main components: Negotiation Manager responsible of requesting negotiations as needed and responding to such requests Negotiation Algorithm which can be implemented using one or more negmas SAOController <https://negmas.readthedocs.io/en/latest/modules/sao.html?highlight=Controller#module-negmas.sao> or directly using negmas SAONegotiator <https://negmas.readthedocs.io/en/latest/modules/sao.html?highlight=Negotiator#module-negmas.sao> . This video <https://youtu.be/10Rjl3ikaDU> __ describes available controllers and negotiators and of course you can - and should - design your own. This figure shows the two inputs you need to define for any negotiation manager: target_quantity and acceptable_unit_price . Their names are self-descriptive. .. image:: negotiation.png Built-in negotiation managers are intentionally pretty basic. It may be that this is the point of improvement that has the highest probability of leading to winning agents (that may not be true though as the trading strategy seems as important). Here are some ideas for improving the negotiation control strategy IDEA 4: Improve the negotiation manager ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The negotiation manager responsible of starting and accepting negotiations in scml is extremely basic. It uses a target quantity that is set directly as the difference between needs and secured quantity and it does not take into account in any way running negotiations. You can access running negotiations using self.negotiations and standing negotiation requests using self.negotiation_requests . It always negotiates with everybody. You can use financial reports to decide whom to negotiate with. It uses fixed ranges for negotiation issues. You can try to dynamically decide the ranges allowed for negotiation issues based on market conditions. For example, you can set the range of prices based on your estimate of the current trading price of products. IDEA 5 Improve signing strategy ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Deciding what to sign is not strictly a part of the negotiation strategy but it needs to be implemented to respond to sign_all_contracts . Currently, it is handled by the trading strategy but you can override that by providing your own SigningStrategy that overrides sign_all_contracts . All negotiations in a single simulation step run in parallel. This means that the negotiation manager is prone to over-contracting. This can then be corrected using a SigningStrategy that intelligently decides what to sign. Negotiation Algorithm ~~~~~~~~~~~~~~~~~~~~~ All built in negotiations are conducted using either simple negotiation algorithm (e.g. time-based strategy, naive tit-for-tat implementation, \u2026) or a simple negmas built in controller. None of the adequately handles the two main challenges: concurrent negotiations within a single simulation step and taking into account future negotiation opportunities. IDEA 6: Improve concurrent negotiation control ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The StepController is the negotiation algorithm used by the StepNegotiationManager employed by the DecentralizingAgent (the top built-in agent). It instantiates one controller to handle buying and another to handle selling for each simulation step . These controllers rely heavily on the SAOSyncController of negmas using a time-based meta-negotiation strategy. That is a very simple algorithm that is not expected to effectively handle concurrent negotiations. Try to find a way to either coordinate the behavior of multiple autonomous negotiators each simulation step or to centrally control these negotiators to achieve the preset target. IDEA 7: Improve sequential negotiation control ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Agents in SCML negotiate repeatedly. This means that the utility of any offer in any negotiation does not only depend on current market conditions but also in expected future negotiations. Built-in agents side step the need to take that into account during negotiation by having a trading strategy and a negotiation manager set their targets for them rendering negotiations in every simulation step independent from future negotiations (given the targets). This is clearly a simplistic heuristic. Try to find a way to take future negotiations into account when designing your agent. One way to do that is to have them affect the utility function used by your controller/negotiator. IDEA 8: Improve the utility functions used ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The IndependentNegotiationManager uses linear independent utility functions with a simple time-base negotiation ( AspirationNegotiator ) for all of its negotiations. The other two negotiation managers employ controllers that define their utilities linearly using some built-in fixed weights for price and quantity. That is obviously suboptimal. 1. Try to improve the utility function used by either the negotiators or the controller (depending on the negotiation manager you use) to achieve higher expected utilities. 2. Try to take the identity of the agent you are negotiating with into account in your utility calculations. A contract with a trustworthy agent has more utility than one with a non-trustworthy agent. You can use the financial reports of agents to judge their trustworthiness . Production Strategy ~~~~~~~~~~~~~~~~~~~ That is the simplest of the three components. There are two main production strategies in scml as described earlier in the second tutorial: supply based or demand based production strategies. IDEA 9: Base production decisions on trading prices (as well as contracts). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Given that disposal cost is zero and storage capacity is infinite, it seems that the only optimization you can do is to avoid over production. Production has cost so over production may not be a good idea. On the other hand, the inventory is valued in SCML 2020 at half the trading price which means that it may be a good idea to convert inputs to outputs (even if you do not sell that output) if the difference in trading prices at the end of simulation offsets your production costs. Try creating a production strategy that takes this effect into account switching between supply based and demand based production using a estimate of the final trading prices of its input and output products. Final Remarks ~~~~~~~~~~~~~ The ideas presented above are, by no means, exclusive or comprehensive. You can combine them and add new ones. The main reason we present these ideas is to challenge you to come with better ones . Happy coding :-) Download Notebook","title":"04.ideas"},{"location":"tutorials/04.ideas/#ideas-for-developing-your-agent","text":"This section of the tutorials will discuss some possible ideas for developing your agent. It is completely optional to read this but it may provide some directions that help you in your quest. We will assume that you are using the component based approach discussed in the second tutorial. Let\u2019s start by reminding ourselves of the agent decomposition used by built in agents (check this video <https://youtu.be/3xwR-aPZSb0> __ explains the main components in details). The three main components of an agent in this decomposition are the trading strategy, negotiation control strategy and production strategy. .. image:: anatomy.png The trading strategy decides what should the agent buy and sell (the trading schedule ) and the negotiation control strategy takes that as input and uses it to drive negotiations in order to carry out this plan. The production strategy controls the factory by deciding how many items to produce at every time step (based on existing inventory and the trading schedule). We will discuss ideas for improving each one of these three components separately. Before diving into these ideas, it is important to note that the overall performance of the agent does not come from having one perfect component but from harmony between all the components constituting it. For example, a trading strategy that generates a perfect trading schedule is useless without a negotiation control strategy capable of achieving that schedule. Trading Strategy ~~~~~~~~~~~~~~~~ Representing the planning department of a company, the trading strategy seems like the obvious target of improvement. This figure shows the outputs of the trading strategy and the three examples implemented in the scml package. .. image:: trading.png The best trading strategy used by the built-in agents is the PredictionBasedTradingStrategy and we will focus on it as it seems the most amenable to improvement. This trading strategy uses two components, a TradePredictionStrategy that predicts the amount of trade on the input and output products of the agent as a function of the simulation step, and an ERPredictionStrategy predicting the quantity that will actually be executed from a contract. These predictions are both set to constants for the built-in component. This immediately suggests the following ideas IDEA 1: Improve trade prediction ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The only TradePredictionStrategy implemented in scml is the FixedTradePredictionStrategy which predicts trade at a fixed amount in every product at every step (currently set to half the number of lines: :math: 5 ). This can definitely be improved. Train a regressor (e.g. a scikit-learn <https://scikit-learn.org/stable/user_guide.html> regressor) on many worlds to receive the product number and the fraction of the simulation steps passed and predict the amount of trade and use this regressor in real time (or store its results in a table that you can load in real time <https://scml.readthedocs.io/en/latest/faq.html#how-can-i-access-a-data-file-in-my-package> ). Improve the regressor using incremental learning in real time during world simulation. This may not be very effective in short simulations but we will simulate up to :math: 200 steps so it may improve performance. IDEA 2: Improve execution rate prediction ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The only ERPredictionStrategy implemented in the system is the FixedERPredictionStrategy which will expect that half of the quantity in any contract will be executed. This can easily be improved using several approaches. Use the financial reports of your suppliers and consumers to predict the possibility that they will breach contracts in the future. Again you can train a regressor that receives few past financial reports and predicts future behavior using simulations against a variety of agents (including your own!) and then load it in real time. Use more general market conditions for prediction of actual trade amount and base your prediction of the contract execution rate on that. IDEA3: Improve the logic of the trading strategy ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The PredictionBasedTradingStrategy just uses the TradePredictionStrategy and ERPredictionStrategy directly for deciding trade but that need not be the optimal thing to do. It may be possible to change that logic of the trading strategy itself to add a higher level of control over the outputs of these base prediction strategies. Negotiation Manager ~~~~~~~~~~~~~~~~~~~ This is a negotiation competition and it seems fit to focus our efforts on negotiation. Moreover, as we indicated earlier, having the perfect trade schedule coming out from the trading strategy is useless for the agent if it cannot negotiate effectively to achieve that schedule. The negotiation control strategy consists of two main components: Negotiation Manager responsible of requesting negotiations as needed and responding to such requests Negotiation Algorithm which can be implemented using one or more negmas SAOController <https://negmas.readthedocs.io/en/latest/modules/sao.html?highlight=Controller#module-negmas.sao> or directly using negmas SAONegotiator <https://negmas.readthedocs.io/en/latest/modules/sao.html?highlight=Negotiator#module-negmas.sao> . This video <https://youtu.be/10Rjl3ikaDU> __ describes available controllers and negotiators and of course you can - and should - design your own. This figure shows the two inputs you need to define for any negotiation manager: target_quantity and acceptable_unit_price . Their names are self-descriptive. .. image:: negotiation.png Built-in negotiation managers are intentionally pretty basic. It may be that this is the point of improvement that has the highest probability of leading to winning agents (that may not be true though as the trading strategy seems as important). Here are some ideas for improving the negotiation control strategy IDEA 4: Improve the negotiation manager ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The negotiation manager responsible of starting and accepting negotiations in scml is extremely basic. It uses a target quantity that is set directly as the difference between needs and secured quantity and it does not take into account in any way running negotiations. You can access running negotiations using self.negotiations and standing negotiation requests using self.negotiation_requests . It always negotiates with everybody. You can use financial reports to decide whom to negotiate with. It uses fixed ranges for negotiation issues. You can try to dynamically decide the ranges allowed for negotiation issues based on market conditions. For example, you can set the range of prices based on your estimate of the current trading price of products. IDEA 5 Improve signing strategy ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Deciding what to sign is not strictly a part of the negotiation strategy but it needs to be implemented to respond to sign_all_contracts . Currently, it is handled by the trading strategy but you can override that by providing your own SigningStrategy that overrides sign_all_contracts . All negotiations in a single simulation step run in parallel. This means that the negotiation manager is prone to over-contracting. This can then be corrected using a SigningStrategy that intelligently decides what to sign. Negotiation Algorithm ~~~~~~~~~~~~~~~~~~~~~ All built in negotiations are conducted using either simple negotiation algorithm (e.g. time-based strategy, naive tit-for-tat implementation, \u2026) or a simple negmas built in controller. None of the adequately handles the two main challenges: concurrent negotiations within a single simulation step and taking into account future negotiation opportunities. IDEA 6: Improve concurrent negotiation control ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The StepController is the negotiation algorithm used by the StepNegotiationManager employed by the DecentralizingAgent (the top built-in agent). It instantiates one controller to handle buying and another to handle selling for each simulation step . These controllers rely heavily on the SAOSyncController of negmas using a time-based meta-negotiation strategy. That is a very simple algorithm that is not expected to effectively handle concurrent negotiations. Try to find a way to either coordinate the behavior of multiple autonomous negotiators each simulation step or to centrally control these negotiators to achieve the preset target. IDEA 7: Improve sequential negotiation control ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Agents in SCML negotiate repeatedly. This means that the utility of any offer in any negotiation does not only depend on current market conditions but also in expected future negotiations. Built-in agents side step the need to take that into account during negotiation by having a trading strategy and a negotiation manager set their targets for them rendering negotiations in every simulation step independent from future negotiations (given the targets). This is clearly a simplistic heuristic. Try to find a way to take future negotiations into account when designing your agent. One way to do that is to have them affect the utility function used by your controller/negotiator. IDEA 8: Improve the utility functions used ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The IndependentNegotiationManager uses linear independent utility functions with a simple time-base negotiation ( AspirationNegotiator ) for all of its negotiations. The other two negotiation managers employ controllers that define their utilities linearly using some built-in fixed weights for price and quantity. That is obviously suboptimal. 1. Try to improve the utility function used by either the negotiators or the controller (depending on the negotiation manager you use) to achieve higher expected utilities. 2. Try to take the identity of the agent you are negotiating with into account in your utility calculations. A contract with a trustworthy agent has more utility than one with a non-trustworthy agent. You can use the financial reports of agents to judge their trustworthiness . Production Strategy ~~~~~~~~~~~~~~~~~~~ That is the simplest of the three components. There are two main production strategies in scml as described earlier in the second tutorial: supply based or demand based production strategies. IDEA 9: Base production decisions on trading prices (as well as contracts). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Given that disposal cost is zero and storage capacity is infinite, it seems that the only optimization you can do is to avoid over production. Production has cost so over production may not be a good idea. On the other hand, the inventory is valued in SCML 2020 at half the trading price which means that it may be a good idea to convert inputs to outputs (even if you do not sell that output) if the difference in trading prices at the end of simulation offsets your production costs. Try creating a production strategy that takes this effect into account switching between supply based and demand based production using a estimate of the final trading prices of its input and output products. Final Remarks ~~~~~~~~~~~~~ The ideas presented above are, by no means, exclusive or comprehensive. You can combine them and add new ones. The main reason we present these ideas is to challenge you to come with better ones . Happy coding :-) Download Notebook","title":"Ideas for developing your agent"},{"location":"tutorials/tutorial/","text":"Preparing your development environment The first step is to install the anl package using: pip install anl The second step in developing your agent for ANL 2024 is to download the template from here . Please familiarize yourself with the competition rules availabe at the competition website . After downloading and uncompressing the template, you should do the following steps: Modify the name of the single class in myagent.py (currently called MyNegotiator ) to a representative name for your agent. We will use AwsomeNegotiator here. You should then implement your agent logic by modifying this class. Remember to change the name of the agent in the last line of the file to match your new class name ( AwsomeNegotiator ). Remember to update the agent class in the submission form on the competition website to AwsomeNegotiator . Start developing your agent as will be explained later in this tutorial You can use the following ways to test your agent: Run the following command to test your agent from the root folder of the extracted skeleton: python -m myagent.myagent Use the anl command line utility from the root folder of the extracted skeleton: anl tournament2024 --path = . --competitors = \"myagent.myagnet.AwsomeNegotiator;Boulware;Conceder\" This method is more flexible as you can control all aspects of the tournament to run. Use anl tournament2024 --help to see all available options. You can directly call anl2024_tournament() passing your agent as one of the competitors. This is the most flexible method and will be used in this tutorial. Developing a negotiator Agents for the ANL competition are standard NegMAS negotiators. As such, they can be developed using any approach used to develop negotiators in NegMAS. To develop a negotiator, you need to inherit from the SAONegotiator class and implement the call method. Here is a simple random negotiator: import random from negmas.sao import SAONegotiator , SAOResponse from negmas import Outcome , ResponseType class MyRandomNegotiator ( SAONegotiator ): def __call__ ( self , state ): offer = state . current_offer if offer is not None and self . ufun . is_not_worse ( offer , None ) and random . random () < 0.25 : return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) return SAOResponse ( ResponseType . REJECT_OFFER , self . nmi . random_outcomes ( 1 )[ 0 ]) Testing the agent from anl.anl2024 import anl2024_tournament from anl.anl2024.negotiators import Boulware , Conceder , RVFitter results = anl2024_tournament ( n_scenarios = 1 , n_repetitions = 3 , nologs = True , njobs =- 1 , competitors = [ MyRandomNegotiator , Boulware ] ) Will run 12 negotiations on 1 scenarios between 2 competitors strategy score 0 Boulware 0.701151 1 MyRandomNegotiator 0.083134 We can immediately notice that MyRandomNegotiator is getting a negative average advantage which means that it sometimes gets agreements that are worse than disagreement (i.e. with utility less than its reserved value). Can you guess why is this happening? How can we resolve that? You can easily check the final scores using the final_scores member of the returned SimpleTournamentResults object results . final_scores .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } strategy score 0 Boulware 0.701151 1 MyRandomNegotiator 0.083134 The returned results are all pandas dataframes. We can use standard pandas functions to get deeper understanding of the results. Here is how to plot a KDE figure comparing different strategies in this tournament: fig , ax = plt . subplots ( figsize = ( 8 , 6 )) df = results . scores for label , data in df . groupby ( 'strategy' ): data . advantage . plot ( kind = \"kde\" , ax = ax , label = label ) plt . ylabel ( \"advantage\" ) plt . legend (); fig , axs = plt . subplots ( 1 , 3 , figsize = ( 16 , 4 )) for i , col in enumerate ([ \"advantage\" , \"welfare\" , \"nash_optimality\" ]): results . scores . groupby ( \"strategy\" )[ col ] . mean () . sort_index () . plot ( kind = \"bar\" , ax = axs [ i ]) axs [ i ] . set_ylabel ( col ) Available helpers Our negotaitor was not so good but it examplifies the simplest method for developing a negotiator in NegMAS. For more information refer to NegMAS Documentation . You develop your agent, as explained above, by implementing the __call__ method of your class. This method, receives an SAOState which represents the current state of the negotiation. The most important members of this state object are current_offer which gives the current offer from the partner (or None if this is the beginning of the negotiation) and relative_time which gives the relative time of the negotiation ranging between 0 and 1 . It should return an SAOResponse represeting the agent's response which consists of two parts: A ResponseType with the following allowed values: ResponseType.ACCEPT_OFFER , accepts the current offer (pass the current offer as the second member of the response). ResponseType.REJECT_OFFER , rejects the current offer (pass you counter-offer as the second member of the response). ResponseType.END_NEGOTIATION , ends the negotiation immediately (pass None as the second member of the response). A counter offer (in case of rejection), the received offer (in case of acceptance) or None if ending the negotiation. The negotiator can use the following objects to help it implement its strategy: self.nmi A SAONMI that gives you access to all the settings of this negotiation and provide some simple helpers: n_steps , time_limit The number of rounds and seconds allowed for this negotiation ( None means no limit). random_outcomes(n) Samples n random outcomes from this negotiation. outcome_space The OutcomeSpace of the negotiation which represent all possible agreements. In ANL 2024, this will always be of type DiscreteCartesianOutcomeSpace with a single issue. discrete_outcomes() A generator of all outcomes in the outcome space. log_info() Logs structured information for this negotiator that can be checked in the logs later (Similarily there are log_error , log_warning , log_debug ). self.ufun A LinearAdditiveUtilityFunction representing the agent's own utility function. This object provides some helpful functionality including: self.ufun.is_better(a, b) Tests if outcome a is better than b (use None for disagreement). Similarily we have, is_worse , is_not_worse and is_not_better . self.ufun.reserved_value Your negotiator's reserved value (between 0 and 1). You can access this also as self.ufun(None) . self.ufun(w) Returns the utility value of the outcome w . It is recommended to cast this value to float (i.e. float(self.ufun(w) ) to support probabilistic utility functions. self.outcome_space The OutcomeSpace of the negotiation which represent all possible agreements. In ANL 2024, this will always be of type DiscreteCartesianOutcomeSpace with a single issue. self.ufun.invert() Returns and caches an InverseUtilityFunction object which can be used to find outcomes given their utilities. The most important services provided by the InverseUtilityFunction returned are: minmax() returns the minimum and maximum values of the ufun (will always be (0, 1) approximately in ANL 2024). best() , worst() returns the best (worst) outcomes. one_in() , some_in() returns one (or some) outcomes within the given range of utilities. next_better() , next_worse() returns the next outcome descendingly (ascendingly) in utility value. self.opponent_ufun A LinearAdditiveUtilityFunction representing the opponent's utility function. You can access this also as self.private_info[\"opponent_ufun\"] . This utility function will have a zero reserved value indepdendent of the opponent's true reserved value. You can actually set the reserved value on this object to your best estimate. All ufun funcationality is available in this object. Other than these objects, your negotiator can access any of the analytic facilities available in NegMAS. For example, you can calculate the pareto_frontier , Nash Bargaining Soluion , Kalai Bargaining Solution , points with maximum wellfare , etc. You can check the implementation of the NashSeeker agent for examples of using these facilities. Other than implementing the __call__ , method you can optionally implement one or more of the following callbacks to initialize your agent: on_negotiation_start(state: SAOState) This callback is called once per negotiation after the ufuns are set but before any offers are exchanged. on_preferences_changed(changes) This callback is called whenever your negotiator's ufun is changed. This will happen at the beginning of each negotiation but it can also happen again if the ufun is changed while the negotiation is running . In ANL 2024, ufuns never change during the negotiation so this callback is equivalent to on_negotiation_start() but for future proofness, you should use this callback for any initialization instead to guarantee that this initialization will be re-run in cases of changing utility function. Understanding our negotiator Now we can analyze the simple random negotiator we developed earlier. Firstly, we find the current offer that we need to respond to: offer = state . current_offer Acceptance Strategy We then accept this offer if three conditions are satisfied: The offer is not None which means that we are not starting the negotiation just now: The offer is not worse than disagreement. This prevents us from accepting irrational outcomes. A random number we generated is less than 0.25. This means we accept rational offers with probability 25%. if offer is not None and self . ufun . is_not_worse ( offer , None ) and random . random () < 0.25 : return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) Offering Strategy If we decided not to accept the offer, we simply generate a single random outcome and offer it: return SAOResponse ( ResponseType . REJECT_OFFER , self . nmi . random_outcomes ( 1 )[ 0 ]) This negotiator did not use the fact that we know the opponent utility function up to reserved value. It did not even use the fact that we know our own utility function. As expected, it did not get a good score. Let's develop a simple yet more meaningful agent that uses both of these pieces of information. Can you now see why is this negotiator is getting negative advantages sometimes? We were careful in our acceptance strategy but not in our offering strategy . There is nothing in our code that prevents our negotiator from offering irrational outcomes (i.e. outcomes worse than disagreement for itself) and sometimes the opponent will just accept those. Can you fix this? A more meaningful negotiator How can we use knowledge of our own and our opponent's utility functions (up to reserved value for them)? Here is one possibility: Acceptance Strategy We accept offers that have a utility above some aspiration level. This aspiration level starts very high (1.0) and goes monotoncially down but never under the reserved value which is reached when the relative time is 1.0 (i.e. by the end of the negotiation). This is implemented in is_acceptable() below. Opponent Modeling We estimate the opponent reserved value under the assumption that they are using a monotonically decreasing curve to select a utility value and offer an outcome around it. This is implemented in update_reserved_value() below. Bidding Strategy Once we have an estimate of their reserved value, we can then find out all outcomes that are rational for both we and them. We can then check the relative time of the negotiation and offer outcomes by conceding over this list of rational outcomes. This is implemented in the generate_offer() method below. from scipy.optimize import curve_fit def aspiration_function ( t , mx , rv , e ): \"\"\"A monotonically decreasing curve starting at mx (t=0) and ending at rv (t=1)\"\"\" return ( mx - rv ) * ( 1.0 - np . power ( t , e )) + rv class SimpleRVFitter ( SAONegotiator ): \"\"\"A simple curve fitting modeling agent\"\"\" def __init__ ( self , * args , e : float = 5.0 , ** kwargs ): \"\"\"Initialization\"\"\" super () . __init__ ( * args , ** kwargs ) self . e = e # keeps track of times at which the opponent offers self . opponent_times : list [ float ] = [] # keeps track of opponent utilities of its offers self . opponent_utilities : list [ float ] = [] # keeps track of the our last estimate of the opponent reserved value self . _past_oppnent_rv = 0.0 # keeps track of the rational outcome set given our estimate of the # opponent reserved value and our knowledge of ours self . _rational : list [ tuple [ float , float , Outcome ]] = [] def __call__ ( self , state ): # update the opponent reserved value in self.opponent_ufun self . update_reserved_value ( state . current_offer , state . relative_time ) # run the acceptance strategy and if the offer received is acceptable, accept it if self . is_acceptable ( state . current_offer , state . relative_time ): return SAOResponse ( ResponseType . ACCEPT_OFFER , state . current_offer ) # call the offering strategy return SAOResponse ( ResponseType . REJECT_OFFER , self . generate_offer ( state . relative_time )) def generate_offer ( self , relative_time ) -> Outcome : # The offering strategy # We only update our estimate of the rational list of outcomes if it is not set or # there is a change in estimated reserved value if ( not self . _rational or abs ( self . opponent_ufun . reserved_value - self . _past_oppnent_rv ) > 1e-3 ): # The rational set of outcomes sorted dependingly according to our utility function # and the opponent utility function (in that order). self . _rational = sorted ( [ ( my_util , opp_util , _ ) for _ in self . nmi . outcome_space . enumerate_or_sample ( levels = 10 , max_cardinality = 100_000 ) if ( my_util := float ( self . ufun ( _ ))) > self . ufun . reserved_value and ( opp_util := float ( self . opponent_ufun ( _ ))) > self . opponent_ufun . reserved_value ], ) # If there are no rational outcomes (e.g. our estimate of the opponent rv is very wrong), # then just revert to offering our top offer if not self . _rational : return self . ufun . best () # find our aspiration level (value between 0 and 1) the higher the higher utility we require asp = aspiration_function ( relative_time , 1.0 , 0.0 , self . e ) # find the index of the rational outcome at the aspiration level (in the rational set of outcomes) max_rational = len ( self . _rational ) - 1 indx = max ( 0 , min ( max_rational , int ( asp * max_rational ))) outcome = self . _rational [ indx ][ - 1 ] return outcome def is_acceptable ( self , offer , relative_time ) -> bool : \"\"\"The acceptance strategy\"\"\" # If there is no offer, there is nothing to accept if offer is None : return False # Find the current aspiration level asp = aspiration_function ( relative_time , 1.0 , self . ufun . reserved_value , self . e ) # accept if the utility of the received offer is higher than # the current aspiration return float ( self . ufun ( offer )) >= asp def update_reserved_value ( self , offer , relative_time ): \"\"\"Learns the reserved value of the partner\"\"\" if offer is None : return # save to the list of utilities received from the opponent and their times self . opponent_utilities . append ( float ( self . opponent_ufun ( offer ))) self . opponent_times . append ( relative_time ) # Use curve fitting to estimate the opponent reserved value # We assume the following: # - The opponent is using a concession strategy with an exponent between 0.2, 5.0 # - The opponent never offers outcomes lower than their reserved value which means # that their rv must be no higher than the worst outcome they offered for themselves. bounds = (( 0.2 , 0.0 ), ( 5.0 , min ( self . opponent_utilities ))) try : optimal_vals , _ = curve_fit ( lambda x , e , rv : aspiration_function ( x , self . opponent_utilities [ 0 ], rv , e ), self . opponent_times , self . opponent_utilities , bounds = bounds , ) self . _past_oppnent_rv = self . opponent_ufun . reserved_value self . opponent_ufun . reserved_value = optimal_vals [ 1 ] except Exception as e : pass anl2024_tournament ( n_scenarios = 1 , n_repetitions = 3 , nologs = True , njobs =- 1 , competitors = [ MyRandomNegotiator , SimpleRVFitter , Boulware , Conceder ] ) . final_scores Will run 48 negotiations on 1 scenarios between 4 competitors strategy score 0 SimpleRVFitter 0.795060 1 Boulware 0.793180 2 Conceder 0.434767 3 MyRandomNegotiator 0.144377 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } strategy score 0 SimpleRVFitter 0.795060 1 Boulware 0.793180 2 Conceder 0.434767 3 MyRandomNegotiator 0.144377 Much better :-) Let's see how each part of this negotiator works and how they fit together. Construction The first method of the negotiator to be called is the __init__ method which is called when the negotiator is created usually before the ufun is set . You can use this method to construct the negotiator setting initial values for any variables you need to run your agent. An important thing to note here is that your negotiator must pass any parameters it does not use to its parent to make sure the object is constructed correctly. This is how we implement this in our SimpleRVFitter : def __init__ ( self , * args , e : float = 5.0 , ** kwargs ): super () . __init__ ( * args , ** kwargs ) We then set the variables we need for our negotiator: self.e stores the exponent of the concession curve we will be use (more on that later). self.opponent_times , self.opponent_utilities keep track of the times the opponent offers and its own utility of its offers. We will use that to estimate the opponent's reserved value using simple curve fitting in update_reserved_values() . self._past_oppnent_rv = 0.0 We start assuming that the opponent has zero reserved value. This is an optimistic assumption because it means that anything rational for us is rational for the opponent so we have more negotiation power. self._rational This is where we will store the list of rational outcomes to concede over. For each outcome we will store our utility, opponent utility and the outcome itself (in that order). Overall Algorithm The overall algorithm is implemented --- as usual --- in the __call__() method. This is the complete algorithm: def __call__ ( self , state ): self . update_reserved_value ( state . current_offer , state . relative_time ) if self . is_acceptable ( state . current_offer , state . relative_time ): return SAOResponse ( ResponseType . ACCEPT_OFFER , state . current_offer ) return SAOResponse ( ResponseType . REJECT_OFFER , self . generate_offer ( state . relative_time )) We start by updating our estimate of the reserved value of the opponent using update_reserved_value() . We then call the acceptance strategy is_acceptable() to check whether the current offer should be accepted. If the current offer is not acceptable, we call the bidding strategy generate_offer() to generate a new offer which we return as our counter-offer. Simple!! Opponent Modling (Estimating Reserved Value) The first step is in our algorithm is to update our estimate of the opponent's reserved value. This is done in three simple steps: If we have not offer from the opponent, there is nothing to do. Just return: if offer is None : return We append the time and opponent's utility to our running list of opponent offer utilities: self . opponent_utilities . append ( float ( self . opponent_ufun ( offer ))) self . opponent_times . append ( relative_time ) We apply a simple curve fitting algorithm from scipy to estimate the opponent's reserved value (and its concession exponent but we are not going to use that): We set the bounds of the reserved value to be between zero (minimum possible value) and the minimum utility the opponent ever offered. This assumes that the opponent only offers rational outcomes for itself. The bounds for the concession curve are set to (0.2, 5.0) which is the usual range of exponents used by time-based strategies. bounds = (( 0.2 , 0.0 ), ( 5.0 , min ( self . opponent_utilities ))) We then just apply curve fitting while keeping the old estimate. We keep the old estimate to check whether there is enough change to warrent reevaluation of the rational outcome sets in our offering strategy. We ignore any errors keeping the old estimate in that case. optimal_vals , _ = curve_fit ( lambda x , e , rv : aspiration_function ( x , self . opponent_utilities [ 0 ], rv , e ), self . opponent_times , self . opponent_utilities , bounds = bounds ) Note that we just pass self.opponent_utilities[0] as the maximum for the concession curve because we know that this is the utility of the first offer from the opponent. Finally, we update the opponent reserved value with our new estimate keeping the latest value for later: self . _past_oppnent_rv = self . opponent_ufun . reserved_value self . opponent_ufun . reserved_value = optimal_vals [ 1 ] Acceptance Strategy Our acceptance strategy is implemented in is_acceptable() and consists of the following steps: Reject if no offer is found (i.e. we are starting the negotiation now): if offer is None : return False Find our current aspiration level which starts at 1.0 (inidicating we will only accept our best offer in the first step) ending at our reserved value (indicating that we are willing to accept any rational outcome by the end of the negotiation). Use the exponent we stored during construction. asp = aspiration_function ( state . relative_time , 1.0 , self . ufun . reserved_value , self . e ) Accept the offer iff its utility is higher than the aspiration level: return float ( self . ufun ( offer )) >= asp Note that this acceptance strategy does not use the estimated opponent reserved value (or the opponent's ufun) in any way. Bidding Strategy Now that we have updated our estimate of the opponent reserved value and decided not to accept their offer, we have to generate our own offer which the job of the bidding strategy implementedin generate_offer() . This is done in three steps as well: If the difference between the current and last estimate of the opponent reserved value is large enough, we create the rational outcome list. This test is implemented by: not self . _rational or abs ( self . opponent_ufun . reserved_value - self . _past_oppnent_rv ) > 1e-3 We then create of all outcomes prepending them with our and opponent's utility values: [ ( my_util , opp_util , _ ) for _ in self . nmi . outcome_space . enumerate_or_sample ( levels = 10 , max_cardinality = 100_000 ) if ( ( my_util := float ( self . ufun ( _ ))) > self . ufun . reserved_value and ( opp_util := float ( self . opponent_ufun ( _ ))) > self . opponent_ufun . reserved_value )] Finally, we sort this list. Because each element is a tuple, the list will be sorted ascendingly by our utility with equal values sorted ascendingly by the opponent utility. self . _rational = sorted ( ... ) If there are no rational outcomes (e.g. our estimate of the opponent rv is very wrong), then just revert to offering our top offer if not self . _rational : return self . ufun . best () If we have a rational set, we calculate an aspiration level that starts at 1 and ends at 0 (note that we do not need to end at the reserved value because all outcomes in self._rational are already no worse than disagreement. We then calculate the outcome that is at the current aspiration level from the end of the rational outcome list and offer it: asp = aspiration_function ( relative_time , 1.0 , 0.0 , self . e ) max_rational = len ( self . _rational ) - 1 indx = max ( 0 , min ( max_rational , int ( asp * max_rational ))) outcome = self . _rational [ indx ][ - 1 ] return outcome Other Examples The ANL package comes with some example negotiators. These are not designed to be stong but to showcase how to use some of the features provided by the platform. MiCRO A strong baseline behavioral negotiation strategy developed by de Jonge, Dave in \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\", ICJAI 2022. This strategy assumes no knowledge of the opponent utility function and is implemented from scratch to showcase the following: Using on_preferences_changed for initialization. Using PresortingInverseUtilityFunction for inverting a utility function. NashSeeker A naive strategy that simply sets the opponent reserved value to a fixed value and then uses helpers from NegMAS to find the Nash Bargaining Solution and use it for deciding what to offer. This showcases: Using NegMAS helpers to calculate the pareto-frontier and the Nash Bargaining Solution RVFitter A strategy very similar to the one we implemented earlier as SimpleRVFitter . Instead of trying to estiamte the opponent reserved value from the first step, this strategy waits until it collects few offers before attempting the etimation. This showcases: Setting the opponent reserved value based on our best estimate. A simple way to use this estimate for our bidding strategy. Boulware, Conceder, Linear Time-based strategies that are implemented by just setting construction parameters of an existing NegMAS negotiator StochasticBoulware, StochasticConceder, StochasticLinear Stochastic versions of the three time-based strategies above implemented by just setting construction parameters of an existing NegMAS negotiator NaiveTitForTat A simple behavioral strategy implemented by just inheriting from an existing NegMAS negotiator. Download Notebook","title":"Tutorial"},{"location":"tutorials/tutorial/#preparing-your-development-environment","text":"The first step is to install the anl package using: pip install anl The second step in developing your agent for ANL 2024 is to download the template from here . Please familiarize yourself with the competition rules availabe at the competition website . After downloading and uncompressing the template, you should do the following steps: Modify the name of the single class in myagent.py (currently called MyNegotiator ) to a representative name for your agent. We will use AwsomeNegotiator here. You should then implement your agent logic by modifying this class. Remember to change the name of the agent in the last line of the file to match your new class name ( AwsomeNegotiator ). Remember to update the agent class in the submission form on the competition website to AwsomeNegotiator . Start developing your agent as will be explained later in this tutorial You can use the following ways to test your agent: Run the following command to test your agent from the root folder of the extracted skeleton: python -m myagent.myagent Use the anl command line utility from the root folder of the extracted skeleton: anl tournament2024 --path = . --competitors = \"myagent.myagnet.AwsomeNegotiator;Boulware;Conceder\" This method is more flexible as you can control all aspects of the tournament to run. Use anl tournament2024 --help to see all available options. You can directly call anl2024_tournament() passing your agent as one of the competitors. This is the most flexible method and will be used in this tutorial.","title":"Preparing your development environment"},{"location":"tutorials/tutorial/#developing-a-negotiator","text":"Agents for the ANL competition are standard NegMAS negotiators. As such, they can be developed using any approach used to develop negotiators in NegMAS. To develop a negotiator, you need to inherit from the SAONegotiator class and implement the call method. Here is a simple random negotiator: import random from negmas.sao import SAONegotiator , SAOResponse from negmas import Outcome , ResponseType class MyRandomNegotiator ( SAONegotiator ): def __call__ ( self , state ): offer = state . current_offer if offer is not None and self . ufun . is_not_worse ( offer , None ) and random . random () < 0.25 : return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) return SAOResponse ( ResponseType . REJECT_OFFER , self . nmi . random_outcomes ( 1 )[ 0 ])","title":"Developing a negotiator"},{"location":"tutorials/tutorial/#testing-the-agent","text":"from anl.anl2024 import anl2024_tournament from anl.anl2024.negotiators import Boulware , Conceder , RVFitter results = anl2024_tournament ( n_scenarios = 1 , n_repetitions = 3 , nologs = True , njobs =- 1 , competitors = [ MyRandomNegotiator , Boulware ] ) Will run 12 negotiations on 1 scenarios between 2 competitors strategy score 0 Boulware 0.701151 1 MyRandomNegotiator 0.083134 We can immediately notice that MyRandomNegotiator is getting a negative average advantage which means that it sometimes gets agreements that are worse than disagreement (i.e. with utility less than its reserved value). Can you guess why is this happening? How can we resolve that? You can easily check the final scores using the final_scores member of the returned SimpleTournamentResults object results . final_scores .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } strategy score 0 Boulware 0.701151 1 MyRandomNegotiator 0.083134 The returned results are all pandas dataframes. We can use standard pandas functions to get deeper understanding of the results. Here is how to plot a KDE figure comparing different strategies in this tournament: fig , ax = plt . subplots ( figsize = ( 8 , 6 )) df = results . scores for label , data in df . groupby ( 'strategy' ): data . advantage . plot ( kind = \"kde\" , ax = ax , label = label ) plt . ylabel ( \"advantage\" ) plt . legend (); fig , axs = plt . subplots ( 1 , 3 , figsize = ( 16 , 4 )) for i , col in enumerate ([ \"advantage\" , \"welfare\" , \"nash_optimality\" ]): results . scores . groupby ( \"strategy\" )[ col ] . mean () . sort_index () . plot ( kind = \"bar\" , ax = axs [ i ]) axs [ i ] . set_ylabel ( col )","title":"Testing the agent"},{"location":"tutorials/tutorial/#available-helpers","text":"Our negotaitor was not so good but it examplifies the simplest method for developing a negotiator in NegMAS. For more information refer to NegMAS Documentation . You develop your agent, as explained above, by implementing the __call__ method of your class. This method, receives an SAOState which represents the current state of the negotiation. The most important members of this state object are current_offer which gives the current offer from the partner (or None if this is the beginning of the negotiation) and relative_time which gives the relative time of the negotiation ranging between 0 and 1 . It should return an SAOResponse represeting the agent's response which consists of two parts: A ResponseType with the following allowed values: ResponseType.ACCEPT_OFFER , accepts the current offer (pass the current offer as the second member of the response). ResponseType.REJECT_OFFER , rejects the current offer (pass you counter-offer as the second member of the response). ResponseType.END_NEGOTIATION , ends the negotiation immediately (pass None as the second member of the response). A counter offer (in case of rejection), the received offer (in case of acceptance) or None if ending the negotiation. The negotiator can use the following objects to help it implement its strategy: self.nmi A SAONMI that gives you access to all the settings of this negotiation and provide some simple helpers: n_steps , time_limit The number of rounds and seconds allowed for this negotiation ( None means no limit). random_outcomes(n) Samples n random outcomes from this negotiation. outcome_space The OutcomeSpace of the negotiation which represent all possible agreements. In ANL 2024, this will always be of type DiscreteCartesianOutcomeSpace with a single issue. discrete_outcomes() A generator of all outcomes in the outcome space. log_info() Logs structured information for this negotiator that can be checked in the logs later (Similarily there are log_error , log_warning , log_debug ). self.ufun A LinearAdditiveUtilityFunction representing the agent's own utility function. This object provides some helpful functionality including: self.ufun.is_better(a, b) Tests if outcome a is better than b (use None for disagreement). Similarily we have, is_worse , is_not_worse and is_not_better . self.ufun.reserved_value Your negotiator's reserved value (between 0 and 1). You can access this also as self.ufun(None) . self.ufun(w) Returns the utility value of the outcome w . It is recommended to cast this value to float (i.e. float(self.ufun(w) ) to support probabilistic utility functions. self.outcome_space The OutcomeSpace of the negotiation which represent all possible agreements. In ANL 2024, this will always be of type DiscreteCartesianOutcomeSpace with a single issue. self.ufun.invert() Returns and caches an InverseUtilityFunction object which can be used to find outcomes given their utilities. The most important services provided by the InverseUtilityFunction returned are: minmax() returns the minimum and maximum values of the ufun (will always be (0, 1) approximately in ANL 2024). best() , worst() returns the best (worst) outcomes. one_in() , some_in() returns one (or some) outcomes within the given range of utilities. next_better() , next_worse() returns the next outcome descendingly (ascendingly) in utility value. self.opponent_ufun A LinearAdditiveUtilityFunction representing the opponent's utility function. You can access this also as self.private_info[\"opponent_ufun\"] . This utility function will have a zero reserved value indepdendent of the opponent's true reserved value. You can actually set the reserved value on this object to your best estimate. All ufun funcationality is available in this object. Other than these objects, your negotiator can access any of the analytic facilities available in NegMAS. For example, you can calculate the pareto_frontier , Nash Bargaining Soluion , Kalai Bargaining Solution , points with maximum wellfare , etc. You can check the implementation of the NashSeeker agent for examples of using these facilities. Other than implementing the __call__ , method you can optionally implement one or more of the following callbacks to initialize your agent: on_negotiation_start(state: SAOState) This callback is called once per negotiation after the ufuns are set but before any offers are exchanged. on_preferences_changed(changes) This callback is called whenever your negotiator's ufun is changed. This will happen at the beginning of each negotiation but it can also happen again if the ufun is changed while the negotiation is running . In ANL 2024, ufuns never change during the negotiation so this callback is equivalent to on_negotiation_start() but for future proofness, you should use this callback for any initialization instead to guarantee that this initialization will be re-run in cases of changing utility function.","title":"Available helpers"},{"location":"tutorials/tutorial/#understanding-our-negotiator","text":"Now we can analyze the simple random negotiator we developed earlier. Firstly, we find the current offer that we need to respond to: offer = state . current_offer Acceptance Strategy We then accept this offer if three conditions are satisfied: The offer is not None which means that we are not starting the negotiation just now: The offer is not worse than disagreement. This prevents us from accepting irrational outcomes. A random number we generated is less than 0.25. This means we accept rational offers with probability 25%. if offer is not None and self . ufun . is_not_worse ( offer , None ) and random . random () < 0.25 : return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) Offering Strategy If we decided not to accept the offer, we simply generate a single random outcome and offer it: return SAOResponse ( ResponseType . REJECT_OFFER , self . nmi . random_outcomes ( 1 )[ 0 ]) This negotiator did not use the fact that we know the opponent utility function up to reserved value. It did not even use the fact that we know our own utility function. As expected, it did not get a good score. Let's develop a simple yet more meaningful agent that uses both of these pieces of information. Can you now see why is this negotiator is getting negative advantages sometimes? We were careful in our acceptance strategy but not in our offering strategy . There is nothing in our code that prevents our negotiator from offering irrational outcomes (i.e. outcomes worse than disagreement for itself) and sometimes the opponent will just accept those. Can you fix this?","title":"Understanding our negotiator"},{"location":"tutorials/tutorial/#a-more-meaningful-negotiator","text":"How can we use knowledge of our own and our opponent's utility functions (up to reserved value for them)? Here is one possibility: Acceptance Strategy We accept offers that have a utility above some aspiration level. This aspiration level starts very high (1.0) and goes monotoncially down but never under the reserved value which is reached when the relative time is 1.0 (i.e. by the end of the negotiation). This is implemented in is_acceptable() below. Opponent Modeling We estimate the opponent reserved value under the assumption that they are using a monotonically decreasing curve to select a utility value and offer an outcome around it. This is implemented in update_reserved_value() below. Bidding Strategy Once we have an estimate of their reserved value, we can then find out all outcomes that are rational for both we and them. We can then check the relative time of the negotiation and offer outcomes by conceding over this list of rational outcomes. This is implemented in the generate_offer() method below. from scipy.optimize import curve_fit def aspiration_function ( t , mx , rv , e ): \"\"\"A monotonically decreasing curve starting at mx (t=0) and ending at rv (t=1)\"\"\" return ( mx - rv ) * ( 1.0 - np . power ( t , e )) + rv class SimpleRVFitter ( SAONegotiator ): \"\"\"A simple curve fitting modeling agent\"\"\" def __init__ ( self , * args , e : float = 5.0 , ** kwargs ): \"\"\"Initialization\"\"\" super () . __init__ ( * args , ** kwargs ) self . e = e # keeps track of times at which the opponent offers self . opponent_times : list [ float ] = [] # keeps track of opponent utilities of its offers self . opponent_utilities : list [ float ] = [] # keeps track of the our last estimate of the opponent reserved value self . _past_oppnent_rv = 0.0 # keeps track of the rational outcome set given our estimate of the # opponent reserved value and our knowledge of ours self . _rational : list [ tuple [ float , float , Outcome ]] = [] def __call__ ( self , state ): # update the opponent reserved value in self.opponent_ufun self . update_reserved_value ( state . current_offer , state . relative_time ) # run the acceptance strategy and if the offer received is acceptable, accept it if self . is_acceptable ( state . current_offer , state . relative_time ): return SAOResponse ( ResponseType . ACCEPT_OFFER , state . current_offer ) # call the offering strategy return SAOResponse ( ResponseType . REJECT_OFFER , self . generate_offer ( state . relative_time )) def generate_offer ( self , relative_time ) -> Outcome : # The offering strategy # We only update our estimate of the rational list of outcomes if it is not set or # there is a change in estimated reserved value if ( not self . _rational or abs ( self . opponent_ufun . reserved_value - self . _past_oppnent_rv ) > 1e-3 ): # The rational set of outcomes sorted dependingly according to our utility function # and the opponent utility function (in that order). self . _rational = sorted ( [ ( my_util , opp_util , _ ) for _ in self . nmi . outcome_space . enumerate_or_sample ( levels = 10 , max_cardinality = 100_000 ) if ( my_util := float ( self . ufun ( _ ))) > self . ufun . reserved_value and ( opp_util := float ( self . opponent_ufun ( _ ))) > self . opponent_ufun . reserved_value ], ) # If there are no rational outcomes (e.g. our estimate of the opponent rv is very wrong), # then just revert to offering our top offer if not self . _rational : return self . ufun . best () # find our aspiration level (value between 0 and 1) the higher the higher utility we require asp = aspiration_function ( relative_time , 1.0 , 0.0 , self . e ) # find the index of the rational outcome at the aspiration level (in the rational set of outcomes) max_rational = len ( self . _rational ) - 1 indx = max ( 0 , min ( max_rational , int ( asp * max_rational ))) outcome = self . _rational [ indx ][ - 1 ] return outcome def is_acceptable ( self , offer , relative_time ) -> bool : \"\"\"The acceptance strategy\"\"\" # If there is no offer, there is nothing to accept if offer is None : return False # Find the current aspiration level asp = aspiration_function ( relative_time , 1.0 , self . ufun . reserved_value , self . e ) # accept if the utility of the received offer is higher than # the current aspiration return float ( self . ufun ( offer )) >= asp def update_reserved_value ( self , offer , relative_time ): \"\"\"Learns the reserved value of the partner\"\"\" if offer is None : return # save to the list of utilities received from the opponent and their times self . opponent_utilities . append ( float ( self . opponent_ufun ( offer ))) self . opponent_times . append ( relative_time ) # Use curve fitting to estimate the opponent reserved value # We assume the following: # - The opponent is using a concession strategy with an exponent between 0.2, 5.0 # - The opponent never offers outcomes lower than their reserved value which means # that their rv must be no higher than the worst outcome they offered for themselves. bounds = (( 0.2 , 0.0 ), ( 5.0 , min ( self . opponent_utilities ))) try : optimal_vals , _ = curve_fit ( lambda x , e , rv : aspiration_function ( x , self . opponent_utilities [ 0 ], rv , e ), self . opponent_times , self . opponent_utilities , bounds = bounds , ) self . _past_oppnent_rv = self . opponent_ufun . reserved_value self . opponent_ufun . reserved_value = optimal_vals [ 1 ] except Exception as e : pass anl2024_tournament ( n_scenarios = 1 , n_repetitions = 3 , nologs = True , njobs =- 1 , competitors = [ MyRandomNegotiator , SimpleRVFitter , Boulware , Conceder ] ) . final_scores Will run 48 negotiations on 1 scenarios between 4 competitors strategy score 0 SimpleRVFitter 0.795060 1 Boulware 0.793180 2 Conceder 0.434767 3 MyRandomNegotiator 0.144377 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } strategy score 0 SimpleRVFitter 0.795060 1 Boulware 0.793180 2 Conceder 0.434767 3 MyRandomNegotiator 0.144377 Much better :-) Let's see how each part of this negotiator works and how they fit together.","title":"A more meaningful negotiator"},{"location":"tutorials/tutorial/#construction","text":"The first method of the negotiator to be called is the __init__ method which is called when the negotiator is created usually before the ufun is set . You can use this method to construct the negotiator setting initial values for any variables you need to run your agent. An important thing to note here is that your negotiator must pass any parameters it does not use to its parent to make sure the object is constructed correctly. This is how we implement this in our SimpleRVFitter : def __init__ ( self , * args , e : float = 5.0 , ** kwargs ): super () . __init__ ( * args , ** kwargs ) We then set the variables we need for our negotiator: self.e stores the exponent of the concession curve we will be use (more on that later). self.opponent_times , self.opponent_utilities keep track of the times the opponent offers and its own utility of its offers. We will use that to estimate the opponent's reserved value using simple curve fitting in update_reserved_values() . self._past_oppnent_rv = 0.0 We start assuming that the opponent has zero reserved value. This is an optimistic assumption because it means that anything rational for us is rational for the opponent so we have more negotiation power. self._rational This is where we will store the list of rational outcomes to concede over. For each outcome we will store our utility, opponent utility and the outcome itself (in that order).","title":"Construction"},{"location":"tutorials/tutorial/#overall-algorithm","text":"The overall algorithm is implemented --- as usual --- in the __call__() method. This is the complete algorithm: def __call__ ( self , state ): self . update_reserved_value ( state . current_offer , state . relative_time ) if self . is_acceptable ( state . current_offer , state . relative_time ): return SAOResponse ( ResponseType . ACCEPT_OFFER , state . current_offer ) return SAOResponse ( ResponseType . REJECT_OFFER , self . generate_offer ( state . relative_time )) We start by updating our estimate of the reserved value of the opponent using update_reserved_value() . We then call the acceptance strategy is_acceptable() to check whether the current offer should be accepted. If the current offer is not acceptable, we call the bidding strategy generate_offer() to generate a new offer which we return as our counter-offer. Simple!!","title":"Overall Algorithm"},{"location":"tutorials/tutorial/#opponent-modling-estimating-reserved-value","text":"The first step is in our algorithm is to update our estimate of the opponent's reserved value. This is done in three simple steps: If we have not offer from the opponent, there is nothing to do. Just return: if offer is None : return We append the time and opponent's utility to our running list of opponent offer utilities: self . opponent_utilities . append ( float ( self . opponent_ufun ( offer ))) self . opponent_times . append ( relative_time ) We apply a simple curve fitting algorithm from scipy to estimate the opponent's reserved value (and its concession exponent but we are not going to use that): We set the bounds of the reserved value to be between zero (minimum possible value) and the minimum utility the opponent ever offered. This assumes that the opponent only offers rational outcomes for itself. The bounds for the concession curve are set to (0.2, 5.0) which is the usual range of exponents used by time-based strategies. bounds = (( 0.2 , 0.0 ), ( 5.0 , min ( self . opponent_utilities ))) We then just apply curve fitting while keeping the old estimate. We keep the old estimate to check whether there is enough change to warrent reevaluation of the rational outcome sets in our offering strategy. We ignore any errors keeping the old estimate in that case. optimal_vals , _ = curve_fit ( lambda x , e , rv : aspiration_function ( x , self . opponent_utilities [ 0 ], rv , e ), self . opponent_times , self . opponent_utilities , bounds = bounds ) Note that we just pass self.opponent_utilities[0] as the maximum for the concession curve because we know that this is the utility of the first offer from the opponent. Finally, we update the opponent reserved value with our new estimate keeping the latest value for later: self . _past_oppnent_rv = self . opponent_ufun . reserved_value self . opponent_ufun . reserved_value = optimal_vals [ 1 ]","title":"Opponent Modling (Estimating Reserved Value)"},{"location":"tutorials/tutorial/#acceptance-strategy","text":"Our acceptance strategy is implemented in is_acceptable() and consists of the following steps: Reject if no offer is found (i.e. we are starting the negotiation now): if offer is None : return False Find our current aspiration level which starts at 1.0 (inidicating we will only accept our best offer in the first step) ending at our reserved value (indicating that we are willing to accept any rational outcome by the end of the negotiation). Use the exponent we stored during construction. asp = aspiration_function ( state . relative_time , 1.0 , self . ufun . reserved_value , self . e ) Accept the offer iff its utility is higher than the aspiration level: return float ( self . ufun ( offer )) >= asp Note that this acceptance strategy does not use the estimated opponent reserved value (or the opponent's ufun) in any way.","title":"Acceptance Strategy"},{"location":"tutorials/tutorial/#bidding-strategy","text":"Now that we have updated our estimate of the opponent reserved value and decided not to accept their offer, we have to generate our own offer which the job of the bidding strategy implementedin generate_offer() . This is done in three steps as well: If the difference between the current and last estimate of the opponent reserved value is large enough, we create the rational outcome list. This test is implemented by: not self . _rational or abs ( self . opponent_ufun . reserved_value - self . _past_oppnent_rv ) > 1e-3 We then create of all outcomes prepending them with our and opponent's utility values: [ ( my_util , opp_util , _ ) for _ in self . nmi . outcome_space . enumerate_or_sample ( levels = 10 , max_cardinality = 100_000 ) if ( ( my_util := float ( self . ufun ( _ ))) > self . ufun . reserved_value and ( opp_util := float ( self . opponent_ufun ( _ ))) > self . opponent_ufun . reserved_value )] Finally, we sort this list. Because each element is a tuple, the list will be sorted ascendingly by our utility with equal values sorted ascendingly by the opponent utility. self . _rational = sorted ( ... ) If there are no rational outcomes (e.g. our estimate of the opponent rv is very wrong), then just revert to offering our top offer if not self . _rational : return self . ufun . best () If we have a rational set, we calculate an aspiration level that starts at 1 and ends at 0 (note that we do not need to end at the reserved value because all outcomes in self._rational are already no worse than disagreement. We then calculate the outcome that is at the current aspiration level from the end of the rational outcome list and offer it: asp = aspiration_function ( relative_time , 1.0 , 0.0 , self . e ) max_rational = len ( self . _rational ) - 1 indx = max ( 0 , min ( max_rational , int ( asp * max_rational ))) outcome = self . _rational [ indx ][ - 1 ] return outcome","title":"Bidding Strategy"},{"location":"tutorials/tutorial/#other-examples","text":"The ANL package comes with some example negotiators. These are not designed to be stong but to showcase how to use some of the features provided by the platform. MiCRO A strong baseline behavioral negotiation strategy developed by de Jonge, Dave in \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\", ICJAI 2022. This strategy assumes no knowledge of the opponent utility function and is implemented from scratch to showcase the following: Using on_preferences_changed for initialization. Using PresortingInverseUtilityFunction for inverting a utility function. NashSeeker A naive strategy that simply sets the opponent reserved value to a fixed value and then uses helpers from NegMAS to find the Nash Bargaining Solution and use it for deciding what to offer. This showcases: Using NegMAS helpers to calculate the pareto-frontier and the Nash Bargaining Solution RVFitter A strategy very similar to the one we implemented earlier as SimpleRVFitter . Instead of trying to estiamte the opponent reserved value from the first step, this strategy waits until it collects few offers before attempting the etimation. This showcases: Setting the opponent reserved value based on our best estimate. A simple way to use this estimate for our bidding strategy. Boulware, Conceder, Linear Time-based strategies that are implemented by just setting construction parameters of an existing NegMAS negotiator StochasticBoulware, StochasticConceder, StochasticLinear Stochastic versions of the three time-based strategies above implemented by just setting construction parameters of an existing NegMAS negotiator NaiveTitForTat A simple behavioral strategy implemented by just inheriting from an existing NegMAS negotiator. Download Notebook","title":"Other Examples"}]}