{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ANL Documentation This repository is the official platform for running ANAC Automated Negotiation Leagues (starting 2024). It will contain a package called anlXXXX for the competition run in year XXXX. For example anl2024 will contain all files related to the 2024's version of the competition. This package is a thin-wrapper around the NegMAS library for automated negotiation. Its main goal is to provide the following functionalities: A set of utility functions to run tournaments in the same settings as in the ANL competition. These functions are always called anl20XX_tournament for year 20XX . A CLI for running tournaments called anl . A place to hold the official implementation of every strategy submitted to the ANL competition after each year. These can be found in the module anl.anl20XX.negotiators for year 20XX . The official website for the ANL competition is: https://scml.cs.brown.edu/anl Installation pip install anl You can also install the in-development version with:: pip install https://github.com/autoneg/anl/archive/master.zip Documentation Documentation for the ANL package: https://yasserfarouk.github.io/anl/ Documentation for the NegMAS library: https://negmas.readthedocs.io CLI After installation, you can try running a tournament using the CLI as: anl tournament2024 To find all the parameters you can customize for running tournaments run: anl tournament2024 --help You can run the following command to check the versions of ANL and NegMAS on your machine: anl version You should get at least these versions: anl: 0.1.3 (NegMAS: 0.10.8)","title":"Index"},{"location":"#anl-documentation","text":"This repository is the official platform for running ANAC Automated Negotiation Leagues (starting 2024). It will contain a package called anlXXXX for the competition run in year XXXX. For example anl2024 will contain all files related to the 2024's version of the competition. This package is a thin-wrapper around the NegMAS library for automated negotiation. Its main goal is to provide the following functionalities: A set of utility functions to run tournaments in the same settings as in the ANL competition. These functions are always called anl20XX_tournament for year 20XX . A CLI for running tournaments called anl . A place to hold the official implementation of every strategy submitted to the ANL competition after each year. These can be found in the module anl.anl20XX.negotiators for year 20XX . The official website for the ANL competition is: https://scml.cs.brown.edu/anl","title":"ANL Documentation"},{"location":"#installation","text":"pip install anl You can also install the in-development version with:: pip install https://github.com/autoneg/anl/archive/master.zip","title":"Installation"},{"location":"#documentation","text":"Documentation for the ANL package: https://yasserfarouk.github.io/anl/ Documentation for the NegMAS library: https://negmas.readthedocs.io","title":"Documentation"},{"location":"#cli","text":"After installation, you can try running a tournament using the CLI as: anl tournament2024 To find all the parameters you can customize for running tournaments run: anl tournament2024 --help You can run the following command to check the versions of ANL and NegMAS on your machine: anl version You should get at least these versions: anl: 0.1.3 (NegMAS: 0.10.8)","title":"CLI"},{"location":"faq/","text":"FAQ How can I access a data file in my package? When your agent is submitted, it is run in an environment different from that in which the tournament will be run. This means that you cannot use hardcoded paths in your agent. Moreover, you (and we) do not know in advance what will be the current directory when the tournament is run. For this reason, it is required that if you access any files in your agent, you should use a path relative to the file in which the code accessing these files is located. Please note that accessing ANY FILES outside the directory of your agent is prohibited and will lead to immediate disqualification for obvious security reasons. There are no second chances in this one. Let's assume that your file structure is something like that: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 myagent.py \u2502 \u2514\u2500\u2500 otherfiles.py \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 myfile.csv \u2514\u2500\u2500 tests Now you want to access the file myfile.csv when you are inside myagent.py . To do so you can use the following code:: import pathlib path_2_myfile = pathlib.Path(__file__).parent.parent / \"data\" / \"myfile.csv\" Can my agent pass data to my other agents between negotiations? NO Passing data to your agents between negotiations will lead to disqualification. Can my agent read data from the HDD outside my agent's folder? NO Your agent can only read files that you submitted to us in your zip file. It cannot modify these files in anyway during the competition. It cannot read from anywhere else in secondary storage. Trying to do so will lead to disqualification. Can my agent write data to the HDD during the negotiation? NO The agent is not allowed to write anything to the hard disk during the competition. Can I print to the screen to debug my agent? PLEASE DO NOT Printing to the screen in your agent will prevent us from monitoring the progress of tournament runs and will slow down the process. Moreover, it is not useful anyway because the tournaments are run in parallel. If you really need to print something (e.g. for debugging purposes), please remove all print statements before submission. We will never touch your code after submission so we cannot remove them. Can I write arbitrary code in my module besides the negotiator class definition? When python imports your module, it runs everything in it so the top level code should be only one of these: - Class definitions - Function definitions - Variable definitions - Arbitrary code that runs in few milliseconds and prints nothing Any other code must be protected inside:: if __name__ == \"__main__\" For example, if you want to run a simulation to test your agent. DO NOT USE SOMETHING LIKE THIS :: anl2024_tournament(....) But something like this:: def main(): anl2024_tournament(....) if __name__ == \"__main__\": main() This way, importing your module will not run the world simulation. I ran a simulation using \"anl tournament2024\" command. Where are my log files? If you did not pass \"--no-log\", you will find the log files at ~/negmas/anl2024/[date-time-uuid] I implement my agent using multiple files. How should I import them? Assume that you have the following file structure base \u251c\u2500\u2500 subfolder \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py In your agent.py file, you want to import your other files:: import component1 import subfolder.component2 This will not work because in the actual competition component1.py and component2.py will not be in python path. There are two ways to solve it: The clean way is to use relative imports. You will need to turn your agent int a package by adding empty __init__.py files to every folder you want to import from:: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py You can now change your import to:: import .component1 import .subfolder.component2 Notice the extra dot ( . ) Another way that does not require any modification of your file structure is to add the following lines before your imports:: import os, sys sys.path.append(os.path.dirname(__file__)) Note that the later method has the disadvantage of putting your components at the end of the path which means that if you have any classes, functions, etc with a name that is defined in any module that appears earlier in the path, yours will be hidden.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-can-i-access-a-data-file-in-my-package","text":"When your agent is submitted, it is run in an environment different from that in which the tournament will be run. This means that you cannot use hardcoded paths in your agent. Moreover, you (and we) do not know in advance what will be the current directory when the tournament is run. For this reason, it is required that if you access any files in your agent, you should use a path relative to the file in which the code accessing these files is located. Please note that accessing ANY FILES outside the directory of your agent is prohibited and will lead to immediate disqualification for obvious security reasons. There are no second chances in this one. Let's assume that your file structure is something like that: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 myagent.py \u2502 \u2514\u2500\u2500 otherfiles.py \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 myfile.csv \u2514\u2500\u2500 tests Now you want to access the file myfile.csv when you are inside myagent.py . To do so you can use the following code:: import pathlib path_2_myfile = pathlib.Path(__file__).parent.parent / \"data\" / \"myfile.csv\"","title":"How can I access a data file in my package?"},{"location":"faq/#can-my-agent-pass-data-to-my-other-agents-between-negotiations","text":"NO Passing data to your agents between negotiations will lead to disqualification.","title":"Can my agent pass data to my other agents between negotiations?"},{"location":"faq/#can-my-agent-read-data-from-the-hdd-outside-my-agents-folder","text":"NO Your agent can only read files that you submitted to us in your zip file. It cannot modify these files in anyway during the competition. It cannot read from anywhere else in secondary storage. Trying to do so will lead to disqualification.","title":"Can my agent read data from the HDD outside my agent's folder?"},{"location":"faq/#can-my-agent-write-data-to-the-hdd-during-the-negotiation","text":"NO The agent is not allowed to write anything to the hard disk during the competition.","title":"Can my agent write data to the HDD during the negotiation?"},{"location":"faq/#can-i-print-to-the-screen-to-debug-my-agent","text":"PLEASE DO NOT Printing to the screen in your agent will prevent us from monitoring the progress of tournament runs and will slow down the process. Moreover, it is not useful anyway because the tournaments are run in parallel. If you really need to print something (e.g. for debugging purposes), please remove all print statements before submission. We will never touch your code after submission so we cannot remove them.","title":"Can I print to the screen to debug my agent?"},{"location":"faq/#can-i-write-arbitrary-code-in-my-module-besides-the-negotiator-class-definition","text":"When python imports your module, it runs everything in it so the top level code should be only one of these: - Class definitions - Function definitions - Variable definitions - Arbitrary code that runs in few milliseconds and prints nothing Any other code must be protected inside:: if __name__ == \"__main__\" For example, if you want to run a simulation to test your agent. DO NOT USE SOMETHING LIKE THIS :: anl2024_tournament(....) But something like this:: def main(): anl2024_tournament(....) if __name__ == \"__main__\": main() This way, importing your module will not run the world simulation.","title":"Can I write arbitrary code in my module besides the negotiator class definition?"},{"location":"faq/#i-ran-a-simulation-using-anl-tournament2024-command-where-are-my-log-files","text":"If you did not pass \"--no-log\", you will find the log files at ~/negmas/anl2024/[date-time-uuid]","title":"I ran a simulation using \"anl tournament2024\" command. Where are my log files?"},{"location":"faq/#i-implement-my-agent-using-multiple-files-how-should-i-import-them","text":"Assume that you have the following file structure base \u251c\u2500\u2500 subfolder \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py In your agent.py file, you want to import your other files:: import component1 import subfolder.component2 This will not work because in the actual competition component1.py and component2.py will not be in python path. There are two ways to solve it: The clean way is to use relative imports. You will need to turn your agent int a package by adding empty __init__.py files to every folder you want to import from:: base \u251c\u2500\u2500 sub \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 component2.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 component1.py \u2514\u2500\u2500 agent.py You can now change your import to:: import .component1 import .subfolder.component2 Notice the extra dot ( . ) Another way that does not require any modification of your file structure is to add the following lines before your imports:: import os, sys sys.path.append(os.path.dirname(__file__)) Note that the later method has the disadvantage of putting your components at the end of the path which means that if you have any classes, functions, etc with a name that is defined in any module that appears earlier in the path, yours will be hidden.","title":"I implement my agent using multiple files. How should I import them?"},{"location":"reference/","text":"ANL This package provides a wrapper around NegMAS functionality to generate and run tournaments a la ANL 2024 competition. You mostly only need to use anl2024_tournament in your code. The other helpers are provided to allow for a finer control over the scenarios used. Tournaments function: anl2024_tournament Runs an ANL 2024 tournament Parameters: Name Type Description Default n_scenarios int Number of negotiation scenarios DEFAULT2024SETTINGS ['n_scenarios'] n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes (or a min/max tuple of n. outcomes) for each scenario DEFAULT2024SETTINGS ['n_outcomes'] competitors tuple [ type [ Negotiator ] | str , ...] | list [ type [ Negotiator ] | str ] list of competitor agents DEFAULT_AN2024_COMPETITORS competitor_params Sequence [ dict | None] | None If given, parameters to construct each competitor None rotate_ufuns bool If given, each scenario will be tried with both orders of the ufuns. DEFAULT2024SETTINGS ['rotate_ufuns'] n_repetitions int Number of times to repeat each negotiation DEFAULT2024SETTINGS ['n_repetitions'] n_steps int | tuple [ int , int ] | None Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['n_steps'] time_limit float | tuple [ float , float ] | None Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['time_limit'] pend float | tuple [ float , float ] Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['pend'] pend_per_second float | tuple [ float , float ] Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['pend_per_second'] step_time_limit float | tuple [ float , float ] | None Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['step_time_limit'] negotiator_time_limit float | tuple [ float , float ] | None Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['negotiator_time_limit'] name str | None Name of the tournament None nologs bool If given, no logs will be saved False njobs int Number of parallel jobs to use. -1 for serial and 0 for all cores 0 plot_fraction float Fraction of negotiations to plot. Only used if not nologs 0.2 verbosity int Verbosity level. The higher the more verbose 1 self_play bool Allow negotiators to run against themselves. DEFAULT2024SETTINGS ['self_play'] randomize_runs bool Randomize the order of negotiations DEFAULT2024SETTINGS ['randomize_runs'] save_every int Save logs every this number of negotiations 0 save_stats bool Save statistics for scenarios True known_partner bool Allow negotiators to know the type of their partner (through their ID) DEFAULT2024SETTINGS ['known_partner'] final_score tuple [ str , str ] The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max DEFAULT2024SETTINGS ['final_score'] base_path Path | None Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments None scenario_generator str | ScenarioGenerator An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. DEFAULT2024SETTINGS ['scenario_generator'] generator_params dict [ str , Any ] | None Parameters passed to the scenario generator DEFAULT2024SETTINGS ['generator_params'] plot_params dict [ str , Any ] | None If given, overrides plotting parameters. See nemgas.sao.SAOMechanism.plot() for all parameters None Returns: Type Description SimpleTournamentResults Tournament results as a SimpleTournamentResults object. Source code in anl/anl2024/runner.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 def anl2024_tournament ( n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore competitors : tuple [ type [ Negotiator ] | str , ... ] | list [ type [ Negotiator ] | str ] = DEFAULT_AN2024_COMPETITORS , rotate_ufuns : bool = DEFAULT2024SETTINGS [ \"rotate_ufuns\" ], # type: ignore n_repetitions : int = DEFAULT2024SETTINGS [ \"n_repetitions\" ], # type: ignore n_steps : int | tuple [ int , int ] | None = DEFAULT2024SETTINGS [ \"n_steps\" ], # type: ignore time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"time_limit\" ], # type: ignore pend : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend\" ], # type: ignore pend_per_second : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend_per_second\" ], # type: ignore step_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"step_time_limit\" ], # type: ignore negotiator_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"negotiator_time_limit\" ], # type: ignore self_play : bool = DEFAULT2024SETTINGS [ \"self_play\" ], # type: ignore randomize_runs : bool = DEFAULT2024SETTINGS [ \"randomize_runs\" ], # type: ignore known_partner : bool = DEFAULT2024SETTINGS [ \"known_partner\" ], # type: ignore final_score : tuple [ str , str ] = DEFAULT2024SETTINGS [ \"final_score\" ], # type: ignore scenario_generator : str | ScenarioGenerator = DEFAULT2024SETTINGS [ \"scenario_generator\" ], # type: ignore generator_params : dict [ str , Any ] | None = DEFAULT2024SETTINGS [ \"generator_params\" ], # type: ignore competitor_params : Sequence [ dict | None ] | None = None , name : str | None = None , nologs : bool = False , njobs : int = 0 , plot_fraction : float = 0.2 , verbosity : int = 1 , save_every : int = 0 , save_stats : bool = True , base_path : Path | None = None , plot_params : dict [ str , Any ] | None = None , raise_exceptions : bool = True , ) -> SimpleTournamentResults : \"\"\"Runs an ANL 2024 tournament Args: n_scenarios: Number of negotiation scenarios n_outcomes: Number of outcomes (or a min/max tuple of n. outcomes) for each scenario competitors: list of competitor agents competitor_params: If given, parameters to construct each competitor rotate_ufuns: If given, each scenario will be tried with both orders of the ufuns. n_repetitions: Number of times to repeat each negotiation n_steps: Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) time_limit: Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) pend: Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) pend_per_second: Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) step_time_limit: Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) negotiator_time_limit: Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) name: Name of the tournament nologs: If given, no logs will be saved njobs: Number of parallel jobs to use. -1 for serial and 0 for all cores plot_fraction: Fraction of negotiations to plot. Only used if not nologs verbosity: Verbosity level. The higher the more verbose self_play: Allow negotiators to run against themselves. randomize_runs: Randomize the order of negotiations save_every: Save logs every this number of negotiations save_stats: Save statistics for scenarios known_partner: Allow negotiators to know the type of their partner (through their ID) final_score: The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max base_path: Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments scenario_generator: An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. generator_params: Parameters passed to the scenario generator plot_params: If given, overrides plotting parameters. See `nemgas.sao.SAOMechanism.plot()` for all parameters Returns: Tournament results as a `SimpleTournamentResults` object. \"\"\" if generator_params is None : generator_params = dict () if isinstance ( scenario_generator , str ): scenario_generator = GENERAROR_MAP [ scenario_generator ] all_outcomes = not scenario_generator == zerosum_pie_scenarios if nologs : path = None elif base_path is not None : path = Path ( base_path ) / ( name if name else unique_name ( \"anl\" )) else : path = DEFAULT_TOURNAMENT_PATH / ( name if name else unique_name ( \"anl\" )) params = dict ( ylimits = ( 0 , 1 ), mark_offers_view = True , mark_pareto_points = all_outcomes , mark_all_outcomes = all_outcomes , mark_nash_points = True , mark_kalai_points = all_outcomes , mark_max_welfare_points = all_outcomes , show_agreement = True , show_pareto_distance = False , show_nash_distance = True , show_kalai_distance = False , show_max_welfare_distance = False , show_max_relative_welfare_distance = False , show_end_reason = True , show_annotations = not all_outcomes , show_reserved = True , show_total_time = True , show_relative_time = True , show_n_steps = True , ) if plot_params : params = params . update ( plot_params ) return cartesian_tournament ( competitors = tuple ( competitors ), scenarios = scenario_generator ( n_scenarios , n_outcomes , ** generator_params ), competitor_params = competitor_params , rotate_ufuns = rotate_ufuns , n_repetitions = n_repetitions , path = path , njobs = njobs , mechanism_type = SAOMechanism , n_steps = n_steps , time_limit = time_limit , pend = pend , pend_per_second = pend_per_second , step_time_limit = step_time_limit , negotiator_time_limit = negotiator_time_limit , mechanism_params = None , plot_fraction = plot_fraction , verbosity = verbosity , self_play = self_play , randomize_runs = randomize_runs , save_every = save_every , save_stats = save_stats , final_score = final_score , id_reveals_type = known_partner , name_reveals_type = True , plot_params = params , raise_exceptions = raise_exceptions , ) constant: DEFAULT_AN2024_COMPETITORS Default set of negotiators (agents) used as competitors constant: DEFAULT_TOURNAMENT_PATH Default location to store tournament logs constant: DEFAULT2024SETTINGS Default settings for ANL 2024 Example Negotiator The package provides few example negotiators. Of special importance is the MiCRO negotiator which provides a full implementation of a recently proposed behavioral strategy. Other negotiators are just wrappers over negotiators provided by NegMAS. class: Boulware Bases: BoulwareTBNegotiator Time-based boulware negotiation strategy Source code in anl/anl2024/negotiators/builtins.py 88 89 90 91 class Boulware ( BoulwareTBNegotiator ): \"\"\" Time-based boulware negotiation strategy \"\"\" class: Linear Bases: LinearTBNegotiator Time-based linear negotiation strategy Source code in anl/anl2024/negotiators/builtins.py 74 75 76 77 78 79 class Linear ( LinearTBNegotiator ): \"\"\" Time-based linear negotiation strategy \"\"\" ... class: Conceder Bases: ConcederTBNegotiator Time-based conceder negotiation strategy Source code in anl/anl2024/negotiators/builtins.py 82 83 84 85 class Conceder ( ConcederTBNegotiator ): \"\"\" Time-based conceder negotiation strategy \"\"\" class: NaiveTitForTat Bases: NaiveTitForTatNegotiator A simple behavioral strategy that assumes a zero-sum game Source code in anl/anl2024/negotiators/builtins.py 17 18 19 20 class NaiveTitForTat ( NaiveTitForTatNegotiator ): \"\"\" A simple behavioral strategy that assumes a zero-sum game \"\"\" class: MiCRO Bases: SAONegotiator A simple implementation of the MiCRO negotiation strategy Remarks This is a simplified implementation of the MiCRO strategy. It is not guaranteed to exactly match the published work. MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. Source code in anl/anl2024/negotiators/builtins.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class MiCRO ( SAONegotiator ): \"\"\" A simple implementation of the MiCRO negotiation strategy Remarks: - This is a simplified implementation of the MiCRO strategy. - It is not guaranteed to exactly match the published work. - MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # initialize local variables self . next_indx : int = 0 self . sorter = None self . _received , self . _sent = set (), set () def __call__ ( self , state : SAOState ) -> SAOResponse : \"\"\"The main implementation of the MiCRO strategy\"\"\" assert self . ufun # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL) if self . sorter is None : # A sorter, sorts a ufun and can be used to get outcomes using their utiility self . sorter = PresortingInverseUtilityFunction ( self . ufun , rational_only = True , eps =- 1 , rel_eps =- 1 ) # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes self . sorter . init () offer = state . current_offer # check whether the offer I received is acceptable response = ResponseType . REJECT_OFFER # check if the offer is acceptable (if one is received) # find my next offer (or best so far if I am not conceding) will_concede = len ( self . _sent ) <= len ( self . _received ) if not will_concede : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) else : # If I cannot concede, I will use my best offer so far (last one I sent) outcome = self . sorter . outcome_at ( self . next_indx ) if outcome is None and self . next_indx > 0 : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) will_concede = False # If I received something, check for acceptance if offer is not None : self . _received . add ( offer ) # The Acceptance Policy of MiCRO # accept if the offer is not worse than my next offer if I am conceding or the best so far if I am not if self . ufun . is_not_worse ( offer , outcome ): return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # I will repeat a past offer in any of the following conditions: # 1. My next offer is worse than disagreement # 2. I am not ready to concede (i.e. I already sent more unique offers than the unique offers I received) # The only way, outcome will still be None is if I have no rational outcomes at all. Should never happen in ANL 2024 if not will_concede or outcome is None or self . ufun . is_worse ( outcome , None ): return SAOResponse ( response , self . sample_sent ()) # I am willing and can concede. Concede by one outcome self . next_indx += 1 self . _sent . add ( outcome ) return SAOResponse ( response , outcome ) def sample_sent ( self ) -> Outcome | None : # Get an outcome from the set I sent so far (or my best if I sent nothing) if not len ( self . _sent ): assert self . sorter is not None return self . sorter . best () return random . choice ( list ( self . _sent )) __call__ ( state ) The main implementation of the MiCRO strategy Source code in anl/anl2024/negotiators/builtins.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __call__ ( self , state : SAOState ) -> SAOResponse : \"\"\"The main implementation of the MiCRO strategy\"\"\" assert self . ufun # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL) if self . sorter is None : # A sorter, sorts a ufun and can be used to get outcomes using their utiility self . sorter = PresortingInverseUtilityFunction ( self . ufun , rational_only = True , eps =- 1 , rel_eps =- 1 ) # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes self . sorter . init () offer = state . current_offer # check whether the offer I received is acceptable response = ResponseType . REJECT_OFFER # check if the offer is acceptable (if one is received) # find my next offer (or best so far if I am not conceding) will_concede = len ( self . _sent ) <= len ( self . _received ) if not will_concede : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) else : # If I cannot concede, I will use my best offer so far (last one I sent) outcome = self . sorter . outcome_at ( self . next_indx ) if outcome is None and self . next_indx > 0 : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) will_concede = False # If I received something, check for acceptance if offer is not None : self . _received . add ( offer ) # The Acceptance Policy of MiCRO # accept if the offer is not worse than my next offer if I am conceding or the best so far if I am not if self . ufun . is_not_worse ( offer , outcome ): return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # I will repeat a past offer in any of the following conditions: # 1. My next offer is worse than disagreement # 2. I am not ready to concede (i.e. I already sent more unique offers than the unique offers I received) # The only way, outcome will still be None is if I have no rational outcomes at all. Should never happen in ANL 2024 if not will_concede or outcome is None or self . ufun . is_worse ( outcome , None ): return SAOResponse ( response , self . sample_sent ()) # I am willing and can concede. Concede by one outcome self . next_indx += 1 self . _sent . add ( outcome ) return SAOResponse ( response , outcome ) Helpers (Scenario Generation) type: ScenarioGenerator Type of callable that can be used for generating scenarios. It must receive the number of scenarios and number of outcomes (as int, tuple or list) and return a list of Scenario s function: mixed_scenarios Generates a mix of zero-sum, monotonic and general scenarios Parameters: Name Type Description Default n_scenarios int Number of scenarios to genearate DEFAULT2024SETTINGS ['n_scenarios'] n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes (or a list of range thereof). DEFAULT2024SETTINGS ['n_outcomes'] reserved_ranges ReservedRanges the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome DEFAULT2024SETTINGS ['reserved_ranges'] log_uniform bool Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. DEFAULT2024SETTINGS ['outcomes_log_uniform'] zerosum_fraction float Fraction of zero-sum scenarios. These are original DivideThePie scenarios DEFAULT2024SETTINGS ['generator_params']['zerosum_fraction'] monotonic_fraction float Fraction of scenarios where each ufun is a monotonic function of the received pie. DEFAULT2024SETTINGS ['generator_params']['monotonic_fraction'] curve_fraction float Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. DEFAULT2024SETTINGS ['generator_params']['curve_fraction'] pareto_first bool If given, the Pareto frontier will always be in the first set of outcomes DEFAULT2024SETTINGS ['generator_params']['pareto_first'] n_ufuns int Number of ufuns to generate per scenario DEFAULT2024SETTINGS ['n_ufuns'] n_pareto int | float | tuple [ float | int , float | int ] | list [ int | float ] Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. DEFAULT2024SETTINGS ['generator_params']['n_pareto'] pareto_log_uniform bool Use log-uniform instead of uniform sampling if n_pareto is a tuple True Returns: Type Description list [ Scenario ] A list Scenario s Source code in anl/anl2024/runner.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def mixed_scenarios ( n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore * , reserved_ranges : ReservedRanges = DEFAULT2024SETTINGS [ \"reserved_ranges\" ], # type: ignore log_uniform : bool = DEFAULT2024SETTINGS [ \"outcomes_log_uniform\" ], # type: ignore zerosum_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"zerosum_fraction\" ], # type: ignore monotonic_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"monotonic_fraction\" ], # type: ignore curve_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"curve_fraction\" ], # type: ignore pareto_first : bool = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"pareto_first\" ], # type: ignore n_ufuns : int = DEFAULT2024SETTINGS [ \"n_ufuns\" ], # type: ignore n_pareto : int | float | tuple [ float | int , float | int ] | list [ int | float ] = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"n_pareto\" ], # type: ignore pareto_log_uniform : bool = True , ) -> list [ Scenario ]: \"\"\"Generates a mix of zero-sum, monotonic and general scenarios Args: n_scenarios: Number of scenarios to genearate n_outcomes: Number of outcomes (or a list of range thereof). reserved_ranges: the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome log_uniform: Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. zerosum_fraction: Fraction of zero-sum scenarios. These are original DivideThePie scenarios monotonic_fraction: Fraction of scenarios where each ufun is a monotonic function of the received pie. curve_fraction: Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. pareto_first: If given, the Pareto frontier will always be in the first set of outcomes n_ufuns: Number of ufuns to generate per scenario n_pareto: Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. pareto_log_uniform: Use log-uniform instead of uniform sampling if n_pareto is a tuple Returns: A list `Scenario` s \"\"\" assert zerosum_fraction + monotonic_fraction <= 1.0 nongeneral_fraction = zerosum_fraction + monotonic_fraction ufun_sets = [] for i in range ( n_scenarios ): r = random . random () n = intin ( n_outcomes , log_uniform ) name = \"S\" if r < nongeneral_fraction : n_pareto_selected = n name = \"DivideThePieGen\" else : if isinstance ( n_pareto , Iterable ): n_pareto = type ( n_pareto )( int ( _ * n + 0.5 ) if _ < 1 else int ( _ ) for _ in n_pareto # type: ignore ) else : n_pareto = int ( 0.5 + n_pareto * n ) if n_pareto < 1 else int ( n_pareto ) n_pareto_selected = intin ( n_pareto , log_uniform = pareto_log_uniform ) # type: ignore if r < zerosum_fraction : vals = generate_utility_values ( n_pareto_selected , n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"zero_sum\" , ) name = \"DivideThePie\" else : vals = generate_utility_values ( n_pareto_selected , n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"curve\" if random . random () < curve_fraction else \"piecewise_linear\" , ) issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( vals [ i ][ k ]) for i , _ in enumerate ( issues [ 0 ] . all )} ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \" { name }{ i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ] function: pie_scenarios Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Parameters: Name Type Description Default n_scenarios int Number of scenarios to create 20 n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). 100 reserved_ranges ReservedRanges Ranges of reserved values for first and second negotiators ((0.0, 0.999999), (0.0, 0.999999)) log_uniform bool If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). True monotonic If true all ufuns are monotonically increasing in the portion of the pie False Remarks When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. Source code in anl/anl2024/runner.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , monotonic = False , ) -> list [ Scenario ]: \"\"\"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). monotonic: If true all ufuns are monotonically increasing in the portion of the pie Remarks: - When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. \"\"\" ufun_sets = [] base_name = \"DivideTyePie\" if monotonic else \"S\" def normalize ( x ): mn , mx = x . min (), x . max () return (( x - mn ) / ( mx - mn )) . tolist () def make_monotonic ( x , i ): x = np . sort ( np . asarray ( x ), axis = None ) if i : x = x [:: - 1 ] r = random . random () if r < 0.33 : x = np . exp ( x ) elif r < 0.67 : x = np . log ( x ) else : pass return normalize ( x ) max_jitter_level = 0.8 for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ( [ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" if not monotonic else \"i1\" , ), ) # funs = [ # dict( # zip( # issues[0].all, # # adjust(np.asarray([random.random() for _ in range(n)])), # generate(n, i), # ) # ) # for i in range(2) # ] os = make_os ( issues , name = f \" { base_name }{ i } \" ) outcomes = list ( os . enumerate_or_sample ()) ufuns = U . generate_bilateral ( outcomes , conflict_level = 0.5 + 0.5 * random . random (), conflict_delta = random . random (), ) jitter_level = random . random () * max_jitter_level funs = [ np . asarray ([ float ( u ( _ )) for _ in outcomes ]) + np . random . random () * jitter_level for u in ufuns ] if monotonic : funs = [ make_monotonic ( x , i ) for i , x in enumerate ( funs )] else : funs = [ normalize ( x ) for x in funs ] ufuns = tuple ( U ( values = ( TableFun ( dict ( zip ( issues [ 0 ] . all , vals ))),), name = f \" { uname }{ i } \" , outcome_space = os , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), ) for ( uname , vals ) in zip (( \"First\" , \"Second\" ), funs ) # for (uname, r, vals) in zip((\"First\", \"Second\"), reserved_ranges, funs) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ] function: arbitrary_pie_scenarios Source code in anl/anl2024/runner.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def arbitrary_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = False , ) function: monotonic_pie_scenarios Source code in anl/anl2024/runner.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def monotonic_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = True , ) function: zerosum_pie_scenarios Creates scenarios all of the DivideThePie variety with proportions giving utility Parameters: Name Type Description Default n_scenarios int Number of scenarios to create 20 n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). 100 reserved_ranges ReservedRanges Ranges of reserved values for first and second negotiators ((0.0, 0.499999), (0.0, 0.499999)) log_uniform bool If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). True Remarks When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently Source code in anl/anl2024/runner.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 def zerosum_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.499999 ), ( 0.0 , 0.499999 )), log_uniform : bool = True , ) -> list [ Scenario ]: \"\"\"Creates scenarios all of the DivideThePie variety with proportions giving utility Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). Remarks: - When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently \"\"\" ufun_sets = [] for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( int ( str ( _ ) . split ( \"_\" )[ k ]) / ( n - 1 )) for _ in issues [ 0 ] . all } ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \"DivideTyePie { i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , pareto = tuple ( tuple ( u ( _ ) for u in ufuns ) for _ in make_os ( issues ) . enumerate_or_sample () ), reserved_ranges = reserved_ranges , ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"Reference"},{"location":"reference/#anl","text":"This package provides a wrapper around NegMAS functionality to generate and run tournaments a la ANL 2024 competition. You mostly only need to use anl2024_tournament in your code. The other helpers are provided to allow for a finer control over the scenarios used.","title":"ANL"},{"location":"reference/#tournaments","text":"","title":"Tournaments"},{"location":"reference/#function-anl2024_tournament","text":"Runs an ANL 2024 tournament Parameters: Name Type Description Default n_scenarios int Number of negotiation scenarios DEFAULT2024SETTINGS ['n_scenarios'] n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes (or a min/max tuple of n. outcomes) for each scenario DEFAULT2024SETTINGS ['n_outcomes'] competitors tuple [ type [ Negotiator ] | str , ...] | list [ type [ Negotiator ] | str ] list of competitor agents DEFAULT_AN2024_COMPETITORS competitor_params Sequence [ dict | None] | None If given, parameters to construct each competitor None rotate_ufuns bool If given, each scenario will be tried with both orders of the ufuns. DEFAULT2024SETTINGS ['rotate_ufuns'] n_repetitions int Number of times to repeat each negotiation DEFAULT2024SETTINGS ['n_repetitions'] n_steps int | tuple [ int , int ] | None Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['n_steps'] time_limit float | tuple [ float , float ] | None Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['time_limit'] pend float | tuple [ float , float ] Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['pend'] pend_per_second float | tuple [ float , float ] Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['pend_per_second'] step_time_limit float | tuple [ float , float ] | None Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['step_time_limit'] negotiator_time_limit float | tuple [ float , float ] | None Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) DEFAULT2024SETTINGS ['negotiator_time_limit'] name str | None Name of the tournament None nologs bool If given, no logs will be saved False njobs int Number of parallel jobs to use. -1 for serial and 0 for all cores 0 plot_fraction float Fraction of negotiations to plot. Only used if not nologs 0.2 verbosity int Verbosity level. The higher the more verbose 1 self_play bool Allow negotiators to run against themselves. DEFAULT2024SETTINGS ['self_play'] randomize_runs bool Randomize the order of negotiations DEFAULT2024SETTINGS ['randomize_runs'] save_every int Save logs every this number of negotiations 0 save_stats bool Save statistics for scenarios True known_partner bool Allow negotiators to know the type of their partner (through their ID) DEFAULT2024SETTINGS ['known_partner'] final_score tuple [ str , str ] The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max DEFAULT2024SETTINGS ['final_score'] base_path Path | None Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments None scenario_generator str | ScenarioGenerator An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. DEFAULT2024SETTINGS ['scenario_generator'] generator_params dict [ str , Any ] | None Parameters passed to the scenario generator DEFAULT2024SETTINGS ['generator_params'] plot_params dict [ str , Any ] | None If given, overrides plotting parameters. See nemgas.sao.SAOMechanism.plot() for all parameters None Returns: Type Description SimpleTournamentResults Tournament results as a SimpleTournamentResults object. Source code in anl/anl2024/runner.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 def anl2024_tournament ( n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore competitors : tuple [ type [ Negotiator ] | str , ... ] | list [ type [ Negotiator ] | str ] = DEFAULT_AN2024_COMPETITORS , rotate_ufuns : bool = DEFAULT2024SETTINGS [ \"rotate_ufuns\" ], # type: ignore n_repetitions : int = DEFAULT2024SETTINGS [ \"n_repetitions\" ], # type: ignore n_steps : int | tuple [ int , int ] | None = DEFAULT2024SETTINGS [ \"n_steps\" ], # type: ignore time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"time_limit\" ], # type: ignore pend : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend\" ], # type: ignore pend_per_second : float | tuple [ float , float ] = DEFAULT2024SETTINGS [ \"pend_per_second\" ], # type: ignore step_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"step_time_limit\" ], # type: ignore negotiator_time_limit : float | tuple [ float , float ] | None = DEFAULT2024SETTINGS [ \"negotiator_time_limit\" ], # type: ignore self_play : bool = DEFAULT2024SETTINGS [ \"self_play\" ], # type: ignore randomize_runs : bool = DEFAULT2024SETTINGS [ \"randomize_runs\" ], # type: ignore known_partner : bool = DEFAULT2024SETTINGS [ \"known_partner\" ], # type: ignore final_score : tuple [ str , str ] = DEFAULT2024SETTINGS [ \"final_score\" ], # type: ignore scenario_generator : str | ScenarioGenerator = DEFAULT2024SETTINGS [ \"scenario_generator\" ], # type: ignore generator_params : dict [ str , Any ] | None = DEFAULT2024SETTINGS [ \"generator_params\" ], # type: ignore competitor_params : Sequence [ dict | None ] | None = None , name : str | None = None , nologs : bool = False , njobs : int = 0 , plot_fraction : float = 0.2 , verbosity : int = 1 , save_every : int = 0 , save_stats : bool = True , base_path : Path | None = None , plot_params : dict [ str , Any ] | None = None , raise_exceptions : bool = True , ) -> SimpleTournamentResults : \"\"\"Runs an ANL 2024 tournament Args: n_scenarios: Number of negotiation scenarios n_outcomes: Number of outcomes (or a min/max tuple of n. outcomes) for each scenario competitors: list of competitor agents competitor_params: If given, parameters to construct each competitor rotate_ufuns: If given, each scenario will be tried with both orders of the ufuns. n_repetitions: Number of times to repeat each negotiation n_steps: Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) time_limit: Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range) pend: Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range) pend_per_second: Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range) step_time_limit: Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range) negotiator_time_limit: Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range) name: Name of the tournament nologs: If given, no logs will be saved njobs: Number of parallel jobs to use. -1 for serial and 0 for all cores plot_fraction: Fraction of negotiations to plot. Only used if not nologs verbosity: Verbosity level. The higher the more verbose self_play: Allow negotiators to run against themselves. randomize_runs: Randomize the order of negotiations save_every: Save logs every this number of negotiations save_stats: Save statistics for scenarios known_partner: Allow negotiators to know the type of their partner (through their ID) final_score: The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max base_path: Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments scenario_generator: An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes. generator_params: Parameters passed to the scenario generator plot_params: If given, overrides plotting parameters. See `nemgas.sao.SAOMechanism.plot()` for all parameters Returns: Tournament results as a `SimpleTournamentResults` object. \"\"\" if generator_params is None : generator_params = dict () if isinstance ( scenario_generator , str ): scenario_generator = GENERAROR_MAP [ scenario_generator ] all_outcomes = not scenario_generator == zerosum_pie_scenarios if nologs : path = None elif base_path is not None : path = Path ( base_path ) / ( name if name else unique_name ( \"anl\" )) else : path = DEFAULT_TOURNAMENT_PATH / ( name if name else unique_name ( \"anl\" )) params = dict ( ylimits = ( 0 , 1 ), mark_offers_view = True , mark_pareto_points = all_outcomes , mark_all_outcomes = all_outcomes , mark_nash_points = True , mark_kalai_points = all_outcomes , mark_max_welfare_points = all_outcomes , show_agreement = True , show_pareto_distance = False , show_nash_distance = True , show_kalai_distance = False , show_max_welfare_distance = False , show_max_relative_welfare_distance = False , show_end_reason = True , show_annotations = not all_outcomes , show_reserved = True , show_total_time = True , show_relative_time = True , show_n_steps = True , ) if plot_params : params = params . update ( plot_params ) return cartesian_tournament ( competitors = tuple ( competitors ), scenarios = scenario_generator ( n_scenarios , n_outcomes , ** generator_params ), competitor_params = competitor_params , rotate_ufuns = rotate_ufuns , n_repetitions = n_repetitions , path = path , njobs = njobs , mechanism_type = SAOMechanism , n_steps = n_steps , time_limit = time_limit , pend = pend , pend_per_second = pend_per_second , step_time_limit = step_time_limit , negotiator_time_limit = negotiator_time_limit , mechanism_params = None , plot_fraction = plot_fraction , verbosity = verbosity , self_play = self_play , randomize_runs = randomize_runs , save_every = save_every , save_stats = save_stats , final_score = final_score , id_reveals_type = known_partner , name_reveals_type = True , plot_params = params , raise_exceptions = raise_exceptions , )","title":"function: anl2024_tournament"},{"location":"reference/#constant-default_an2024_competitors","text":"Default set of negotiators (agents) used as competitors","title":"constant: DEFAULT_AN2024_COMPETITORS"},{"location":"reference/#constant-default_tournament_path","text":"Default location to store tournament logs","title":"constant: DEFAULT_TOURNAMENT_PATH"},{"location":"reference/#constant-default2024settings","text":"Default settings for ANL 2024","title":"constant: DEFAULT2024SETTINGS"},{"location":"reference/#example-negotiator","text":"The package provides few example negotiators. Of special importance is the MiCRO negotiator which provides a full implementation of a recently proposed behavioral strategy. Other negotiators are just wrappers over negotiators provided by NegMAS.","title":"Example Negotiator"},{"location":"reference/#class-boulware","text":"Bases: BoulwareTBNegotiator Time-based boulware negotiation strategy Source code in anl/anl2024/negotiators/builtins.py 88 89 90 91 class Boulware ( BoulwareTBNegotiator ): \"\"\" Time-based boulware negotiation strategy \"\"\"","title":"class: Boulware"},{"location":"reference/#class-linear","text":"Bases: LinearTBNegotiator Time-based linear negotiation strategy Source code in anl/anl2024/negotiators/builtins.py 74 75 76 77 78 79 class Linear ( LinearTBNegotiator ): \"\"\" Time-based linear negotiation strategy \"\"\" ...","title":"class: Linear"},{"location":"reference/#class-conceder","text":"Bases: ConcederTBNegotiator Time-based conceder negotiation strategy Source code in anl/anl2024/negotiators/builtins.py 82 83 84 85 class Conceder ( ConcederTBNegotiator ): \"\"\" Time-based conceder negotiation strategy \"\"\"","title":"class: Conceder"},{"location":"reference/#class-naivetitfortat","text":"Bases: NaiveTitForTatNegotiator A simple behavioral strategy that assumes a zero-sum game Source code in anl/anl2024/negotiators/builtins.py 17 18 19 20 class NaiveTitForTat ( NaiveTitForTatNegotiator ): \"\"\" A simple behavioral strategy that assumes a zero-sum game \"\"\"","title":"class: NaiveTitForTat"},{"location":"reference/#class-micro","text":"Bases: SAONegotiator A simple implementation of the MiCRO negotiation strategy Remarks This is a simplified implementation of the MiCRO strategy. It is not guaranteed to exactly match the published work. MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. Source code in anl/anl2024/negotiators/builtins.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class MiCRO ( SAONegotiator ): \"\"\" A simple implementation of the MiCRO negotiation strategy Remarks: - This is a simplified implementation of the MiCRO strategy. - It is not guaranteed to exactly match the published work. - MiCRO was introduced here: de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # initialize local variables self . next_indx : int = 0 self . sorter = None self . _received , self . _sent = set (), set () def __call__ ( self , state : SAOState ) -> SAOResponse : \"\"\"The main implementation of the MiCRO strategy\"\"\" assert self . ufun # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL) if self . sorter is None : # A sorter, sorts a ufun and can be used to get outcomes using their utiility self . sorter = PresortingInverseUtilityFunction ( self . ufun , rational_only = True , eps =- 1 , rel_eps =- 1 ) # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes self . sorter . init () offer = state . current_offer # check whether the offer I received is acceptable response = ResponseType . REJECT_OFFER # check if the offer is acceptable (if one is received) # find my next offer (or best so far if I am not conceding) will_concede = len ( self . _sent ) <= len ( self . _received ) if not will_concede : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) else : # If I cannot concede, I will use my best offer so far (last one I sent) outcome = self . sorter . outcome_at ( self . next_indx ) if outcome is None and self . next_indx > 0 : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) will_concede = False # If I received something, check for acceptance if offer is not None : self . _received . add ( offer ) # The Acceptance Policy of MiCRO # accept if the offer is not worse than my next offer if I am conceding or the best so far if I am not if self . ufun . is_not_worse ( offer , outcome ): return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # I will repeat a past offer in any of the following conditions: # 1. My next offer is worse than disagreement # 2. I am not ready to concede (i.e. I already sent more unique offers than the unique offers I received) # The only way, outcome will still be None is if I have no rational outcomes at all. Should never happen in ANL 2024 if not will_concede or outcome is None or self . ufun . is_worse ( outcome , None ): return SAOResponse ( response , self . sample_sent ()) # I am willing and can concede. Concede by one outcome self . next_indx += 1 self . _sent . add ( outcome ) return SAOResponse ( response , outcome ) def sample_sent ( self ) -> Outcome | None : # Get an outcome from the set I sent so far (or my best if I sent nothing) if not len ( self . _sent ): assert self . sorter is not None return self . sorter . best () return random . choice ( list ( self . _sent ))","title":"class: MiCRO"},{"location":"reference/#anl.anl2024.negotiators.MiCRO.__call__","text":"The main implementation of the MiCRO strategy Source code in anl/anl2024/negotiators/builtins.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __call__ ( self , state : SAOState ) -> SAOResponse : \"\"\"The main implementation of the MiCRO strategy\"\"\" assert self . ufun # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL) if self . sorter is None : # A sorter, sorts a ufun and can be used to get outcomes using their utiility self . sorter = PresortingInverseUtilityFunction ( self . ufun , rational_only = True , eps =- 1 , rel_eps =- 1 ) # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes self . sorter . init () offer = state . current_offer # check whether the offer I received is acceptable response = ResponseType . REJECT_OFFER # check if the offer is acceptable (if one is received) # find my next offer (or best so far if I am not conceding) will_concede = len ( self . _sent ) <= len ( self . _received ) if not will_concede : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) else : # If I cannot concede, I will use my best offer so far (last one I sent) outcome = self . sorter . outcome_at ( self . next_indx ) if outcome is None and self . next_indx > 0 : outcome = self . sorter . outcome_at ( self . next_indx - 1 ) will_concede = False # If I received something, check for acceptance if offer is not None : self . _received . add ( offer ) # The Acceptance Policy of MiCRO # accept if the offer is not worse than my next offer if I am conceding or the best so far if I am not if self . ufun . is_not_worse ( offer , outcome ): return SAOResponse ( ResponseType . ACCEPT_OFFER , offer ) # I will repeat a past offer in any of the following conditions: # 1. My next offer is worse than disagreement # 2. I am not ready to concede (i.e. I already sent more unique offers than the unique offers I received) # The only way, outcome will still be None is if I have no rational outcomes at all. Should never happen in ANL 2024 if not will_concede or outcome is None or self . ufun . is_worse ( outcome , None ): return SAOResponse ( response , self . sample_sent ()) # I am willing and can concede. Concede by one outcome self . next_indx += 1 self . _sent . add ( outcome ) return SAOResponse ( response , outcome )","title":"__call__()"},{"location":"reference/#helpers-scenario-generation","text":"","title":"Helpers (Scenario Generation)"},{"location":"reference/#type-scenariogenerator","text":"Type of callable that can be used for generating scenarios. It must receive the number of scenarios and number of outcomes (as int, tuple or list) and return a list of Scenario s","title":"type: ScenarioGenerator"},{"location":"reference/#function-mixed_scenarios","text":"Generates a mix of zero-sum, monotonic and general scenarios Parameters: Name Type Description Default n_scenarios int Number of scenarios to genearate DEFAULT2024SETTINGS ['n_scenarios'] n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes (or a list of range thereof). DEFAULT2024SETTINGS ['n_outcomes'] reserved_ranges ReservedRanges the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome DEFAULT2024SETTINGS ['reserved_ranges'] log_uniform bool Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. DEFAULT2024SETTINGS ['outcomes_log_uniform'] zerosum_fraction float Fraction of zero-sum scenarios. These are original DivideThePie scenarios DEFAULT2024SETTINGS ['generator_params']['zerosum_fraction'] monotonic_fraction float Fraction of scenarios where each ufun is a monotonic function of the received pie. DEFAULT2024SETTINGS ['generator_params']['monotonic_fraction'] curve_fraction float Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. DEFAULT2024SETTINGS ['generator_params']['curve_fraction'] pareto_first bool If given, the Pareto frontier will always be in the first set of outcomes DEFAULT2024SETTINGS ['generator_params']['pareto_first'] n_ufuns int Number of ufuns to generate per scenario DEFAULT2024SETTINGS ['n_ufuns'] n_pareto int | float | tuple [ float | int , float | int ] | list [ int | float ] Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. DEFAULT2024SETTINGS ['generator_params']['n_pareto'] pareto_log_uniform bool Use log-uniform instead of uniform sampling if n_pareto is a tuple True Returns: Type Description list [ Scenario ] A list Scenario s Source code in anl/anl2024/runner.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def mixed_scenarios ( n_scenarios : int = DEFAULT2024SETTINGS [ \"n_scenarios\" ], # type: ignore n_outcomes : int | tuple [ int , int ] | list [ int ] = DEFAULT2024SETTINGS [ \"n_outcomes\" ], # type: ignore * , reserved_ranges : ReservedRanges = DEFAULT2024SETTINGS [ \"reserved_ranges\" ], # type: ignore log_uniform : bool = DEFAULT2024SETTINGS [ \"outcomes_log_uniform\" ], # type: ignore zerosum_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"zerosum_fraction\" ], # type: ignore monotonic_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"monotonic_fraction\" ], # type: ignore curve_fraction : float = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"curve_fraction\" ], # type: ignore pareto_first : bool = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"pareto_first\" ], # type: ignore n_ufuns : int = DEFAULT2024SETTINGS [ \"n_ufuns\" ], # type: ignore n_pareto : int | float | tuple [ float | int , float | int ] | list [ int | float ] = DEFAULT2024SETTINGS [ \"generator_params\" ][ \"n_pareto\" ], # type: ignore pareto_log_uniform : bool = True , ) -> list [ Scenario ]: \"\"\"Generates a mix of zero-sum, monotonic and general scenarios Args: n_scenarios: Number of scenarios to genearate n_outcomes: Number of outcomes (or a list of range thereof). reserved_ranges: the range allowed for reserved values for each ufun. Note that the upper limit will be overridden to guarantee the existence of at least one rational outcome log_uniform: Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range. zerosum_fraction: Fraction of zero-sum scenarios. These are original DivideThePie scenarios monotonic_fraction: Fraction of scenarios where each ufun is a monotonic function of the received pie. curve_fraction: Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of a piecewise linear Pareto frontier. pareto_first: If given, the Pareto frontier will always be in the first set of outcomes n_ufuns: Number of ufuns to generate per scenario n_pareto: Number of outcomes on the Pareto frontier in general scenarios. Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities. Each value can either be an integer > 1 or a fraction of the number of outcomes in the scenario. pareto_log_uniform: Use log-uniform instead of uniform sampling if n_pareto is a tuple Returns: A list `Scenario` s \"\"\" assert zerosum_fraction + monotonic_fraction <= 1.0 nongeneral_fraction = zerosum_fraction + monotonic_fraction ufun_sets = [] for i in range ( n_scenarios ): r = random . random () n = intin ( n_outcomes , log_uniform ) name = \"S\" if r < nongeneral_fraction : n_pareto_selected = n name = \"DivideThePieGen\" else : if isinstance ( n_pareto , Iterable ): n_pareto = type ( n_pareto )( int ( _ * n + 0.5 ) if _ < 1 else int ( _ ) for _ in n_pareto # type: ignore ) else : n_pareto = int ( 0.5 + n_pareto * n ) if n_pareto < 1 else int ( n_pareto ) n_pareto_selected = intin ( n_pareto , log_uniform = pareto_log_uniform ) # type: ignore if r < zerosum_fraction : vals = generate_utility_values ( n_pareto_selected , n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"zero_sum\" , ) name = \"DivideThePie\" else : vals = generate_utility_values ( n_pareto_selected , n , n_ufuns = n_ufuns , pareto_first = pareto_first , pareto_generator = \"curve\" if random . random () < curve_fraction else \"piecewise_linear\" , ) issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( vals [ i ][ k ]) for i , _ in enumerate ( issues [ 0 ] . all )} ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \" { name }{ i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"function: mixed_scenarios"},{"location":"reference/#function-pie_scenarios","text":"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Parameters: Name Type Description Default n_scenarios int Number of scenarios to create 20 n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). 100 reserved_ranges ReservedRanges Ranges of reserved values for first and second negotiators ((0.0, 0.999999), (0.0, 0.999999)) log_uniform bool If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). True monotonic If true all ufuns are monotonically increasing in the portion of the pie False Remarks When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. Source code in anl/anl2024/runner.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , monotonic = False , ) -> list [ Scenario ]: \"\"\"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from. If a list, samples from this list will be used (with replacement). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). monotonic: If true all ufuns are monotonically increasing in the portion of the pie Remarks: - When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently. \"\"\" ufun_sets = [] base_name = \"DivideTyePie\" if monotonic else \"S\" def normalize ( x ): mn , mx = x . min (), x . max () return (( x - mn ) / ( mx - mn )) . tolist () def make_monotonic ( x , i ): x = np . sort ( np . asarray ( x ), axis = None ) if i : x = x [:: - 1 ] r = random . random () if r < 0.33 : x = np . exp ( x ) elif r < 0.67 : x = np . log ( x ) else : pass return normalize ( x ) max_jitter_level = 0.8 for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ( [ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" if not monotonic else \"i1\" , ), ) # funs = [ # dict( # zip( # issues[0].all, # # adjust(np.asarray([random.random() for _ in range(n)])), # generate(n, i), # ) # ) # for i in range(2) # ] os = make_os ( issues , name = f \" { base_name }{ i } \" ) outcomes = list ( os . enumerate_or_sample ()) ufuns = U . generate_bilateral ( outcomes , conflict_level = 0.5 + 0.5 * random . random (), conflict_delta = random . random (), ) jitter_level = random . random () * max_jitter_level funs = [ np . asarray ([ float ( u ( _ )) for _ in outcomes ]) + np . random . random () * jitter_level for u in ufuns ] if monotonic : funs = [ make_monotonic ( x , i ) for i , x in enumerate ( funs )] else : funs = [ normalize ( x ) for x in funs ] ufuns = tuple ( U ( values = ( TableFun ( dict ( zip ( issues [ 0 ] . all , vals ))),), name = f \" { uname }{ i } \" , outcome_space = os , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), ) for ( uname , vals ) in zip (( \"First\" , \"Second\" ), funs ) # for (uname, r, vals) in zip((\"First\", \"Second\"), reserved_ranges, funs) ) sample_reserved_values ( ufuns , reserved_ranges = reserved_ranges ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"function: pie_scenarios"},{"location":"reference/#function-arbitrary_pie_scenarios","text":"Source code in anl/anl2024/runner.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def arbitrary_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = False , )","title":"function: arbitrary_pie_scenarios"},{"location":"reference/#function-monotonic_pie_scenarios","text":"Source code in anl/anl2024/runner.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def monotonic_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.999999 ), ( 0.0 , 0.999999 )), log_uniform : bool = True , ) -> list [ Scenario ]: return pie_scenarios ( n_scenarios , n_outcomes , reserved_ranges = reserved_ranges , log_uniform = log_uniform , monotonic = True , )","title":"function: monotonic_pie_scenarios"},{"location":"reference/#function-zerosum_pie_scenarios","text":"Creates scenarios all of the DivideThePie variety with proportions giving utility Parameters: Name Type Description Default n_scenarios int Number of scenarios to create 20 n_outcomes int | tuple [ int , int ] | list [ int ] Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). 100 reserved_ranges ReservedRanges Ranges of reserved values for first and second negotiators ((0.0, 0.499999), (0.0, 0.499999)) log_uniform bool If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). True Remarks When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently Source code in anl/anl2024/runner.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 def zerosum_pie_scenarios ( n_scenarios : int = 20 , n_outcomes : int | tuple [ int , int ] | list [ int ] = 100 , * , reserved_ranges : ReservedRanges = (( 0.0 , 0.499999 ), ( 0.0 , 0.499999 )), log_uniform : bool = True , ) -> list [ Scenario ]: \"\"\"Creates scenarios all of the DivideThePie variety with proportions giving utility Args: n_scenarios: Number of scenarios to create n_outcomes: Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from). reserved_ranges: Ranges of reserved values for first and second negotiators log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple). Remarks: - When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently \"\"\" ufun_sets = [] for i in range ( n_scenarios ): n = intin ( n_outcomes , log_uniform ) issues = ( make_issue ([ f \" { i } _ { n - 1 - i } \" for i in range ( n )], \"portions\" ),) ufuns = tuple ( U ( values = ( TableFun ( { _ : float ( int ( str ( _ ) . split ( \"_\" )[ k ]) / ( n - 1 )) for _ in issues [ 0 ] . all } ), ), name = f \" { uname }{ i } \" , # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)), outcome_space = make_os ( issues , name = f \"DivideTyePie { i } \" ), ) for k , uname in enumerate (( \"First\" , \"Second\" )) # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges)) ) sample_reserved_values ( ufuns , pareto = tuple ( tuple ( u ( _ ) for u in ufuns ) for _ in make_os ( issues ) . enumerate_or_sample () ), reserved_ranges = reserved_ranges , ) ufun_sets . append ( ufuns ) return [ Scenario ( outcome_space = ufuns [ 0 ] . outcome_space , # type: ignore We are sure this is not None ufuns = ufuns , ) for ufuns in ufun_sets ]","title":"function: zerosum_pie_scenarios"}]}