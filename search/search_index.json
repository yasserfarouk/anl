{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ANL Documentation","text":"<p>This repository is the official platform for running ANAC Automated Negotiation Leagues (starting 2024). It will contain a module called <code>anlXXXX</code> for the competition run in year XXXX. For example anl2024 will contain all files related to the 2024's version of the competition.</p> <p>This package is a thin-wrapper around the NegMAS library for automated negotiation. Its main goal is to provide the following functionalities:</p> <ol> <li>A method for generating scenarios to run tournaments in the same settings as in the ANL competition. These functions are always called <code>anl20XX_tournament</code> for year <code>20XX</code>.</li> <li>A CLI for running tournaments called <code>anl</code>.</li> <li>A visualizer for inspecting tournament results and negotiations in details called <code>anlv</code>.</li> <li>A place to hold the official implementation of every strategy submitted to the ANL competition after each year. These can be found in the module <code>anl.anl20XX.negotiators</code> for year <code>20XX</code>.</li> </ol> <p>The official website for the ANL competition is: https://scml.cs.brown.edu/anl</p>"},{"location":"#quick-start","title":"Quick start","text":"<pre><code>pip install anl\n</code></pre> <p>You can also install the in-development version with::</p> <pre><code>pip install https://github.com/autoneg/anl/archive/master.zip\n</code></pre>"},{"location":"#cli","title":"CLI","text":"<p>After installation, you can try running a tournament using the CLI:</p> <pre><code>anl tournament2024\n</code></pre> <p>To find all the parameters you can customize for running tournaments run:</p> <pre><code>anl tournament2024 --help\n</code></pre> <p>You can run the following command to check the versions of ANL and NegMAS on your machine:</p> <pre><code>anl version\n</code></pre> <p>You should get at least these versions:</p> <pre><code>anl: 0.1.5 (NegMAS: 0.10.9)\n</code></pre> <p>Other than the two commands mentioned above (tournament2024, version), you can use the CLI to generate and save scenarios which you can later reuse with the tournament2024 command using --scenarios-path. As an example:</p> <p><pre><code>anl make-scenarios myscenarios --scenarios=5\nanl tournament2024 --scenarios-path=myscenarios --scenarios=0\n</code></pre> The first command will create 5 scenarios and save them under <code>myscenarios</code>. The second command will use these scenarios without generating any new scenarios to run a tournament.</p>"},{"location":"#visualizer","title":"Visualizer","text":"<p> ANL comes with a simple visualizer that can be used to visualize logs from tournaments after the fact.</p> <p>To start the visualizer type:</p> <p><pre><code>anlv show\n</code></pre> This will allow you to select any tournament that is stored in the default location (~/negmas/anl2024/tournaments). You can also show the tournament stored in a specific location 'your-tournament-path' using:</p> <pre><code>anlv show your-tournament-path\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Info</p> <p>This is not required to participate in the ANL competition</p> <p>If you would like to contribute to ANL, please clone the repo, then run:</p> <pre><code>python -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt\npython -m pip install -r docs/requirements.txt\npython -m pip install -e .\n</code></pre> <p>You can then submit Pull Requests which will be carefully reviewed.</p> <p>If you have an issue, please report it here. If you have something to discuss, please report it here.</p>"},{"location":"faq/","title":"FAQ","text":"<p>The Frequently Asked Questions (FAQ) for the ANL competition will be collected in this document.</p> <p>If you have further questions, please get in contact with the organisers.</p>"},{"location":"install/","title":"Preparing Development Environment","text":"<p>To participate in ANL 2024, you need to prepare a local development environment in your machine, download the skeleton, and start hacking. This section of the documentation describes two ways to do that.</p>"},{"location":"install/#direct-recommended","title":"Direct (Recommended)","text":"<p>This is the recommended method. It requires you to use an installation of python $3.11$ or later on your machine.</p>"},{"location":"install/#0-installing-python","title":"0. Installing Python","text":"<p>If -- for some reason -- you do not have python installed in your machine, start by installing it from python.org. You can also use any other method to install python 3.11 or later.</p>"},{"location":"install/#1-creating-and-activating-a-virtual-environment","title":"1. Creating and activating a virtual environment","text":"<p>Info</p> <p>optional but highly recommended</p> <p>As always recommended, you should create a virtual environment for your development. You can use your IDE to do that or simply run the following command: <pre><code>python -m venv .venv\n</code></pre> You should always activate your virtual environment using:</p> WindowsLinux/MacOS <pre><code>.venv\\Scripts\\activate.bat\n</code></pre> <pre><code>source .venv/bin/activate\n</code></pre> <p>Of course, you can use any other method for creating your virtual environment (e.g. anaconda, hatch, poetry, pyenv, virtualenv or any similar method). Whatever method you use, it is highly recommended not to install packages (anl or otherwise) directly on your base python system.</p>"},{"location":"install/#2-installing-the-anl-package","title":"2. Installing the ANL package","text":"<p>The second step is to install the <code>anl</code> package using:</p> <pre><code>python -m pip install anl\n</code></pre>"},{"location":"install/#3-development","title":"3. Development","text":"<p>The next step is to download the template from here. Please familiarize yourself with the competition rules availabe at the competition website. After downloading and uncompressing the template, you should do the following steps:</p> <ol> <li>Modify the name of the single class in <code>myagent.py</code> (currently called <code>MyNegotiator</code>) to a representative name for your agent. We will use <code>AwsomeNegotiator</code> here. You should then implement your agent logic by modifying this class.<ul> <li>Remember to change the name of the agent in the last line of the file to match your new class name (<code>AwsomeNegotiator</code>).</li> </ul> </li> <li>Start developing your agent as will be explained later in the tutorial.</li> <li> <p>You can use the following ways to test your agent:</p> <ul> <li>Run the following command to test your agent from the root folder of the extracted skeleton:   <pre><code>python -m myagent.myagent\n</code></pre></li> <li> <p>Use the <code>anl</code> command line utility from the root folder of the extracted skeleton:   <pre><code>anl tournament2024 --path=. --competitors=\"myagent.myagnet.AwsomeNegotiator;Boulware;Conceder\"\n</code></pre>   This method is more flexible as you can control all aspects of the tournament to run.   Use <code>anl tournament2024 --help</code>  to see all available options.</p> </li> <li> <p>You can directly call <code>anl2024_tournament()</code> passing your agent as one of the competitors. This is the most flexible method and will be used in the tutorial.</p> </li> <li> <p>You can visualize tournaments by running:</p> <pre><code>anlv show\n</code></pre> </li> </ul> </li> <li> <p>Submit your agent to the official submission website:</p> <ul> <li>Remember to update the <code>Class Name</code> (<code>AwsomeNegotiator</code> in our case) and <code>Agent Module</code> (<code>myagent.myagent</code> in our case) in the submission form on the  competition website to <code>AwsomeNegotiator</code>.</li> <li>If you need any libraries that are not already provided by <code>anl</code>, please include them in the <code>Dependencies</code> in a semi-colon separated list when submitting your agent.</li> </ul> </li> </ol>"},{"location":"install/#using-docker","title":"Using Docker","text":"<p>Warning</p> <p>You only need this if you do not want (or cannot) install locally on your machine.</p> <p>If you prefer not to install python and the anl package on your machine or if you cannot use python 3.11 or later, you can use docker for development as follows.</p>"},{"location":"install/#0-installing-docker","title":"0. Installing Docker","text":"<p>Docker is a container system that allows you to run applications in separated containers within your machine. Please install it using the official installation guide.</p>"},{"location":"install/#1-build-the-image-and-container","title":"1. Build the image and container","text":"<p>Run the following command on the root folder of your template to create and prepare the image and container that will be used for development:</p> <pre><code>docker compose up --build\n</code></pre> <p>This will take several minutes first time. Once this initial step is done, you can start the container again without rebuilding it using:</p> <pre><code>docker compose up\n</code></pre>"},{"location":"install/#2-development","title":"2. Development","text":"<p>The first step is to download the template from here. Please familiarize yourself with the competition rules availabe at the competition website. After downloading and uncompressing the template, you should do the following steps:</p> <ol> <li>Modify the name of the single class in <code>myagent.py</code> (currently called <code>MyNegotiator</code>) to a representative name for your agent. We will use <code>AwsomeNegotiator</code> here. You should then implement your agent logic by modifying this class.<ul> <li>Remember to change the name of the agent in the last line of the file to match your new class name (<code>AwsomeNegotiator</code>).</li> <li>Remember to update the <code>Class Name</code> (<code>AwsomeNegotiator</code> in our case) and <code>Agent Module</code> (<code>myagent.myagent</code> in our case) in the submission form on the  competition website to <code>AwsomeNegotiator</code>.</li> <li>If you need any libraries that are not already provided by <code>anl</code>, please include them in the <code>Dependencies</code> in a semi-colon separated list when submitting your agent.</li> </ul> </li> <li>Start developing your agent as will be explained later in the tutorial. Any changes you do in the skeleton, will be reflected in the container.</li> <li> <p>You can use the following ways to test your agent:</p> <ul> <li> <p>Run the following command to test your agent from the root folder of the extracted skeleton:</p> WindowsLinux/MacOS <pre><code>docker-run.bash python -m myagent.myagent\n</code></pre> <pre><code>docker-run.sh python -m myagent.myagent\n</code></pre> </li> <li> <p>Use the <code>anl</code> command line utility from the root folder of the extracted skeleton:</p> WindowsLinux/MacOS <pre><code>docker-run.bat anl tournament2024 --path=. --competitors=\"myagent.myagnet.AwsomeNegotiator;Boulware;Conceder\"\n</code></pre> <pre><code>docker-run.sh anl tournament2024 --path=. --competitors=\"myagent.myagnet.AwsomeNegotiator;Boulware;Conceder\"\n</code></pre> <p>This method is more flexible as you can control all aspects of the tournament to run.</p> </li> <li> <p>You can directly call <code>anl2024_tournament()</code> passing your agent as one of the competitors. This is the most flexible method and will be used in the tutorial.</p> </li> <li> <p>You can visualize tournaments by running:</p> WindowsLinux/MacOS <pre><code>docker-run.bat anlv show\n</code></pre> <pre><code>docker-run.sh anlv show\n</code></pre> </li> </ul> </li> <li> <p>Submit your agent to the official submission website:</p> <ul> <li>Remember to update the <code>Class Name</code> (<code>AwsomeNegotiator</code> in our case) and <code>Agent Module</code> (<code>myagent.myagent</code> in our case) in the submission form on the  competition website to <code>AwsomeNegotiator</code>.</li> <li>If you need any libraries that are not already provided by <code>anl</code>, please include them in the <code>Dependencies</code> in a semi-colon separated list when submitting your agent.</li> </ul> </li> </ol>"},{"location":"install/#3-clear-the-container","title":"3. Clear the container","text":"<p>When you finish your development, you can stop the containers used by this docker network using:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"reference/","title":"ANL Reference","text":"<p>This package provides a wrapper around NegMAS functionality to generate and run tournaments a la ANL 2024 competition. You mostly only need to use <code>anl2024_tournament</code> in your code. The other helpers are provided to allow for a finer control over the scenarios used.</p>"},{"location":"reference/#example-negotiators","title":"Example Negotiators","text":"<p>The package provides few example negotiators. Of special importance is the <code>MiCRO</code> negotiator which provides a full implementation of a recently proposed behavioral strategy. Other negotiators are just wrappers over negotiators provided by NegMAS.</p>"},{"location":"reference/#anl.anl2024.negotiators.builtins.micro.MiCRO","title":"<code>anl.anl2024.negotiators.builtins.micro.MiCRO</code>","text":"<p>             Bases: <code>SAONegotiator</code></p> <p>A simple implementation of the MiCRO negotiation strategy</p> Remarks <ul> <li>This is a simplified implementation of the MiCRO strategy.</li> <li>It is not guaranteed to exactly match the published work.</li> <li>MiCRO was introduced here:   de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\"   Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022.</li> <li>Note that MiCRO works optimally if both negotiators can concede all the way to agreement. If one of them   has a high reservation value preventing it from doing so, or if the allowed number of steps is small, MiCRO   will not reach agreement (even against itself).</li> </ul> Source code in <code>anl\\anl2024\\negotiators\\builtins\\micro.py</code> <pre><code>class MiCRO(SAONegotiator):\n    \"\"\"\n    A simple implementation of the MiCRO negotiation strategy\n\n    Remarks:\n        - This is a simplified implementation of the MiCRO strategy.\n        - It is not guaranteed to exactly match the published work.\n        - MiCRO was introduced here:\n          de Jonge, Dave. \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\"\n          Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI. 2022.\n        - Note that MiCRO works optimally if both negotiators can concede all the way to agreement. If one of them\n          has a high reservation value preventing it from doing so, or if the allowed number of steps is small, MiCRO\n          will not reach agreement (even against itself).\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # initialize local variables\n        self.worst_offer_utility: float = float(\"inf\")\n        self.sorter = None\n        self._received, self._sent = set(), set()\n\n    def __call__(self, state: SAOState) -&gt; SAOResponse:\n        # The main implementation of the MiCRO strategy\n        assert self.ufun\n        # initialize the sorter (This should better be done in on_negotiation_start() to allow for reuse but this is not needed in ANL)\n        if self.sorter is None:\n            # A sorter, sorts a ufun and can be used to get outcomes using their utiility\n            self.sorter = PresortingInverseUtilityFunction(\n                self.ufun, rational_only=True, eps=-1, rel_eps=-1\n            )\n            # Initialize the sorter. This is an O(nlog n) operation where n is the number of outcomes\n            self.sorter.init()\n        # get the current offer and prepare for rejecting it\n        offer = state.current_offer\n\n        # If I received something, check for acceptance\n        if offer is not None:\n            self._received.add(offer)\n\n        # Find out my next offer and the acceptable offer\n        will_concede = len(self._sent) &lt;= len(self._received)\n        # My next offer is either a conceding outcome if will_concede or sampled randomly from my past offers\n        next_offer = (\n            self.sample_sent() if not will_concede else self.sorter.next_worse()\n        )\n        # If I exhausted all my rational offers, do not concede\n        if next_offer is None:\n            will_concede, next_offer = False, self.sample_sent()\n        else:\n            next_utility = float(self.ufun(next_offer))\n            if next_utility &lt; self.ufun.reserved_value:\n                will_concede, next_offer = False, self.sample_sent()\n        next_utility = float(self.ufun(next_offer))\n        # Find my acceptable outcome. It will None if I did not offer anything yet.\n        acceptable_utility = (\n            self.worst_offer_utility if not will_concede else next_utility\n        )\n\n        # The Acceptance Policy of MiCRO\n        # accept if the offer is not worse than my acceptable offer if I am conceding or the best so far if I am not\n        offer_utility = float(self.ufun(offer))\n        if (\n            offer is not None\n            and offer_utility &gt;= acceptable_utility\n            and offer_utility &gt;= self.ufun.reserved_value\n        ):\n            return SAOResponse(ResponseType.ACCEPT_OFFER, offer)\n        # If I cannot find any offers, I know that there are NO rational outcomes in this negotiation for me and will end it.\n        if next_offer is None:\n            return SAOResponse(ResponseType.END_NEGOTIATION, None)\n        # Offer my next-offer and record it\n        self._sent.add(next_offer)\n        self.worst_offer_utility = next_utility\n        return SAOResponse(ResponseType.REJECT_OFFER, next_offer)\n\n    def sample_sent(self) -&gt; Outcome | None:\n        # Get an outcome from the set I sent so far (or my best if I sent nothing)\n        if not len(self._sent):\n            return None\n        return random.choice(list(self._sent))\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.nash_seeker.NashSeeker","title":"<code>anl.anl2024.negotiators.builtins.nash_seeker.NashSeeker</code>","text":"<p>             Bases: <code>SAONegotiator</code></p> <p>Assumes that the opponent has a fixed reserved value and seeks the Nash equilibrium.</p> <p>Parameters:</p> Name Type Description Default <code>opponent_reserved_value</code> <code>float</code> <p>Assumed reserved value for the opponent</p> <code>0.25</code> <code>nash_factor</code> <p>Fraction (or multiple) of the agent utility at the Nash Point (assuming the <code>opponent_reserved_value</code>) that is acceptable</p> <code>0.9</code> Source code in <code>anl\\anl2024\\negotiators\\builtins\\nash_seeker.py</code> <pre><code>class NashSeeker(SAONegotiator):\n    \"\"\"Assumes that the opponent has a fixed reserved value and seeks the Nash equilibrium.\n\n    Args:\n        opponent_reserved_value: Assumed reserved value for the opponent\n        nash_factor: Fraction (or multiple) of the agent utility at the Nash Point (assuming the `opponent_reserved_value`) that is acceptable\n\n    \"\"\"\n\n    def __init__(\n        self, *args, opponent_reserved_value: float = 0.25, nash_factor=0.9, **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self._opponent_r = opponent_reserved_value\n        self._outcomes: list[Outcome] = []\n        self._min_acceptable = float(\"inf\")\n        self._nash_factor = nash_factor\n        self._best: Outcome = None  # type: ignore\n\n    def on_preferences_changed(self, changes):\n        _ = changes  # silenting a typing warning\n        # This callback is called at the start of the negotiation after the ufun is set\n        assert self.ufun is not None and self.ufun.outcome_space is not None\n        # save my best outcome for later use\n        self._best = self.ufun.best()\n        # check that I have access to  the opponent ufun\n        assert self.opponent_ufun is not None\n        # set the reserved value of the opponent\n        self.opponent_ufun.reserved_value = self._opponent_r\n        # consider my and my parther's ufuns\n        ufuns = (self.ufun, self.opponent_ufun)\n        # list all outcomes\n        outcomes = list(self.ufun.outcome_space.enumerate_or_sample())\n        # find the pareto-front and the nash point\n        frontier_utils, frontier_indices = pareto_frontier(ufuns, outcomes)\n        frontier_outcomes = [outcomes[_] for _ in frontier_indices]\n        my_frontier_utils = [_[0] for _ in frontier_utils]\n        nash = nash_points(ufuns, frontier_utils)  # type: ignore\n        if nash:\n            # find my utility at the Nash Bargaining Solution.\n            my_nash_utility = nash[0][0][0]\n        else:\n            my_nash_utility = 0.5 * (float(self.ufun.max()) + self.ufun.reserved_value)\n        # Set the acceptable utility limit\n        self._min_acceptable = my_nash_utility * self._nash_factor\n        # Set the set of outcomes to offer from\n        self._outcomes = [\n            w\n            for u, w in zip(my_frontier_utils, frontier_outcomes)\n            if u &gt;= self._min_acceptable\n        ]\n\n    def __call__(self, state: SAOState) -&gt; SAOResponse:\n        # just assert that I have a ufun and I know the outcome space.\n        assert self.ufun is not None and self.ufun.outcome_space is not None\n        # read the current offer from the state. None means I am starting the negotiation\n        offer = state.current_offer\n        # Accept the offer if its utility is higher than my utility at the Nash Bargaining Solution with the assumed opponent ufun\n        if offer and float(self.ufun(offer)) &gt;= self._min_acceptable:\n            return SAOResponse(ResponseType.ACCEPT_OFFER, offer)\n        # If I could not find the Nash Bargaining Solution, just offer my best outcome forever.\n        if not self._outcomes:\n            return SAOResponse(ResponseType.REJECT_OFFER, self._best)\n        # Offer some outcome with high utility relative to the Nash Bargaining Solution\n        return SAOResponse(ResponseType.REJECT_OFFER, random.choice(self._outcomes))\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.rv_fitter.RVFitter","title":"<code>anl.anl2024.negotiators.builtins.rv_fitter.RVFitter</code>","text":"<p>             Bases: <code>SAONegotiator</code></p> <p>A simple negotiator that uses curve fitting to learn the reserved value.</p> <p>Parameters:</p> Name Type Description Default <code>min_unique_utilities</code> <code>int</code> <p>Number of different offers from the opponent before starting to                   attempt learning their reserved value.</p> <code>10</code> <code>e</code> <code>float</code> <p>The concession exponent used for the agent's offering strategy</p> <code>5.0</code> <code>stochasticity</code> <code>float</code> <p>The level of stochasticity in the offers.</p> <code>0.1</code> <code>enable_logging</code> <code>bool</code> <p>If given, a log will be stored  for the estimates.</p> <code>False</code> <p>Remarks:</p> <pre><code>- Assumes that the opponent is using a time-based offering strategy that offers\n  the outcome at utility $u(t) = (u_0 - r) - r \\exp(t^e)$ where $u_0$ is the utility of\n  the first offer (directly read from the opponent ufun), $e$ is an exponent that controls the\n  concession rate and $r$ is the reserved value we want to learn.\n- After it receives offers with enough different utilities, it starts finding the optimal values\n  for $e$ and $r$.\n- When it is time to respond, RVFitter, calculates the set of rational outcomes **for both agents**\n  based on its knowledge of the opponent ufun (given) and reserved value (learned). It then applies\n  the same concession curve defined above to concede over an ordered list of these outcomes.\n- Is this better than using the same concession curve on the outcome space without even trying to learn\n  the opponent reserved value? Maybe sometimes but empirical evaluation shows that it is not in general.\n- Note that the way we check for availability of enough data for training is based on the uniqueness of\n  the utility of offers from the opponent (for the opponent). Given that these are real values, this approach\n  is suspect because of rounding errors. If two outcomes have the same utility they may appear to have different\n  but very close utilities because or rounding errors (or genuine very small differences). Such differences should\n  be ignored.\n- Note also that we start assuming that the opponent reserved value is 0.0 which means that we are only restricted\n  with our own reserved values when calculating the rational outcomes. This is the best case scenario for us because\n  we have MORE negotiation power when the partner has LOWER utility.\n</code></pre> Source code in <code>anl\\anl2024\\negotiators\\builtins\\rv_fitter.py</code> <pre><code>class RVFitter(SAONegotiator):\n    \"\"\"A simple negotiator that uses curve fitting to learn the reserved value.\n\n    Args:\n        min_unique_utilities: Number of different offers from the opponent before starting to\n                              attempt learning their reserved value.\n        e: The concession exponent used for the agent's offering strategy\n        stochasticity: The level of stochasticity in the offers.\n        enable_logging: If given, a log will be stored  for the estimates.\n\n    Remarks:\n\n        - Assumes that the opponent is using a time-based offering strategy that offers\n          the outcome at utility $u(t) = (u_0 - r) - r \\\\exp(t^e)$ where $u_0$ is the utility of\n          the first offer (directly read from the opponent ufun), $e$ is an exponent that controls the\n          concession rate and $r$ is the reserved value we want to learn.\n        - After it receives offers with enough different utilities, it starts finding the optimal values\n          for $e$ and $r$.\n        - When it is time to respond, RVFitter, calculates the set of rational outcomes **for both agents**\n          based on its knowledge of the opponent ufun (given) and reserved value (learned). It then applies\n          the same concession curve defined above to concede over an ordered list of these outcomes.\n        - Is this better than using the same concession curve on the outcome space without even trying to learn\n          the opponent reserved value? Maybe sometimes but empirical evaluation shows that it is not in general.\n        - Note that the way we check for availability of enough data for training is based on the uniqueness of\n          the utility of offers from the opponent (for the opponent). Given that these are real values, this approach\n          is suspect because of rounding errors. If two outcomes have the same utility they may appear to have different\n          but very close utilities because or rounding errors (or genuine very small differences). Such differences should\n          be ignored.\n        - Note also that we start assuming that the opponent reserved value is 0.0 which means that we are only restricted\n          with our own reserved values when calculating the rational outcomes. This is the best case scenario for us because\n          we have MORE negotiation power when the partner has LOWER utility.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        min_unique_utilities: int = 10,\n        e: float = 5.0,\n        stochasticity: float = 0.1,\n        enable_logging: bool = False,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.min_unique_utilities = min_unique_utilities\n        self.e = e\n        self.stochasticity = stochasticity\n        # keeps track of times at which the opponent offers\n        self.opponent_times: list[float] = []\n        # keeps track of opponent utilities of its offers\n        self.opponent_utilities: list[float] = []\n        # keeps track of the our last estimate of the opponent reserved value\n        self._past_oppnent_rv = 0.0\n        # keeps track of the rational outcome set given our estimate of the\n        # opponent reserved value and our knowledge of ours\n        self._rational: list[tuple[float, float, Outcome]] = []\n        self._enable_logging = enable_logging\n\n    def __call__(self, state: SAOState) -&gt; SAOResponse:\n        assert self.ufun and self.opponent_ufun\n        # update the opponent reserved value in self.opponent_ufun\n        self.update_reserved_value(state)\n        # rune the acceptance strategy and if the offer received is acceptable, accept it\n        if self.is_acceptable(state):\n            return SAOResponse(ResponseType.ACCEPT_OFFER, state.current_offer)\n        # The offering strategy\n        # We only update our estimate of the rational list of outcomes if it is not set or\n        # there is a change in estimated reserved value\n        if (\n            not self._rational\n            or abs(self.opponent_ufun.reserved_value - self._past_oppnent_rv) &gt; 1e-3\n        ):\n            # The rational set of outcomes sorted dependingly according to our utility function\n            # and the opponent utility function (in that order).\n            self._rational = sorted(\n                [\n                    (my_util, opp_util, _)\n                    for _ in self.nmi.outcome_space.enumerate_or_sample(\n                        levels=10, max_cardinality=100_000\n                    )\n                    if (my_util := float(self.ufun(_))) &gt; self.ufun.reserved_value\n                    and (opp_util := float(self.opponent_ufun(_)))\n                    &gt; self.opponent_ufun.reserved_value\n                ],\n            )\n        # If there are no rational outcomes (i.e. our estimate of the opponent rv is very wrogn),\n        # then just revert to offering our top offer\n        if not self._rational:\n            return SAOResponse(ResponseType.REJECT_OFFER, self.ufun.best())\n        # find our aspiration level (value between 0 and 1) the higher the higher utility we require\n        asp = aspiration_function(state.relative_time, 1.0, 0.0, self.e)\n        # find the index of the rational outcome at the aspiration level (in the rational set of outcomes)\n        n_rational = len(self._rational)\n        max_rational = n_rational - 1\n        min_indx = max(0, min(max_rational, int(asp * max_rational)))\n        # find current stochasticity which goes down from the set level to zero linearly\n        s = aspiration_function(state.relative_time, self.stochasticity, 0.0, 1.0)\n        # find the index of the maximum utility we require based on stochasticity (going down over time)\n        max_indx = max(0, min(int(min_indx + s * n_rational), max_rational))\n        # offer an outcome in the selected range\n        indx = random.randint(min_indx, max_indx) if min_indx != max_indx else min_indx\n        outcome = self._rational[indx][-1]\n        return SAOResponse(ResponseType.REJECT_OFFER, outcome)\n\n    def is_acceptable(self, state: SAOState) -&gt; bool:\n        # The acceptance strategy\n        assert self.ufun and self.opponent_ufun\n        # get the offer from the mechanism state\n        offer = state.current_offer\n        # If there is no offer, there is nothing to accept\n        if offer is None:\n            return False\n        # Find the current aspiration level\n        asp = aspiration_function(\n            state.relative_time, 1.0, self.ufun.reserved_value, self.e\n        )\n        # accept if the utility of the received offer is higher than\n        # the current aspiration\n        return float(self.ufun(offer)) &gt;= asp\n\n    def update_reserved_value(self, state: SAOState):\n        # Learns the reserved value of the partner\n        assert self.opponent_ufun is not None\n        # extract the current offer from the state\n        offer = state.current_offer\n        if offer is None:\n            return\n        # save to the list of utilities received from the opponent and their times\n        self.opponent_utilities.append(float(self.opponent_ufun(offer)))\n        self.opponent_times.append(state.relative_time)\n\n        # If we do not have enough data, just assume that the opponent\n        # reserved value is zero\n        n_unique = len(set(self.opponent_utilities))\n        if n_unique &lt; self.min_unique_utilities:\n            self._past_oppnent_rv = 0.0\n            self.opponent_ufun.reserved_value = 0.0\n            return\n        # Use curve fitting to estimate the opponent reserved value\n        # We assume the following:\n        # - The opponent is using a concession strategy with an exponent between 0.2, 5.0\n        # - The opponent never offers outcomes lower than their reserved value which means\n        #   that their rv must be no higher than the worst outcome they offered for themselves.\n        bounds = ((0.2, 0.0), (5.0, min(self.opponent_utilities)))\n        err = \"\"\n        try:\n            optimal_vals, _ = curve_fit(\n                lambda x, e, rv: aspiration_function(\n                    x, self.opponent_utilities[0], rv, e\n                ),\n                self.opponent_times,\n                self.opponent_utilities,\n                bounds=bounds,\n            )\n            self._past_oppnent_rv = self.opponent_ufun.reserved_value\n            self.opponent_ufun.reserved_value = optimal_vals[1]\n        except Exception as e:\n            err, optimal_vals = f\"{str(e)}\", [None, None]\n\n        # log my estimate\n        if self._enable_logging:\n            self.nmi.log_info(\n                self.id,\n                dict(\n                    estimated_rv=self.opponent_ufun.reserved_value,\n                    n_unique=n_unique,\n                    opponent_utility=self.opponent_utilities[-1],\n                    estimated_exponent=optimal_vals[0],\n                    estimated_max=self.opponent_utilities[0],\n                    error=err,\n                ),\n            )\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.Boulware","title":"<code>anl.anl2024.negotiators.builtins.wrappers.Boulware</code>","text":"<p>             Bases: <code>BoulwareTBNegotiator</code></p> <p>Time-based boulware negotiation strategy</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class Boulware(BoulwareTBNegotiator):\n    \"\"\"\n    Time-based boulware negotiation strategy\n    \"\"\"\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.Linear","title":"<code>anl.anl2024.negotiators.builtins.wrappers.Linear</code>","text":"<p>             Bases: <code>LinearTBNegotiator</code></p> <p>Time-based linear negotiation strategy</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class Linear(LinearTBNegotiator):\n    \"\"\"\n    Time-based linear negotiation strategy\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.Conceder","title":"<code>anl.anl2024.negotiators.builtins.wrappers.Conceder</code>","text":"<p>             Bases: <code>ConcederTBNegotiator</code></p> <p>Time-based conceder negotiation strategy</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class Conceder(ConcederTBNegotiator):\n    \"\"\"\n    Time-based conceder negotiation strategy\n    \"\"\"\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.StochasticBoulware","title":"<code>anl.anl2024.negotiators.builtins.wrappers.StochasticBoulware</code>","text":"<p>             Bases: <code>AspirationNegotiator</code></p> <p>Time-based boulware negotiation strategy (offers above the limit instead of at it)</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class StochasticBoulware(AspirationNegotiator):\n    \"\"\"\n    Time-based boulware negotiation strategy (offers above the limit instead of at it)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(\n            *args,\n            max_aspiration=1.0,\n            aspiration_type=\"boulware\",\n            tolerance=0.00001,\n            stochastic=True,\n            presort=True,\n            **kwargs\n        )\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.StochasticLinear","title":"<code>anl.anl2024.negotiators.builtins.wrappers.StochasticLinear</code>","text":"<p>             Bases: <code>AspirationNegotiator</code></p> <p>Time-based linear negotiation strategy (offers above the limit instead of at it)</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class StochasticLinear(AspirationNegotiator):\n    \"\"\"\n    Time-based linear negotiation strategy (offers above the limit instead of at it)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(\n            *args,\n            max_aspiration=1.0,\n            aspiration_type=\"linear\",\n            tolerance=0.00001,\n            stochastic=True,\n            presort=True,\n            **kwargs\n        )\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.StochasticConceder","title":"<code>anl.anl2024.negotiators.builtins.wrappers.StochasticConceder</code>","text":"<p>             Bases: <code>AspirationNegotiator</code></p> <p>Time-based conceder negotiation strategy (offers above the limit instead of at it)</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class StochasticConceder(AspirationNegotiator):\n    \"\"\"\n    Time-based conceder negotiation strategy (offers above the limit instead of at it)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(\n            *args,\n            max_aspiration=1.0,\n            aspiration_type=\"conceder\",\n            tolerance=0.00001,\n            stochastic=True,\n            presort=True,\n            **kwargs\n        )\n</code></pre>"},{"location":"reference/#anl.anl2024.negotiators.builtins.wrappers.NaiveTitForTat","title":"<code>anl.anl2024.negotiators.builtins.wrappers.NaiveTitForTat</code>","text":"<p>             Bases: <code>NaiveTitForTatNegotiator</code></p> <p>A simple behavioral strategy that assumes a zero-sum game</p> Source code in <code>anl\\anl2024\\negotiators\\builtins\\wrappers.py</code> <pre><code>class NaiveTitForTat(NaiveTitForTatNegotiator):\n    \"\"\"\n    A simple behavioral strategy that assumes a zero-sum game\n    \"\"\"\n</code></pre>"},{"location":"reference/#tournaments","title":"Tournaments","text":""},{"location":"reference/#anl.anl2024.anl2024_tournament","title":"<code>anl.anl2024.anl2024_tournament</code>","text":"<p>Runs an ANL 2024 tournament</p> <p>Parameters:</p> Name Type Description Default <code>scenarios</code> <code>tuple[Scenario, ...] | list[Scenario]</code> <p>A list of predefined scenarios to use for the tournament</p> <code>tuple()</code> <code>n_scenarios</code> <code>int</code> <p>Number of negotiation scenarios to generate specifically for this tournament</p> <code>DEFAULT2024SETTINGS['n_scenarios']</code> <code>n_outcomes</code> <code>int | tuple[int, int] | list[int]</code> <p>Number of outcomes (or a min/max tuple of n. outcomes) for each scenario</p> <code>DEFAULT2024SETTINGS['n_outcomes']</code> <code>competitors</code> <code>tuple[type[Negotiator] | str, ...] | list[type[Negotiator] | str]</code> <p>list of competitor agents</p> <code>DEFAULT_AN2024_COMPETITORS</code> <code>competitor_params</code> <code>Sequence[dict | None] | None</code> <p>If given, parameters to construct each competitor</p> <code>None</code> <code>rotate_ufuns</code> <code>bool</code> <p>If given, each scenario will be tried with both orders of the ufuns.</p> <code>DEFAULT2024SETTINGS['rotate_ufuns']</code> <code>n_repetitions</code> <code>int</code> <p>Number of times to repeat each negotiation</p> <code>DEFAULT2024SETTINGS['n_repetitions']</code> <code>n_steps</code> <code>int | tuple[int, int] | None</code> <p>Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range)</p> <code>DEFAULT2024SETTINGS['n_steps']</code> <code>time_limit</code> <code>float | tuple[float, float] | None</code> <p>Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range)</p> <code>DEFAULT2024SETTINGS['time_limit']</code> <code>pend</code> <code>float | tuple[float, float]</code> <p>Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range)</p> <code>DEFAULT2024SETTINGS['pend']</code> <code>pend_per_second</code> <code>float | tuple[float, float]</code> <p>Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range)</p> <code>DEFAULT2024SETTINGS['pend_per_second']</code> <code>step_time_limit</code> <code>float | tuple[float, float] | None</code> <p>Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range)</p> <code>DEFAULT2024SETTINGS['step_time_limit']</code> <code>negotiator_time_limit</code> <code>float | tuple[float, float] | None</code> <p>Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range)</p> <code>DEFAULT2024SETTINGS['negotiator_time_limit']</code> <code>name</code> <code>str | None</code> <p>Name of the tournament</p> <code>None</code> <code>nologs</code> <code>bool</code> <p>If given, no logs will be saved</p> <code>False</code> <code>njobs</code> <code>int</code> <p>Number of parallel jobs to use. -1 for serial and 0 for all cores</p> <code>0</code> <code>plot_fraction</code> <code>float</code> <p>Fraction of negotiations to plot. Only used if not nologs</p> <code>0.2</code> <code>verbosity</code> <code>int</code> <p>Verbosity level. The higher the more verbose</p> <code>1</code> <code>self_play</code> <code>bool</code> <p>Allow negotiators to run against themselves.</p> <code>DEFAULT2024SETTINGS['self_play']</code> <code>randomize_runs</code> <code>bool</code> <p>Randomize the order of negotiations</p> <code>DEFAULT2024SETTINGS['randomize_runs']</code> <code>save_every</code> <code>int</code> <p>Save logs every this number of negotiations</p> <code>0</code> <code>save_stats</code> <code>bool</code> <p>Save statistics for scenarios</p> <code>True</code> <code>known_partner</code> <code>bool</code> <p>Allow negotiators to know the type of their partner (through their ID)</p> <code>DEFAULT2024SETTINGS['known_partner']</code> <code>final_score</code> <code>tuple[str, str]</code> <p>The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max</p> <code>DEFAULT2024SETTINGS['final_score']</code> <code>base_path</code> <code>Path | None</code> <p>Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments</p> <code>None</code> <code>scenario_generator</code> <code>str | ScenarioGenerator</code> <p>An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes.</p> <code>DEFAULT2024SETTINGS['scenario_generator']</code> <code>generator_params</code> <code>dict[str, Any] | None</code> <p>Parameters passed to the scenario generator</p> <code>DEFAULT2024SETTINGS['generator_params']</code> <code>plot_params</code> <code>dict[str, Any] | None</code> <p>If given, overrides plotting parameters. See <code>nemgas.sao.SAOMechanism.plot()</code> for all parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>SimpleTournamentResults</code> <p>Tournament results as a <code>SimpleTournamentResults</code> object.</p> Source code in <code>anl\\anl2024\\runner.py</code> <pre><code>def anl2024_tournament(\n    scenarios: tuple[Scenario, ...] | list[Scenario] = tuple(),\n    n_scenarios: int = DEFAULT2024SETTINGS[\"n_scenarios\"],  # type: ignore\n    n_outcomes: int | tuple[int, int] | list[int] = DEFAULT2024SETTINGS[\"n_outcomes\"],  # type: ignore\n    competitors: tuple[type[Negotiator] | str, ...]\n    | list[type[Negotiator] | str] = DEFAULT_AN2024_COMPETITORS,\n    rotate_ufuns: bool = DEFAULT2024SETTINGS[\"rotate_ufuns\"],  # type: ignore\n    n_repetitions: int = DEFAULT2024SETTINGS[\"n_repetitions\"],  # type: ignore\n    n_steps: int | tuple[int, int] | None = DEFAULT2024SETTINGS[\"n_steps\"],  # type: ignore\n    time_limit: float | tuple[float, float] | None = DEFAULT2024SETTINGS[\"time_limit\"],  # type: ignore\n    pend: float | tuple[float, float] = DEFAULT2024SETTINGS[\"pend\"],  # type: ignore\n    pend_per_second: float\n    | tuple[float, float] = DEFAULT2024SETTINGS[\"pend_per_second\"],  # type: ignore\n    step_time_limit: float\n    | tuple[float, float]\n    | None = DEFAULT2024SETTINGS[\"step_time_limit\"],  # type: ignore\n    negotiator_time_limit: float\n    | tuple[float, float]\n    | None = DEFAULT2024SETTINGS[\"negotiator_time_limit\"],  # type: ignore\n    self_play: bool = DEFAULT2024SETTINGS[\"self_play\"],  # type: ignore\n    randomize_runs: bool = DEFAULT2024SETTINGS[\"randomize_runs\"],  # type: ignore\n    known_partner: bool = DEFAULT2024SETTINGS[\"known_partner\"],  # type: ignore\n    final_score: tuple[str, str] = DEFAULT2024SETTINGS[\"final_score\"],  # type: ignore\n    scenario_generator: str\n    | ScenarioGenerator = DEFAULT2024SETTINGS[\"scenario_generator\"],  # type: ignore\n    generator_params: dict[str, Any] | None = DEFAULT2024SETTINGS[\"generator_params\"],  # type: ignore\n    competitor_params: Sequence[dict | None] | None = None,\n    name: str | None = None,\n    nologs: bool = False,\n    njobs: int = 0,\n    plot_fraction: float = 0.2,\n    verbosity: int = 1,\n    save_every: int = 0,\n    save_stats: bool = True,\n    base_path: Path | None = None,\n    plot_params: dict[str, Any] | None = None,\n    raise_exceptions: bool = True,\n) -&gt; SimpleTournamentResults:\n    \"\"\"Runs an ANL 2024 tournament\n\n    Args:\n        scenarios: A list of predefined scenarios to use for the tournament\n        n_scenarios: Number of negotiation scenarios to generate specifically for this tournament\n        n_outcomes: Number of outcomes (or a min/max tuple of n. outcomes) for each scenario\n        competitors: list of competitor agents\n        competitor_params: If given, parameters to construct each competitor\n        rotate_ufuns: If given, each scenario will be tried with both orders of the ufuns.\n        n_repetitions: Number of times to repeat each negotiation\n        n_steps: Number of steps/rounds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range)\n        time_limit: Number of seconds allowed for the each negotiation (None for no-limit and a 2-valued tuple for sampling from a range)\n        pend: Probability of ending the negotiation every step/round (None for no-limit and a 2-valued tuple for sampling from a range)\n        pend_per_second: Probability of ending the negotiation every second (None for no-limit and a 2-valued tuple for sampling from a range)\n        step_time_limit: Time limit for every negotiation step (None for no-limit and a 2-valued tuple for sampling from a range)\n        negotiator_time_limit: Time limit for all actions of every negotiator (None for no-limit and a 2-valued tuple for sampling from a range)\n        name: Name of the tournament\n        nologs: If given, no logs will be saved\n        njobs: Number of parallel jobs to use. -1 for serial and 0 for all cores\n        plot_fraction: Fraction of negotiations to plot. Only used if not nologs\n        verbosity: Verbosity level. The higher the more verbose\n        self_play: Allow negotiators to run against themselves.\n        randomize_runs: Randomize the order of negotiations\n        save_every: Save logs every this number of negotiations\n        save_stats: Save statistics for scenarios\n        known_partner: Allow negotiators to know the type of their partner (through their ID)\n        final_score: The metric and statistic used to calculate the score. Metrics are: advantage, utility, welfare, partner_welfare and Stats are: median, mean, std, min, max\n        base_path: Folder in which to generate the logs folder for this tournament. Default is ~/negmas/anl2024/tournaments\n        scenario_generator: An alternative method for generating bilateral negotiation scenarios. Must receive the number of scenarios and number of outcomes.\n        generator_params: Parameters passed to the scenario generator\n        plot_params: If given, overrides plotting parameters. See `nemgas.sao.SAOMechanism.plot()` for all parameters\n\n    Returns:\n        Tournament results as a `SimpleTournamentResults` object.\n    \"\"\"\n    if generator_params is None:\n        generator_params = dict()\n    if isinstance(scenario_generator, str):\n        scenario_generator = GENERAROR_MAP[scenario_generator]\n    all_outcomes = not scenario_generator == zerosum_pie_scenarios\n    if nologs:\n        path = None\n    elif base_path is not None:\n        path = Path(base_path) / (name if name else unique_name(\"anl\"))\n    else:\n        path = DEFAULT_TOURNAMENT_PATH / (name if name else unique_name(\"anl\"))\n    params = dict(\n        ylimits=(0, 1),\n        mark_offers_view=True,\n        mark_pareto_points=all_outcomes,\n        mark_all_outcomes=all_outcomes,\n        mark_nash_points=True,\n        mark_kalai_points=all_outcomes,\n        mark_max_welfare_points=all_outcomes,\n        show_agreement=True,\n        show_pareto_distance=False,\n        show_nash_distance=True,\n        show_kalai_distance=False,\n        show_max_welfare_distance=False,\n        show_max_relative_welfare_distance=False,\n        show_end_reason=True,\n        show_annotations=not all_outcomes,\n        show_reserved=True,\n        show_total_time=True,\n        show_relative_time=True,\n        show_n_steps=True,\n    )\n    if plot_params:\n        params = params.update(plot_params)\n    scenarios = list(scenarios) + list(\n        scenario_generator(n_scenarios, n_outcomes, **generator_params)\n    )\n    private_infos = [\n        tuple(\n            dict(opponent_ufun=U(values=_.values, weights=_.weights, bias=_._bias, reserved_value=0, outcome_space=_.outcome_space))  # type: ignore\n            for _ in s.ufuns[::-1]\n        )\n        for s in scenarios\n    ]\n    return cartesian_tournament(\n        competitors=tuple(competitors),\n        scenarios=scenarios,\n        competitor_params=competitor_params,\n        private_infos=private_infos,  # type: ignore\n        rotate_ufuns=rotate_ufuns,\n        n_repetitions=n_repetitions,\n        path=path,\n        njobs=njobs,\n        mechanism_type=SAOMechanism,\n        n_steps=n_steps,\n        time_limit=time_limit,\n        pend=pend,\n        pend_per_second=pend_per_second,\n        step_time_limit=step_time_limit,\n        negotiator_time_limit=negotiator_time_limit,\n        mechanism_params=None,\n        plot_fraction=plot_fraction,\n        verbosity=verbosity,\n        self_play=self_play,\n        randomize_runs=randomize_runs,\n        save_every=save_every,\n        save_stats=save_stats,\n        final_score=final_score,\n        id_reveals_type=known_partner,\n        name_reveals_type=True,\n        plot_params=params,\n        raise_exceptions=raise_exceptions,\n    )\n</code></pre>"},{"location":"reference/#anl.anl2024.DEFAULT_AN2024_COMPETITORS","title":"<code>anl.anl2024.DEFAULT_AN2024_COMPETITORS = (RVFitter, NashSeeker, MiCRO, Boulware, Conceder, Linear)</code>  <code>module-attribute</code>","text":"<p>Default set of negotiators (agents) used as competitors</p>"},{"location":"reference/#anl.anl2024.DEFAULT_TOURNAMENT_PATH","title":"<code>anl.anl2024.DEFAULT_TOURNAMENT_PATH = Path.home() / 'negmas' / 'anl2024' / 'tournaments'</code>  <code>module-attribute</code>","text":"<p>Default location to store tournament logs</p>"},{"location":"reference/#anl.anl2024.DEFAULT2024SETTINGS","title":"<code>anl.anl2024.DEFAULT2024SETTINGS = dict(n_ufuns=2, n_scenarios=50, n_outcomes=(900, 1100), n_steps=(10, 10000), n_repetitions=5, reserved_ranges=((0.0, 1.0), (0.0, 1.0)), competitors=DEFAULT_AN2024_COMPETITORS, rotate_ufuns=False, time_limit=60, pend=0, pend_per_second=0, step_time_limit=None, negotiator_time_limit=None, self_play=True, randomize_runs=True, known_partner=False, final_score=('advantage', 'mean'), scenario_generator='mix', outcomes_log_uniform=True, generator_params=dict(reserved_ranges=((0.0, 1.0), (0.0, 1.0)), log_uniform=False, zerosum_fraction=0.05, monotonic_fraction=0.25, curve_fraction=0.5, pareto_first=False, n_pareto=(0.005, 0.25)))</code>  <code>module-attribute</code>","text":"<p>Default settings for ANL 2024</p>"},{"location":"reference/#helpers-scenario-generation","title":"Helpers (Scenario Generation)","text":""},{"location":"reference/#anl.anl2024.ScenarioGenerator","title":"<code>anl.anl2024.ScenarioGenerator = Callable[[int, int | tuple[int, int] | list[int]], list[Scenario]]</code>  <code>module-attribute</code>","text":"<p>Type of callable that can be used for generating scenarios. It must receive the number of scenarios and number of outcomes (as int, tuple or list) and return a list of <code>Scenario</code> s</p>"},{"location":"reference/#anl.anl2024.mixed_scenarios","title":"<code>anl.anl2024.mixed_scenarios</code>","text":"<p>Generates a mix of zero-sum, monotonic and general scenarios</p> <p>Parameters:</p> Name Type Description Default <code>n_scenarios</code> <code>int</code> <p>Number of scenarios to genearate</p> <code>DEFAULT2024SETTINGS['n_scenarios']</code> <code>n_outcomes</code> <code>int | tuple[int, int] | list[int]</code> <p>Number of outcomes (or a list of range thereof).</p> <code>DEFAULT2024SETTINGS['n_outcomes']</code> <code>reserved_ranges</code> <code>ReservedRanges</code> <p>the range allowed for reserved values for each ufun.              Note that the upper limit will be overridden to guarantee              the existence of at least one rational outcome</p> <code>DEFAULT2024SETTINGS['reserved_ranges']</code> <code>log_uniform</code> <code>bool</code> <p>Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range.</p> <code>DEFAULT2024SETTINGS['outcomes_log_uniform']</code> <code>zerosum_fraction</code> <code>float</code> <p>Fraction of zero-sum scenarios. These are original DivideThePie scenarios</p> <code>DEFAULT2024SETTINGS['generator_params']['zerosum_fraction']</code> <code>monotonic_fraction</code> <code>float</code> <p>Fraction of scenarios where each ufun is a monotonic function of the received pie.</p> <code>DEFAULT2024SETTINGS['generator_params']['monotonic_fraction']</code> <code>curve_fraction</code> <code>float</code> <p>Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of             a piecewise linear Pareto frontier.</p> <code>DEFAULT2024SETTINGS['generator_params']['curve_fraction']</code> <code>pareto_first</code> <code>bool</code> <p>If given, the Pareto frontier will always be in the first set of outcomes</p> <code>DEFAULT2024SETTINGS['generator_params']['pareto_first']</code> <code>n_ufuns</code> <code>int</code> <p>Number of ufuns to generate per scenario</p> <code>DEFAULT2024SETTINGS['n_ufuns']</code> <code>n_pareto</code> <code>int | float | tuple[float | int, float | int] | list[int | float]</code> <p>Number of outcomes on the Pareto frontier in general scenarios.     Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities.     Each value can either be an integer &gt; 1 or a fraction of the number of outcomes in the scenario.</p> <code>DEFAULT2024SETTINGS['generator_params']['n_pareto']</code> <code>pareto_log_uniform</code> <code>bool</code> <p>Use log-uniform instead of uniform sampling if n_pareto is a tuple</p> <code>False</code> <code>n_trials</code> <p>Number of times to retry generating each scenario if failures occures</p> <code>10</code> <p>Returns:</p> Type Description <code>list[Scenario]</code> <p>A list <code>Scenario</code> s</p> Source code in <code>anl\\anl2024\\runner.py</code> <pre><code>def mixed_scenarios(\n    n_scenarios: int = DEFAULT2024SETTINGS[\"n_scenarios\"],  # type: ignore\n    n_outcomes: int\n    | tuple[int, int]\n    | list[int] = DEFAULT2024SETTINGS[\"n_outcomes\"],  # type: ignore\n    *,\n    reserved_ranges: ReservedRanges = DEFAULT2024SETTINGS[\"reserved_ranges\"],  # type: ignore\n    log_uniform: bool = DEFAULT2024SETTINGS[\"outcomes_log_uniform\"],  # type: ignore\n    zerosum_fraction: float = DEFAULT2024SETTINGS[\"generator_params\"][\"zerosum_fraction\"],  # type: ignore\n    monotonic_fraction: float = DEFAULT2024SETTINGS[\"generator_params\"][\"monotonic_fraction\"],  # type: ignore\n    curve_fraction: float = DEFAULT2024SETTINGS[\"generator_params\"][\"curve_fraction\"],  # type: ignore\n    pareto_first: bool = DEFAULT2024SETTINGS[\"generator_params\"][\"pareto_first\"],  # type: ignore\n    n_ufuns: int = DEFAULT2024SETTINGS[\"n_ufuns\"],  # type: ignore\n    n_pareto: int | float | tuple[float | int, float | int] | list[int | float] = DEFAULT2024SETTINGS[\"generator_params\"][\"n_pareto\"],  # type: ignore\n    pareto_log_uniform: bool = False,\n    n_trials=10,\n) -&gt; list[Scenario]:\n    \"\"\"Generates a mix of zero-sum, monotonic and general scenarios\n\n    Args:\n        n_scenarios: Number of scenarios to genearate\n        n_outcomes: Number of outcomes (or a list of range thereof).\n        reserved_ranges: the range allowed for reserved values for each ufun.\n                         Note that the upper limit will be overridden to guarantee\n                         the existence of at least one rational outcome\n        log_uniform: Use log-uniform instead of uniform sampling if n_outcomes is a tuple giving a range.\n        zerosum_fraction: Fraction of zero-sum scenarios. These are original DivideThePie scenarios\n        monotonic_fraction: Fraction of scenarios where each ufun is a monotonic function of the received pie.\n        curve_fraction: Fraction of general and monotonic scenarios that use a curve for Pareto generation instead of\n                        a piecewise linear Pareto frontier.\n        pareto_first: If given, the Pareto frontier will always be in the first set of outcomes\n        n_ufuns: Number of ufuns to generate per scenario\n        n_pareto: Number of outcomes on the Pareto frontier in general scenarios.\n                Can be specified as a number, a tuple of a min and max to sample within, a list of possibilities.\n                Each value can either be an integer &gt; 1 or a fraction of the number of outcomes in the scenario.\n        pareto_log_uniform: Use log-uniform instead of uniform sampling if n_pareto is a tuple\n        n_trials: Number of times to retry generating each scenario if failures occures\n\n    Returns:\n        A list `Scenario` s\n    \"\"\"\n    assert zerosum_fraction + monotonic_fraction &lt;= 1.0\n    nongeneral_fraction = zerosum_fraction + monotonic_fraction\n    ufun_sets = []\n    for i in range(n_scenarios):\n        r = random.random()\n        n = intin(n_outcomes, log_uniform)\n        name = \"S\"\n        if r &lt; nongeneral_fraction:\n            n_pareto_selected = n\n            name = \"DivideThePieGen\"\n        else:\n            if isinstance(n_pareto, Iterable):\n                n_pareto = type(n_pareto)(\n                    int(_ * n + 0.5) if _ &lt; 1 else int(_) for _ in n_pareto  # type: ignore\n                )\n            else:\n                n_pareto = int(0.5 + n_pareto * n) if n_pareto &lt; 1 else int(n_pareto)\n            n_pareto_selected = intin(n_pareto, log_uniform=pareto_log_uniform)  # type: ignore\n        for _ in range(n_trials):\n            try:\n                if r &lt; zerosum_fraction:\n                    vals = generate_utility_values(\n                        n_pareto=n_pareto_selected,\n                        n_outcomes=n,\n                        n_ufuns=n_ufuns,\n                        pareto_first=pareto_first,\n                        pareto_generator=\"zero_sum\",\n                    )\n                    name = \"DivideThePie\"\n                else:\n                    if n_pareto_selected &lt; 2:\n                        n_pareto_selected = 2\n                    vals = generate_utility_values(\n                        n_pareto=n_pareto_selected,\n                        n_outcomes=n,\n                        n_ufuns=n_ufuns,\n                        pareto_first=pareto_first,\n                        pareto_generator=\"curve\"\n                        if random.random() &lt; curve_fraction\n                        else \"piecewise_linear\",\n                    )\n                break\n            except:\n                continue\n        else:\n            continue\n\n        issues = (make_issue([f\"{i}_{n-1 - i}\" for i in range(n)], \"portions\"),)\n        ufuns = tuple(\n            U(\n                values=(\n                    TableFun(\n                        {_: float(vals[i][k]) for i, _ in enumerate(issues[0].all)}\n                    ),\n                ),\n                name=f\"{uname}{i}\",\n                # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)),\n                outcome_space=make_os(issues, name=f\"{name}{i}\"),\n            )\n            for k, uname in enumerate((\"First\", \"Second\"))\n            # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges))\n        )\n        sample_reserved_values(ufuns, reserved_ranges=reserved_ranges)\n        ufun_sets.append(ufuns)\n\n    return [\n        Scenario(\n            outcome_space=ufuns[0].outcome_space,  # type: ignore We are sure this is not None\n            ufuns=ufuns,\n        )\n        for ufuns in ufun_sets\n    ]\n</code></pre>"},{"location":"reference/#anl.anl2024.pie_scenarios","title":"<code>anl.anl2024.pie_scenarios</code>","text":"<p>Creates single-issue scenarios with arbitrary/monotonically increasing utility functions</p> <p>Parameters:</p> Name Type Description Default <code>n_scenarios</code> <code>int</code> <p>Number of scenarios to create</p> <code>20</code> <code>n_outcomes</code> <code>int | tuple[int, int] | list[int]</code> <p>Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from.         If a list, samples from this list will be used (with replacement).</p> <code>100</code> <code>reserved_ranges</code> <code>ReservedRanges</code> <p>Ranges of reserved values for first and second negotiators</p> <code>((0.0, 0.999999), (0.0, 0.999999))</code> <code>log_uniform</code> <code>bool</code> <p>If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple).</p> <code>True</code> <code>monotonic</code> <p>If true all ufuns are monotonically increasing in the portion of the pie</p> <code>False</code> Remarks <ul> <li>When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently.</li> </ul> Source code in <code>anl\\anl2024\\runner.py</code> <pre><code>def pie_scenarios(\n    n_scenarios: int = 20,\n    n_outcomes: int | tuple[int, int] | list[int] = 100,\n    *,\n    reserved_ranges: ReservedRanges = ((0.0, 0.999999), (0.0, 0.999999)),\n    log_uniform: bool = True,\n    monotonic=False,\n) -&gt; list[Scenario]:\n    \"\"\"Creates single-issue scenarios with arbitrary/monotonically increasing utility functions\n\n    Args:\n        n_scenarios: Number of scenarios to create\n        n_outcomes: Number of outcomes per scenario. If a tuple it will be interpreted as a min/max range to sample n. outcomes from.\n                    If a list, samples from this list will be used (with replacement).\n        reserved_ranges: Ranges of reserved values for first and second negotiators\n        log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple).\n        monotonic: If true all ufuns are monotonically increasing in the portion of the pie\n\n    Remarks:\n        - When n_outcomes is a tuple, the number of outcomes for each scenario will be sampled independently.\n    \"\"\"\n    ufun_sets = []\n    base_name = \"DivideTyePie\" if monotonic else \"S\"\n\n    def normalize(x):\n        mn, mx = x.min(), x.max()\n        return ((x - mn) / (mx - mn)).tolist()\n\n    def make_monotonic(x, i):\n        x = np.sort(np.asarray(x), axis=None)\n\n        if i:\n            x = x[::-1]\n        r = random.random()\n        if r &lt; 0.33:\n            x = np.exp(x)\n        elif r &lt; 0.67:\n            x = np.log(x)\n        else:\n            pass\n        return normalize(x)\n\n    max_jitter_level = 0.8\n    for i in range(n_scenarios):\n        n = intin(n_outcomes, log_uniform)\n        issues = (\n            make_issue(\n                [f\"{i}_{n-1 - i}\" for i in range(n)],\n                \"portions\" if not monotonic else \"i1\",\n            ),\n        )\n        # funs = [\n        #     dict(\n        #         zip(\n        #             issues[0].all,\n        #             # adjust(np.asarray([random.random() for _ in range(n)])),\n        #             generate(n, i),\n        #         )\n        #     )\n        #     for i in range(2)\n        # ]\n        os = make_os(issues, name=f\"{base_name}{i}\")\n        outcomes = list(os.enumerate_or_sample())\n        ufuns = U.generate_bilateral(\n            outcomes,\n            conflict_level=0.5 + 0.5 * random.random(),\n            conflict_delta=random.random(),\n        )\n        jitter_level = random.random() * max_jitter_level\n        funs = [\n            np.asarray([float(u(_)) for _ in outcomes])\n            + np.random.random() * jitter_level\n            for u in ufuns\n        ]\n\n        if monotonic:\n            funs = [make_monotonic(x, i) for i, x in enumerate(funs)]\n        else:\n            funs = [normalize(x) for x in funs]\n        ufuns = tuple(\n            U(\n                values=(TableFun(dict(zip(issues[0].all, vals))),),\n                name=f\"{uname}{i}\",\n                outcome_space=os,\n                # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)),\n            )\n            for (uname, vals) in zip((\"First\", \"Second\"), funs)\n            # for (uname, r, vals) in zip((\"First\", \"Second\"), reserved_ranges, funs)\n        )\n        sample_reserved_values(ufuns, reserved_ranges=reserved_ranges)\n        ufun_sets.append(ufuns)\n\n    return [\n        Scenario(\n            outcome_space=ufuns[0].outcome_space,  # type: ignore We are sure this is not None\n            ufuns=ufuns,\n        )\n        for ufuns in ufun_sets\n    ]\n</code></pre>"},{"location":"reference/#anl.anl2024.arbitrary_pie_scenarios","title":"<code>anl.anl2024.arbitrary_pie_scenarios</code>","text":"Source code in <code>anl\\anl2024\\runner.py</code> <pre><code>def arbitrary_pie_scenarios(\n    n_scenarios: int = 20,\n    n_outcomes: int | tuple[int, int] | list[int] = 100,\n    *,\n    reserved_ranges: ReservedRanges = ((0.0, 0.999999), (0.0, 0.999999)),\n    log_uniform: bool = True,\n) -&gt; list[Scenario]:\n    return pie_scenarios(\n        n_scenarios,\n        n_outcomes,\n        reserved_ranges=reserved_ranges,\n        log_uniform=log_uniform,\n        monotonic=False,\n    )\n</code></pre>"},{"location":"reference/#anl.anl2024.monotonic_pie_scenarios","title":"<code>anl.anl2024.monotonic_pie_scenarios</code>","text":"Source code in <code>anl\\anl2024\\runner.py</code> <pre><code>def monotonic_pie_scenarios(\n    n_scenarios: int = 20,\n    n_outcomes: int | tuple[int, int] | list[int] = 100,\n    *,\n    reserved_ranges: ReservedRanges = ((0.0, 0.999999), (0.0, 0.999999)),\n    log_uniform: bool = True,\n) -&gt; list[Scenario]:\n    return pie_scenarios(\n        n_scenarios,\n        n_outcomes,\n        reserved_ranges=reserved_ranges,\n        log_uniform=log_uniform,\n        monotonic=True,\n    )\n</code></pre>"},{"location":"reference/#anl.anl2024.zerosum_pie_scenarios","title":"<code>anl.anl2024.zerosum_pie_scenarios</code>","text":"<p>Creates scenarios all of the DivideThePie variety with proportions giving utility</p> <p>Parameters:</p> Name Type Description Default <code>n_scenarios</code> <code>int</code> <p>Number of scenarios to create</p> <code>20</code> <code>n_outcomes</code> <code>int | tuple[int, int] | list[int]</code> <p>Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from).</p> <code>100</code> <code>reserved_ranges</code> <code>ReservedRanges</code> <p>Ranges of reserved values for first and second negotiators</p> <code>((0.0, 0.499999), (0.0, 0.499999))</code> <code>log_uniform</code> <code>bool</code> <p>If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple).</p> <code>True</code> Remarks <ul> <li>When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently</li> </ul> Source code in <code>anl\\anl2024\\runner.py</code> <pre><code>def zerosum_pie_scenarios(\n    n_scenarios: int = 20,\n    n_outcomes: int | tuple[int, int] | list[int] = 100,\n    *,\n    reserved_ranges: ReservedRanges = ((0.0, 0.499999), (0.0, 0.499999)),\n    log_uniform: bool = True,\n) -&gt; list[Scenario]:\n    \"\"\"Creates scenarios all of the DivideThePie variety with proportions giving utility\n\n    Args:\n        n_scenarios: Number of scenarios to create\n        n_outcomes: Number of outcomes per scenario (if a tuple it will be interpreted as a min/max range to sample n. outcomes from).\n        reserved_ranges: Ranges of reserved values for first and second negotiators\n        log_uniform: If given, the distribution used will be uniform on the logarithm of n. outcomes (only used when n_outcomes is a 2-valued tuple).\n\n    Remarks:\n        - When n_outcomes is a tuple, the number of outcomes for each outcome will be sampled independently\n    \"\"\"\n    ufun_sets = []\n    for i in range(n_scenarios):\n        n = intin(n_outcomes, log_uniform)\n        issues = (make_issue([f\"{i}_{n-1 - i}\" for i in range(n)], \"portions\"),)\n        ufuns = tuple(\n            U(\n                values=(\n                    TableFun(\n                        {\n                            _: float(int(str(_).split(\"_\")[k]) / (n - 1))\n                            for _ in issues[0].all\n                        }\n                    ),\n                ),\n                name=f\"{uname}{i}\",\n                # reserved_value=(r[0] + random.random() * (r[1] - r[0] - 1e-8)),\n                outcome_space=make_os(issues, name=f\"DivideTyePie{i}\"),\n            )\n            for k, uname in enumerate((\"First\", \"Second\"))\n            # for k, (uname, r) in enumerate(zip((\"First\", \"Second\"), reserved_ranges))\n        )\n        sample_reserved_values(\n            ufuns,\n            pareto=tuple(\n                tuple(u(_) for u in ufuns)\n                for _ in make_os(issues).enumerate_or_sample()\n            ),\n            reserved_ranges=reserved_ranges,\n        )\n        ufun_sets.append(ufuns)\n\n    return [\n        Scenario(\n            outcome_space=ufuns[0].outcome_space,  # type: ignore We are sure this is not None\n            ufuns=ufuns,\n        )\n        for ufuns in ufun_sets\n    ]\n</code></pre>"},{"location":"tutorials/04.ideas/","title":"04.ideas","text":""},{"location":"tutorials/04.ideas/#ideas-for-developing-your-agent","title":"Ideas for developing your agent","text":"<p>This section of the tutorials will discuss some possible ideas for developing your agent. It is completely optional to read this but it may provide some directions that help you in your quest. We will assume that you are using the component based approach discussed in the second tutorial.</p> <p>Let\u2019s start by reminding ourselves of the agent decomposition used by built in agents (check this <code>video &lt;https://youtu.be/3xwR-aPZSb0&gt;</code>__ explains the main components in details).</p> <p>The three main components of an agent in this decomposition are the trading strategy, negotiation control strategy and production strategy.</p> <p>.. image:: anatomy.png</p> <p>The trading strategy decides what should the agent buy and sell (the trading schedule) and the negotiation control strategy takes that as input and uses it to drive negotiations in order to carry out this plan. The production strategy controls the factory by deciding how many items to produce at every time step (based on existing inventory and the trading schedule).</p> <p>We will discuss ideas for improving each one of these three components separately.</p> <p>Before diving into these ideas, it is important to note that the overall performance of the agent does not come from having one perfect component but from harmony between all the components constituting it. For example, a trading strategy that generates a perfect trading schedule is useless without a negotiation control strategy capable of achieving that schedule.</p> <p>Trading Strategy ~~~~~~~~~~~~~~~~</p> <p>Representing the planning department of a company, the trading strategy seems like the obvious target of improvement. This figure shows the outputs of the trading strategy and the three examples implemented in the <code>scml</code> package.</p> <p>.. image:: trading.png</p> <p>The best trading strategy used by the built-in agents is the <code>PredictionBasedTradingStrategy</code> and we will focus on it as it seems the most amenable to improvement.</p> <p>This trading strategy uses two components, a <code>TradePredictionStrategy</code> that predicts the amount of trade on the input and output products of the agent as a function of the simulation step, and an <code>ERPredictionStrategy</code> predicting the quantity that will actually be executed from a contract. These predictions are both set to constants for the built-in component. This immediately suggests the following ideas</p> <p>IDEA 1: Improve trade prediction ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>The only <code>TradePredictionStrategy</code> implemented in <code>scml</code> is the <code>FixedTradePredictionStrategy</code> which predicts trade at a fixed amount in every product at every step (currently set to half the number of lines: :math:<code>5</code>). This can definitely be improved.</p> <ol> <li>Train a regressor (e.g.\u00a0a    <code>scikit-learn &lt;https://scikit-learn.org/stable/user_guide.html&gt;</code>    regressor) on many worlds to receive the product number and the    fraction of the simulation steps passed and predict the amount of    trade and use this regressor in real time (or store its results in a    table that you can <code>load in real    time &lt;https://scml.readthedocs.io/en/latest/faq.html#how-can-i-access-a-data-file-in-my-package&gt;</code>).</li> <li>Improve the regressor using incremental learning in real time during    world simulation. This may not be very effective in short simulations    but we will simulate up to :math:<code>200</code> steps so it may improve    performance.</li> </ol> <p>IDEA 2: Improve execution rate prediction ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>The only <code>ERPredictionStrategy</code> implemented in the system is the <code>FixedERPredictionStrategy</code> which will expect that half of the quantity in any contract will be executed. This can easily be improved using several approaches.</p> <ol> <li>Use the financial reports of your suppliers and consumers to predict    the possibility that they will breach contracts in the future. Again    you can train a regressor that receives few past financial reports    and predicts future behavior using simulations against a variety of    agents (including your own!) and then load it in real time.</li> <li>Use more general market conditions for prediction of actual trade    amount and base your prediction of the contract execution rate on    that.</li> </ol> <p>IDEA3: Improve the logic of the trading strategy ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>The <code>PredictionBasedTradingStrategy</code> just uses the <code>TradePredictionStrategy</code> and <code>ERPredictionStrategy</code> directly for deciding trade but that need not be the optimal thing to do. It may be possible to change that logic of the trading strategy itself to add a higher level of control over the outputs of these base prediction strategies.</p> <p>Negotiation Manager ~~~~~~~~~~~~~~~~~~~</p> <p>This is a negotiation competition and it seems fit to focus our efforts on negotiation. Moreover, as we indicated earlier, having the perfect trade schedule coming out from the trading strategy is useless for the agent if it cannot negotiate effectively to achieve that schedule.</p> <p>The negotiation control strategy consists of two main components:</p> <ul> <li>Negotiation Manager responsible of requesting negotiations as    needed and responding to such requests</li> <li>Negotiation Algorithm which can be implemented using one or more    <code>negmas</code> <code>SAOController &lt;https://negmas.readthedocs.io/en/latest/modules/sao.html?highlight=Controller#module-negmas.sao&gt;</code>    or directly using <code>negmas</code> <code>SAONegotiator &lt;https://negmas.readthedocs.io/en/latest/modules/sao.html?highlight=Negotiator#module-negmas.sao&gt;</code>.    This <code>video &lt;https://youtu.be/10Rjl3ikaDU&gt;</code>__ describes available    controllers and negotiators and of course you can - and should -    design your own.</li> </ul> <p>This figure shows the two inputs you need to define for any negotiation manager: <code>target_quantity</code> and <code>acceptable_unit_price</code>. Their names are self-descriptive.</p> <p>.. image:: negotiation.png</p> <p>Built-in negotiation managers are intentionally pretty basic. It may be that this is the point of improvement that has the highest probability of leading to winning agents (that may not be true though as the trading strategy seems as important). Here are some ideas for improving the negotiation control strategy</p> <p>IDEA 4: Improve the negotiation manager ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>The negotiation manager responsible of starting and accepting negotiations in <code>scml</code> is extremely basic.</p> <ol> <li>It uses a target quantity that is set directly as the difference    between needs and secured quantity and it does not take into account    in any way running negotiations. You can access running negotiations    using <code>self.negotiations</code> and standing negotiation requests using    <code>self.negotiation_requests</code>.</li> <li>It always negotiates with everybody. You can use financial reports to    decide whom to negotiate with.</li> <li>It uses fixed ranges for negotiation issues. You can try to    dynamically decide the ranges allowed for negotiation issues based on    market conditions. For example, you can set the range of prices based    on your estimate of the current trading price of products.</li> </ol> <p>IDEA 5 Improve signing strategy ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Deciding what to sign is not strictly a part of the negotiation strategy but it needs to be implemented to respond to <code>sign_all_contracts</code>. Currently, it is handled by the trading strategy but you can override that by providing your own <code>SigningStrategy</code> that overrides <code>sign_all_contracts</code>.</p> <p>All negotiations in a single simulation step run in parallel. This means that the negotiation manager is prone to over-contracting. This can then be corrected using a <code>SigningStrategy</code> that intelligently decides what to sign.</p> <p>Negotiation Algorithm ~~~~~~~~~~~~~~~~~~~~~</p> <p>All built in negotiations are conducted using either simple negotiation algorithm (e.g.\u00a0time-based strategy, naive tit-for-tat implementation, \u2026) or a simple <code>negmas</code> built in controller. None of the adequately handles the two main challenges: concurrent negotiations within a single simulation step and taking into account future negotiation opportunities.</p> <p>IDEA 6: Improve concurrent negotiation control ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>The <code>StepController</code> is the negotiation algorithm used by the <code>StepNegotiationManager</code> employed by the <code>DecentralizingAgent</code> (the top built-in agent). It instantiates one controller to handle buying and another to handle selling for each simulation step. These controllers rely heavily on the <code>SAOSyncController</code> of <code>negmas</code> using a time-based meta-negotiation strategy. That is a very simple algorithm that is not expected to effectively handle concurrent negotiations. Try to find a way to either coordinate the behavior of multiple autonomous negotiators each simulation step or to centrally control these negotiators to achieve the preset target.</p> <p>IDEA 7: Improve sequential negotiation control ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Agents in SCML negotiate repeatedly. This means that the utility of any offer in any negotiation does not only depend on current market conditions but also in expected future negotiations. Built-in agents side step the need to take that into account during negotiation by having a trading strategy and a negotiation manager set their targets for them rendering negotiations in every simulation step independent from future negotiations (given the targets). This is clearly a simplistic heuristic. Try to find a way to take future negotiations into account when designing your agent. One way to do that is to have them affect the utility function used by your controller/negotiator.</p> <p>IDEA 8: Improve the utility functions used ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>The <code>IndependentNegotiationManager</code> uses linear independent utility functions with a simple time-base negotiation (<code>AspirationNegotiator</code>) for all of its negotiations. The other two negotiation managers employ controllers that define their utilities linearly using some built-in fixed weights for price and quantity. That is obviously suboptimal. 1. Try to improve the utility function used by either the negotiators or the controller (depending on the negotiation manager you use) to achieve higher expected utilities. 2. Try to take the identity of the agent you are negotiating with into account in your utility calculations. A contract with a trustworthy agent has more utility than one with a non-trustworthy agent. You can use the financial reports of agents to judge their trustworthiness.</p> <p>Production Strategy ~~~~~~~~~~~~~~~~~~~</p> <p>That is the simplest of the three components. There are two main production strategies in <code>scml</code> as described earlier in the second tutorial: supply based or demand based production strategies.</p> <p>IDEA 9: Base production decisions on trading prices (as well as contracts). ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>Given that disposal cost is zero and storage capacity is infinite, it seems that the only optimization you can do is to avoid over production. Production has cost so over production may not be a good idea. On the other hand, the inventory is valued in SCML 2020 at half the trading price which means that it may be a good idea to convert inputs to outputs (even if you do not sell that output) if the difference in trading prices at the end of simulation offsets your production costs. Try creating a production strategy that takes this effect into account switching between supply based and demand based production using a estimate of the final trading prices of its input and output products.</p> <p>Final Remarks ~~~~~~~~~~~~~</p> <p>The ideas presented above are, by no means, exclusive or comprehensive. You can combine them and add new ones. The main reason we present these ideas is to challenge you to come with better ones. Happy coding :-) Download Notebook</p>"},{"location":"tutorials/Tutorial_run_a_negotiation/","title":"Running a negotiation","text":"<pre><code>%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# setup disply parameters\nfrom matplotlib import pylab as plt\nimport seaborn as sns\nfrom matplotlib.ticker import StrMethodFormatter\nfloat_formatter = StrMethodFormatter('{x:0.03f}')\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:95% !important; }&lt;/style&gt;\"))\nSMALL_SIZE = 14\nMEDIUM_SIZE = 16\nBIGGER_SIZE = 20\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\nplt.rc('figure', figsize=(18, 6)) # set figure size\nplt.rc(\"animation\", html=\"html5\")\nimport random\nrandom.seed(345)\nimport numpy as np\nnp.random.seed(345)\nfrom rich import print\n</code></pre>"},{"location":"tutorials/Tutorial_run_a_negotiation/#running-a-negotiation","title":"Running a negotiation","text":"<p>NegMAS has several built-in negotiation <code>Mechanisms</code>, negotiation agents (<code>Negotiators</code>), and <code>UtilityFunctions</code>. You can use these to run negotiations as follows.</p> <p>Imagine a buyer and a seller negotiating over the price of a single object. First, we make an issue \"price\" with 50 discrete values. Note here, it is possible to create multiple issues, but we will not include that here. If you are interested, see the NegMAS documentation for a tutorial.</p> <pre><code>from negmas import (\n    make_issue,\n    SAOMechanism,\n   TimeBasedConcedingNegotiator,\n)\nfrom anl.anl2024.negotiators import Boulware, Conceder, RVFitter\nfrom negmas.preferences import LinearAdditiveUtilityFunction as UFun\nfrom negmas.preferences.value_fun import IdentityFun, AffineFun\nimport matplotlib.pyplot as plt\n\n\n# create negotiation agenda (issues)\nissues = [\n    make_issue(name=\"price\", values=50),\n]\n\n# create the mechanism\nsession = SAOMechanism(issues=issues, n_steps=20)\n</code></pre> <p>The negotiation protocol in NegMAS is handled by a <code>Mechanism</code> object. Here we instantiate a<code>SAOMechanism</code> which implements the Stacked Alternating Offers Protocol. In this protocol, negotiators exchange offers until an offer is accepted by all negotiators (in this case 2), a negotiators leaves the table ending the negotiation or a time-out condition is met. In the example above, we use a limit on the number of rounds of <code>20</code> (a step of a mechanism is an executed round).</p> <p>Next, we define the utilities of the seller and the buyer. The utility function of the seller is defined by the <code>IdentityFun</code>  which means that the higher the price, the higher the utility function. The buyer's utility function is reversed. The last two lines make sure that utility is scaled between 0 and 1.</p> <pre><code>seller_utility = UFun(\n    values=[IdentityFun()],\n    outcome_space=session.outcome_space,\n)\n\nbuyer_utility = UFun(\n    values=[AffineFun(slope=-1)],\n    outcome_space=session.outcome_space,\n)\n\nseller_utility = seller_utility.normalize()\nbuyer_utility = buyer_utility.normalize()\n</code></pre> <p>Then we add two agents with a boulware strategy. The negotiation ends with status overview. For example, you can see if the negotiation timed-out, what agreement was found, and how long the negotiation took. Moreover, we output the full negotiation history. For a more visual representation, we can plot the session. This shows the bidding curve, but also the proximity to e.g. the Nash point.</p> <pre><code># create and add agent A and B\nsession.add(Boulware(name=\"seller\"), ufun=seller_utility)\nsession.add(Boulware(name=\"buyer\"), ufun=buyer_utility)\n\n# run the negotiation and show the results\nprint(session.run())\n\n# negotiation history\nfor i, _ in enumerate(session.history):\n    print(f\"{i:03}: {_.new_offers}\")  # the first line gives the offer of the seller and the buyer  in the first round\n\nsession.plot(ylimits=(0.0, 1.01), show_reserved=False, mark_max_welfare_points=False)\nplt.show()\n</code></pre> <pre>SAOState(\n    running=False,\n    waiting=False,\n    started=True,\n    step=18,\n    time=0.006180799915455282,\n    relative_time=0.9047619047619048,\n    broken=False,\n    timedout=False,\n    agreement=(23,),\n    results=None,\n    n_negotiators=2,\n    has_error=False,\n    error_details='',\n    threads={},\n    last_thread='',\n    current_offer=(23,),\n    current_proposer='seller-69622b52-7474-4027-bdda-e2928ad686cf',\n    current_proposer_agent=None,\n    n_acceptances=2,\n    new_offers=[('seller-69622b52-7474-4027-bdda-e2928ad686cf', (23,))],\n    new_offerer_agents=[None],\n    last_negotiator='seller'\n)\n</pre> <p></p> <p>Download Notebook</p>"},{"location":"tutorials/tutorial_develop/","title":"ANL 2024 Tutorial","text":""},{"location":"tutorials/tutorial_develop/#developing-a-negotiator","title":"Developing a negotiator","text":"<p>The agents for the ANL competition are standard NegMAS negotiators. As such, they can be developed using any approach used to develop negotiators in NegMAS.</p> <p>To develop a negotiator, you need to inherit from the SAONegotiator class and implement the <code>__call__()</code> method.</p> <p>Here is a simple random negotiator:</p> <pre><code>import random\nfrom negmas.sao import SAONegotiator, SAOResponse\nfrom negmas import Outcome, ResponseType\nclass MyRandomNegotiator(SAONegotiator):\n    def __call__(self, state):\n        offer = state.current_offer\n        if offer is not None and self.ufun.is_not_worse(offer, None) and random.random() &lt; 0.25 :\n            return SAOResponse(ResponseType.ACCEPT_OFFER, offer)\n        return SAOResponse(ResponseType.REJECT_OFFER, self.nmi.random_outcomes(1)[0])\n</code></pre>"},{"location":"tutorials/tutorial_develop/#testing-the-agent","title":"Testing the agent","text":"<pre><code>from anl.anl2024 import anl2024_tournament\nfrom anl.anl2024.negotiators import Boulware, Conceder, RVFitter\n</code></pre> <pre><code>results = anl2024_tournament(\n    n_scenarios=1, n_repetitions=3, nologs=True, njobs=-1,\n    competitors=[MyRandomNegotiator, Boulware]\n)\n</code></pre> <pre>Will run 12 negotiations on 1 scenarios between 2 competitors\n</pre> <pre><code>Output()\n</code></pre> <pre></pre> <pre>\n</pre> <pre>             strategy     score\n0            Boulware  0.701151\n1  MyRandomNegotiator  0.059973\n</pre> <p>We can immediately notice that <code>MyRandomNegotiator</code> is getting a negative average advantage which means that it sometimes gets agreements that are worse than disagreement (i.e. with utility less than its reserved value). Can you guess why is this happening? How can we resolve that?</p>"},{"location":"tutorials/tutorial_develop/#note-about-running-tournaments","title":"Note about running tournaments","text":"<ul> <li>When running a tournament using <code>anl2024_tournament</code> inside a Jupyter Notebook, you must pass <code>njobs=-1</code> to force serial execution of negotiations. This is required because the multiprocessing library used by NegMAS does not play nicely with Jupyter Notebooks. If you run the tournament using the same method from a <code>.py</code> python script file, you can omit this argument to run a tournament using all available cores.</li> <li>When you pass <code>nologs=True</code>, no logs are stored for this tournament. If you omit this argument, a log will be created under <code>~/negmas/anl2024/tournaments</code> which can be visualized using the ANL visualizer by running:</li> </ul> <pre><code>anlv show\n</code></pre>"},{"location":"tutorials/tutorial_develop/#back-to-the-tutorial","title":"Back to the tutorial","text":"<p>You can easily check the final scores using the <code>final_scores</code> member of the returned SimpleTournamentResults object.</p> <pre><code>results.final_scores\n</code></pre> strategy score 0 Boulware 0.701151 1 MyRandomNegotiator 0.059973 <p>The returned results are all pandas dataframes. We can use standard pandas functions to get deeper understanding of the results. Here is how to plot a KDE figure comparing different strategies in this tournament:</p> <pre><code>fig, ax = plt.subplots(figsize=(8,6))\ndf = results.scores\nfor label, data in df.groupby('strategy'):\n    data.advantage.plot(kind=\"kde\", ax=ax, label=label)\nplt.ylabel(\"advantage\")\nplt.legend();\n</code></pre> <p></p> <pre><code>fig, axs = plt.subplots(1, 3, figsize=(16,4))\nfor i, col in enumerate([\"advantage\", \"welfare\", \"nash_optimality\"]):\n    results.scores.groupby(\"strategy\")[col].mean().sort_index().plot(kind=\"bar\", ax=axs[i])\n    axs[i].set_ylabel(col)\n</code></pre> <p></p>"},{"location":"tutorials/tutorial_develop/#available-helpers","title":"Available helpers","text":"<p>Our negotaitor was not so good but it examplifies the simplest method for developing a negotiator in NegMAS. For more information refer to NegMAS Documentation. You develop your agent, as explained above, by implementing the <code>__call__</code> method of your class.</p> <p>This method, receives an SAOState which represents the current <code>state</code> of the negotiation. The most important members of this state object are <code>current_offer</code> which gives the current offer from the partner (or <code>None</code> if this is the beginning of the negotiation) and <code>relative_time</code> which gives the relative time of the negotiation ranging between <code>0</code> and <code>1</code>.</p> <p>It should return an SAOResponse represeting the agent's <code>response</code> which consists of two parts:</p> <ol> <li>A ResponseType with the following allowed values:<ul> <li><code>ResponseType.ACCEPT_OFFER</code>, accepts the current offer (pass the current offer as the second member of the response).</li> <li><code>ResponseType.REJECT_OFFER</code>, rejects the current offer (pass you counter-offer as the second member of the response).</li> <li><code>ResponseType.END_NEGOTIATION</code>, ends the negotiation immediately (pass <code>None</code> as the second member of the response).</li> </ul> </li> <li>A counter offer (in case of rejection), the received offer (in case of acceptance) or <code>None</code> if ending the negotiation.</li> </ol> <p>The negotiator can use the following objects to help it implement its strategy:</p> <ul> <li><code>self.nmi</code> A SAONMI that gives you access to all the settings of this negotiation and provide some simple helpers:<ul> <li><code>n_steps</code>, <code>time_limit</code> The number of rounds and seconds allowed for this negotiation (<code>None</code> means no limit).</li> <li><code>random_outcomes(n)</code> Samples <code>n</code> random outcomes from this negotiation.</li> <li><code>outcome_space</code> The OutcomeSpace of the negotiation which represent all possible agreements. In ANL 2024, this will always be of type DiscreteCartesianOutcomeSpace with a single issue.</li> <li><code>discrete_outcomes()</code> A generator of all outcomes in the outcome space.</li> <li><code>log_info()</code> Logs structured information for this negotiator that can be checked in the logs later (Similarily there are <code>log_error</code>, <code>log_warning</code>, <code>log_debug</code>).</li> </ul> </li> <li><code>self.ufun</code> A LinearAdditiveUtilityFunction representing the agent's own utility function. This object provides some helpful functionality including:</li> <li><code>self.ufun.is_better(a, b)</code> Tests if outcome <code>a</code> is better than <code>b</code> (use <code>None</code> for disagreement). Similarily we have, <code>is_worse</code>, <code>is_not_worse</code> and <code>is_not_better</code>.</li> <li><code>self.ufun.reserved_value</code> Your negotiator's reserved value (between 0 and 1). You can access this also as <code>self.ufun(None)</code>.</li> <li><code>self.ufun(w)</code> Returns the utility value of the outcome <code>w</code>. It is recommended to cast this value to float (i.e. <code>float(self.ufun(w)</code>) to support probabilistic utility functions.</li> <li><code>self.outcome_space</code> The OutcomeSpace of the negotiation which represent all possible agreements. In ANL 2024, this will always be of type DiscreteCartesianOutcomeSpace with a single issue.</li> <li><code>self.ufun.invert()</code> Returns and caches an InverseUtilityFunction object which can be used to find outcomes given their utilities. The most important services provided by the InverseUtilityFunction returned are:<ul> <li><code>minmax()</code> returns the minimum and maximum values of the ufun (will always be (0, 1) approximately in ANL 2024).</li> <li><code>best()</code>, <code>worst()</code> returns the best (worst) outcomes.</li> <li><code>one_in()</code>, <code>some_in()</code> returns one (or some) outcomes within the given range of utilities.</li> <li><code>next_better()</code>, <code>next_worse()</code> returns the next outcome descendingly (ascendingly) in utility value.</li> </ul> </li> <li><code>self.opponent_ufun</code> A LinearAdditiveUtilityFunction representing the opponent's utility function. You can access this also as <code>self.private_info[\"opponent_ufun\"]</code>. This utility function will have a zero reserved value indepdendent of the opponent's true reserved value. You can actually set the reserved value on this object to your best estimate. All ufun funcationality is available in this object.</li> </ul> <p>Other than these objects, your negotiator can access any of the analytic facilities available in NegMAS. For example, you can calculate the pareto_frontier, Nash Bargaining Soluion, Kalai Bargaining Solution, points with maximum wellfare, etc. You can check the implementation of the NashSeeker agent for examples of using these facilities.</p> <p>Other than implementing the <code>__call__</code>, method you can optionally implement one or more of the following callbacks to initialize your agent:</p> <ul> <li><code>on_negotiation_start(state: SAOState)</code> This callback is called once per negotiation after the ufuns are set but before any offers are exchanged.</li> <li><code>on_preferences_changed(changes)</code> This callback is called whenever your negotiator's ufun is changed. This will happen at the beginning of each negotiation but it can also happen again if the ufun is changed while the negotiation is running. In ANL 2024, ufuns never change during the negotiation so this callback is equivalent to <code>on_negotiation_start()</code> but for future proofness, you should use this callback for any initialization instead to guarantee that this initialization will be re-run in cases of changing utility function.</li> </ul>"},{"location":"tutorials/tutorial_develop/#understanding-our-negotiator","title":"Understanding our negotiator","text":"<p>Now we can analyze the simple random negotiator we developed earlier.</p> <ul> <li>Firstly, we find the current offer that we need to respond to:   <pre><code>offer = state.current_offer\n</code></pre></li> <li>Acceptance Strategy We then accept this offer if three conditions are satisfied:</li> <li>The offer is not <code>None</code> which means that we are not starting the negotiation just now:</li> <li>The offer is not worse than disagreement. This prevents us from accepting irrational outcomes.</li> <li>A random number we generated is less than 0.25. This means we accept rational offers with probability 25%.     <pre><code>if offer is not None and self.ufun.is_not_worse(offer, None) and random.random() &lt; 0.25:\n    return SAOResponse(ResponseType.ACCEPT_OFFER, offer)\n</code></pre></li> <li>Offering Strategy If we decided not to accept the offer, we simply generate a single random outcome and offer it:   <pre><code>return SAOResponse(ResponseType.REJECT_OFFER, self.nmi.random_outcomes(1)[0])\n</code></pre></li> </ul> <p>This negotiator did not use the fact that we know the opponent utility function up to reserved value. It did not even use the fact that we know our own utility function. As expected, it did not get a good score. Let's develop a simple yet more meaningful agent that uses both of these pieces of information.</p> <p>Can you now see why is this negotiator is getting negative advantages sometimes? We were careful in our acceptance strategy but not in our offering strategy. There is nothing in our code that prevents our negotiator from offering irrational outcomes (i.e. outcomes worse than disagreement for itself) and sometimes the opponent will just accept those. Can you fix this?</p>"},{"location":"tutorials/tutorial_develop/#a-more-meaningful-negotiator","title":"A more meaningful negotiator","text":"<p>How can we use knowledge of our own and our opponent's utility functions (up to reserved value for them)? Here is one possibility:</p> <ul> <li>Acceptance Strategy We accept offers that have a utility above some aspiration level. This aspiration level starts very high (1.0) and goes monotoncially down but never under the reserved value which is reached when the relative time is 1.0 (i.e. by the end of the negotiation). This is implemented in <code>is_acceptable()</code> below.</li> <li> <p>Opponent Modeling We estimate the opponent reserved value under the assumption that they are using a monotonically decreasing curve to select a utility value and offer an outcome around it. This is implemented in <code>update_reserved_value()</code> below.</p> </li> <li> <p>Bidding Strategy Once we have an estimate of their reserved value, we can then find out all outcomes that are rational for both we and them. We can then check the relative time of the negotiation and offer outcomes by conceding over this list of rational outcomes. This is implemented in the <code>generate_offer()</code> method below.</p> </li> </ul> <pre><code>from scipy.optimize import curve_fit\n\ndef aspiration_function(t, mx, rv, e):\n    \"\"\"A monotonically decreasing curve starting at mx (t=0) and ending at rv (t=1)\"\"\"\n    return (mx - rv) * (1.0 - np.power(t, e)) + rv\n\n\nclass SimpleRVFitter(SAONegotiator):\n    \"\"\"A simple curve fitting modeling agent\"\"\"\n    def __init__(self, *args, e: float = 5.0, **kwargs):\n        \"\"\"Initialization\"\"\"\n        super().__init__(*args, **kwargs)\n        self.e = e\n        # keeps track of times at which the opponent offers\n        self.opponent_times: list[float] = []\n        # keeps track of opponent utilities of its offers\n        self.opponent_utilities: list[float] = []\n        # keeps track of our last estimate of the opponent reserved value\n        self._past_oppnent_rv = 0.0\n        # keeps track of the rational outcome set given our estimate of the\n        # opponent reserved value and our knowledge of ours\n        self._rational: list[tuple[float, float, Outcome]] = []\n\n    def __call__(self, state):\n        # update the opponent reserved value in self.opponent_ufun\n        self.update_reserved_value(state.current_offer, state.relative_time)\n        # run the acceptance strategy and if the offer received is acceptable, accept it\n        if self.is_acceptable(state.current_offer, state.relative_time):\n            return SAOResponse(ResponseType.ACCEPT_OFFER, state.current_offer)\n        # call the offering strategy\n        return SAOResponse(ResponseType.REJECT_OFFER, self.generate_offer(state.relative_time))\n\n    def generate_offer(self, relative_time) -&gt; Outcome:\n        # The offering strategy\n        # We only update our estimate of the rational list of outcomes if it is not set or\n        # there is a change in estimated reserved value\n        if (\n            not self._rational\n            or abs(self.opponent_ufun.reserved_value - self._past_oppnent_rv) &gt; 1e-3\n        ):\n            # The rational set of outcomes sorted dependingly according to our utility function\n            # and the opponent utility function (in that order).\n            self._rational = sorted(\n                [\n                    (my_util, opp_util, _)\n                    for _ in self.nmi.outcome_space.enumerate_or_sample(\n                        levels=10, max_cardinality=100_000\n                    )\n                    if (my_util := float(self.ufun(_))) &gt; self.ufun.reserved_value\n                    and (opp_util := float(self.opponent_ufun(_)))\n                    &gt; self.opponent_ufun.reserved_value\n                ],\n            )\n        # If there are no rational outcomes (e.g. our estimate of the opponent rv is very wrong),\n        # then just revert to offering our top offer\n        if not self._rational:\n            return self.ufun.best()\n        # find our aspiration level (value between 0 and 1) the higher the higher utility we require\n        asp = aspiration_function(relative_time, 1.0, 0.0, self.e)\n        # find the index of the rational outcome at the aspiration level (in the rational set of outcomes)\n        max_rational = len(self._rational) - 1\n        indx = max(0, min(max_rational, int(asp * max_rational)))\n        outcome = self._rational[indx][-1]\n        return outcome\n\n    def is_acceptable(self, offer, relative_time) -&gt; bool:\n        \"\"\"The acceptance strategy\"\"\"\n        # If there is no offer, there is nothing to accept\n        if offer is None:\n            return False\n        # Find the current aspiration level\n        asp = aspiration_function(\n            relative_time, 1.0, self.ufun.reserved_value, self.e\n        )\n        # accept if the utility of the received offer is higher than\n        # the current aspiration\n        return float(self.ufun(offer)) &gt;= asp\n\n    def update_reserved_value(self, offer, relative_time):\n        \"\"\"Learns the reserved value of the partner\"\"\"\n        if offer is None:\n            return\n        # save to the list of utilities received from the opponent and their times\n        self.opponent_utilities.append(float(self.opponent_ufun(offer)))\n        self.opponent_times.append(relative_time)\n        # Use curve fitting to estimate the opponent reserved value\n        # We assume the following:\n        # - The opponent is using a concession strategy with an exponent between 0.2, 5.0\n        # - The opponent never offers outcomes lower than their reserved value which means\n        #   that their rv must be no higher than the worst outcome they offered for themselves.\n        bounds = ((0.2, 0.0), (5.0, min(self.opponent_utilities)))\n        try:\n            optimal_vals, _ = curve_fit(\n                lambda x, e, rv: aspiration_function(\n                    x, self.opponent_utilities[0], rv, e\n                ),\n                self.opponent_times,\n                self.opponent_utilities,\n                bounds=bounds,\n            )\n            self._past_oppnent_rv = self.opponent_ufun.reserved_value\n            self.opponent_ufun.reserved_value = optimal_vals[1]\n        except Exception as e:\n            pass\n</code></pre> <pre><code>anl2024_tournament(\n    n_scenarios=1, n_repetitions=3, nologs=True, njobs=-1,\n    competitors=[MyRandomNegotiator, SimpleRVFitter, Boulware, Conceder]\n).final_scores\n</code></pre> <pre>Will run 48 negotiations on 1 scenarios between 4 competitors\n</pre> <pre><code>Output()\n</code></pre> <pre></pre> <pre>\n</pre> <pre>             strategy     score\n0            Boulware  0.625088\n1      SimpleRVFitter  0.572325\n2            Conceder  0.363117\n3  MyRandomNegotiator -0.318512\n</pre> strategy score 0 Boulware 0.625088 1 SimpleRVFitter 0.572325 2 Conceder 0.363117 3 MyRandomNegotiator -0.318512 <p>Much better :-)</p> <p>Let's see how each part of this negotiator works and how they fit together.</p>"},{"location":"tutorials/tutorial_develop/#construction","title":"Construction","text":"<p>The first method of the negotiator to be called is the <code>__init__</code> method which is called when the negotiator is created usually before the ufun is set. You can use this method to construct the negotiator setting initial values for any variables you need to run your agent.</p> <p>An important thing to note here is that your negotiator must pass any parameters it does not use to its parent to make sure the object is constructed correctly. This is how we implement this in our <code>SimpleRVFitter</code>:</p> <pre><code>def __init__(self, *args, e: float = 5.0, **kwargs):\n    super().__init__(*args, **kwargs)\n</code></pre> <p>We then set the variables we need for our negotiator:</p> <ul> <li><code>self.e</code> stores the exponent of the concession curve we will be use (more on that later).</li> <li><code>self.opponent_times</code>, <code>self.opponent_utilities</code> keep track of the times the opponent offers and its own utility of its offers. We will use that to estimate the opponent's reserved value using simple curve fitting in <code>update_reserved_values()</code>.</li> <li><code>self._past_oppnent_rv = 0.0</code> We start assuming that the opponent has zero reserved value. This is an optimistic assumption because it means that anything rational for us is rational for the opponent so we have more negotiation power.</li> <li><code>self._rational</code> This is where we will store the list of rational outcomes to concede over. For each outcome we will store our utility, opponent utility and the outcome itself (in that order).</li> </ul>"},{"location":"tutorials/tutorial_develop/#overall-algorithm","title":"Overall Algorithm","text":"<p>The overall algorithm is implemented --- as usual --- in the <code>__call__()</code> method. This is the complete algorithm:</p> <pre><code>def __call__(self, state):\n    self.update_reserved_value(state.current_offer, state.relative_time)\n    if self.is_acceptable(state.current_offer, state.relative_time):\n        return SAOResponse(ResponseType.ACCEPT_OFFER, state.current_offer)\n    return SAOResponse(ResponseType.REJECT_OFFER, self.generate_offer(state.relative_time))\n</code></pre> <p>We start by updating our estimate of the reserved value of the opponent using <code>update_reserved_value()</code>. We then call the acceptance strategy <code>is_acceptable()</code> to check whether the current offer should be accepted. If the current offer is not acceptable, we call the bidding strategy <code>generate_offer()</code> to generate a new offer which we return as our counter-offer. Simple!!</p>"},{"location":"tutorials/tutorial_develop/#opponent-modeling-estimating-reserved-value","title":"Opponent Modeling (Estimating Reserved Value)","text":"<p>The first step is in our algorithm is to update our estimate of the opponent's reserved value. This is done in three simple steps:</p> <ol> <li>If we have not offer from the opponent, there is nothing to do. Just return:    <pre><code>if offer is None:\n     return\n</code></pre></li> <li>We append the time and opponent's utility to our running list of opponent offer utilities:    <pre><code>self.opponent_utilities.append(float(self.opponent_ufun(offer)))\nself.opponent_times.append(relative_time)\n</code></pre></li> <li> <p>We apply a simple curve fitting algorithm from scipy to estimate the opponent's reserved value (and its concession exponent but we are not going to use that):</p> <ul> <li>We set the bounds of the reserved value to be between zero (minimum possible value) and the minimum utility the opponent ever offered. This assumes that the opponent only offers rational outcomes for itself. The bounds for the concession curve are set to (0.2, 5.0) which is the usual range of exponents used by time-based strategies.   <pre><code>bounds = ((0.2, 0.0), (5.0, min(self.opponent_utilities)))\n</code></pre></li> <li>We then just apply curve fitting while keeping the old estimate. We keep the old estimate to check whether there is enough change to warrent reevaluation of the rational outcome sets in our offering strategy. We ignore any errors keeping the old estimate in that case.</li> </ul> <p><pre><code>optimal_vals, _ = curve_fit(\n    lambda x, e, rv: aspiration_function(x, self.opponent_utilities[0], rv, e),\n    self.opponent_times, self.opponent_utilities, bounds=bounds\n)\n</code></pre>   Note that we just pass <code>self.opponent_utilities[0]</code> as the maximum for the concession curve because we know that this is the utility of the first offer from the opponent.</p> <ul> <li>Finally, we update the opponent reserved value with our new estimate keeping the latest value for later:   <pre><code>self._past_oppnent_rv = self.opponent_ufun.reserved_value\nself.opponent_ufun.reserved_value = optimal_vals[1]\n</code></pre></li> </ul> </li> </ol>"},{"location":"tutorials/tutorial_develop/#acceptance-strategy","title":"Acceptance Strategy","text":"<p>Our acceptance strategy is implemented in <code>is_acceptable()</code> and consists of the following steps:</p> <ol> <li>Reject if no offer is found (i.e. we are starting the negotiation now):    <pre><code>if offer is None:\n    return False\n</code></pre></li> <li>Find our current aspiration level which starts at 1.0 (inidicating we will only accept our best offer in the first step) ending at our reserved value (indicating that we are willing to accept any rational outcome by the end of the negotiation). Use the exponent we stored during construction.    <pre><code>asp = aspiration_function(state.relative_time, 1.0, self.ufun.reserved_value, self.e)\n</code></pre></li> <li>Accept the offer iff its utility is higher than the aspiration level:     <pre><code>return float(self.ufun(offer)) &gt;= asp\n</code></pre> Note that this acceptance strategy does not use the estimated opponent reserved value (or the opponent's ufun) in any way.</li> </ol>"},{"location":"tutorials/tutorial_develop/#bidding-strategy","title":"Bidding Strategy","text":"<p>Now that we have updated our estimate of the opponent reserved value and decided not to accept their offer, we have to generate our own offer which the job of the bidding strategy implementedin <code>generate_offer()</code>. This is done in three steps as well:</p> <ol> <li> <p>If the difference between the current and last estimate of the opponent reserved value is large enough, we create the rational outcome list.</p> <ul> <li>This test is implemented by: <pre><code>not self._rational or abs(self.opponent_ufun.reserved_value - self._past_oppnent_rv) &gt; 1e-3\n</code></pre></li> <li>We then create of all outcomes prepending them with our and opponent's utility values:   <pre><code>[ (my_util, opp_util, _)\n  for _ in self.nmi.outcome_space.enumerate_or_sample(\n      levels=10, max_cardinality=100_000\n  )\n  if (\n      (my_util := float(self.ufun(_))) &gt; self.ufun.reserved_value\n      and (opp_util := float(self.opponent_ufun(_))) &gt; self.opponent_ufun.reserved_value\n)]\n</code></pre></li> <li>Finally, we sort this list. Because each element is a tuple, the list will be sorted ascendingly by our utility with equal values sorted ascendingly by the opponent utility.   <pre><code>self._rational = sorted(...)\n</code></pre></li> </ul> </li> <li> <p>If there are no rational outcomes (e.g. our estimate of the opponent rv is very wrong), then just revert to offering our top offer    <pre><code>if not self._rational:\n     return self.ufun.best()\n</code></pre></p> </li> <li>If we have a rational set, we calculate an aspiration level that starts at 1 and ends at 0 (note that we do not need to end at the reserved value because all outcomes in <code>self._rational</code> are already no worse than disagreement. We then calculate the outcome that is at the current aspiration level from the end of the rational outcome list and offer it:    <pre><code>asp = aspiration_function(relative_time, 1.0, 0.0, self.e)\nmax_rational = len(self._rational) - 1\nindx = max(0, min(max_rational, int(asp * max_rational)))\noutcome = self._rational[indx][-1]\nreturn outcome\n</code></pre></li> </ol>"},{"location":"tutorials/tutorial_develop/#running-a-single-negotiation","title":"Running a single negotiation","text":"<p>What if we now want to see what happens in a single negotiation using our shiny new negotiator? We first need a scenario to define the outcome space and ufuns. We can then add negotiators to it and run it. Let's see an example:</p> <pre><code>import copy\nfrom negmas.sao import SAOMechanism\nfrom anl.anl2024.runner import mixed_scenarios\nfrom anl.anl2024.negotiators.builtins import Linear\n\n# create a scenario\ns = mixed_scenarios(1)[0]\n# copy ufuns and set rv to 0 in the copies\nufuns0 = [copy.deepcopy(u) for u in s.ufuns]\nfor u in ufuns0:\n    u.reserved_value = 0.0\n# create the negotiation mechanism\nsession = SAOMechanism(n_steps=1000, outcome_space=s.outcome_space)\n# add negotiators. Remember to pass the opponent_ufun in private_info\nsession.add(\n    SimpleRVFitter(name=\"SimpleRVFitter\",\n                   private_info=dict(opponent_ufun=ufuns0[1]))\n    , ufun=s.ufuns[0]\n)\nsession.add(Linear(name=\"Linear\"), ufun=s.ufuns[1])\n# run the negotiation and plot the results\nsession.run()\nsession.plot()\nplt.show()\n</code></pre> <p></p> <p>Notice how in the second half of the negotiation, the SimpleRVFitter is only offering outcomes that are rational for both negotiators (can you see that in the left-side plot? can you see it in the top right-side plot?). This means that the curve fitting approach is working OK here. The opponent is a time-based strategy in this case though.</p> <p>What happens if it was not? Let's try it against the builtin RVFitter for example</p> <pre><code>from anl.anl2024.negotiators import RVFitter\n# create the negotiation mechanism\nsession = SAOMechanism(n_steps=1000, outcome_space=s.outcome_space)\n# add negotiators. Remember to pass the opponent_ufun in private_info\nsession.add(\n    SimpleRVFitter(name=\"SimpleRVFitter\",\n                   private_info=dict(opponent_ufun=ufuns0[1]))\n    , ufun=s.ufuns[0]\n)\nsession.add(\n    RVFitter(name=\"RVFitter\",\n                   private_info=dict(opponent_ufun=ufuns0[0]))\n    , ufun=s.ufuns[1]\n)\n\n# run the negotiation and plot the results\nsession.run()\nsession.plot()\nplt.show()\n</code></pre> <p></p> <p>This time, our simple RV fitter could not really learn the opponent reserved value effectively. We can see that from the fact that it kept offering outcomes that are irrational for the opponent almost until the end of the negotiation.</p> <p>The builtin <code>RVFitter</code> seems better in this case. It took longer but it seems to only offer rational outcomes for its opponent (our SimpleRVFitter) after around 60% of the available negotiation time.</p>"},{"location":"tutorials/tutorial_develop/#other-examples","title":"Other Examples","text":"<p>The ANL package comes with some example negotiators. These are not designed to be stong but to showcase how to use some of the features provided by the platform.</p> <ul> <li>MiCRO A strong baseline behavioral negotiation strategy developed by de Jonge, Dave in \"An Analysis of the Linear Bilateral ANAC Domains Using the MiCRO Benchmark Strategy.\", ICJAI 2022. This strategy assumes no knowledge of the opponent utility function and is implemented from scratch to showcase the following:<ul> <li>Using <code>on_preferences_changed</code> for initialization.</li> <li>Using PresortingInverseUtilityFunction for inverting a utility function.</li> </ul> </li> <li>NashSeeker A naive strategy that simply sets the opponent reserved value to a fixed value and then uses helpers from NegMAS to find the Nash Bargaining Solution and use it for deciding what to offer. This showcases:<ul> <li>Using NegMAS helpers to calculate the pareto-frontier and the Nash Bargaining Solution</li> </ul> </li> <li>RVFitter A strategy very similar to the one we implemented earlier as <code>SimpleRVFitter</code>. Instead of trying to estiamte the opponent reserved value from the first step, this strategy waits until it collects few offers before attempting the etimation. This showcases:<ul> <li>Setting the opponent reserved value based on our best estimate.</li> <li>A simple way to use this estimate for our bidding strategy.</li> <li>Using logging. Logs can be saved using <code>self.nmi.log_info(dict(my_key=my_value))</code> and found under the logs folder.</li> </ul> </li> <li>Boulware, Conceder, Linear Time-based strategies that are implemented by just setting construction parameters of an existing NegMAS negotiator</li> <li>StochasticBoulware, StochasticConceder, StochasticLinear Stochastic versions of the three time-based strategies above implemented by just setting construction parameters of an existing NegMAS negotiator</li> <li>NaiveTitForTat A simple behavioral strategy implemented by just inheriting from an existing NegMAS negotiator.</li> </ul> <p>Download Notebook</p>"},{"location":"tutorials/tutorial_scenarios/","title":"Visualize scenarios","text":"<pre><code>%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# setup disply parameters\nfrom matplotlib import pylab as plt\nimport seaborn as sns\nfrom matplotlib.ticker import StrMethodFormatter\nfloat_formatter = StrMethodFormatter('{x:0.03f}')\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:95% !important; }&lt;/style&gt;\"))\nSMALL_SIZE = 14\nMEDIUM_SIZE = 16\nBIGGER_SIZE = 20\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\nplt.rc('figure', figsize=(18, 6)) # set figure size\nplt.rc(\"animation\", html=\"html5\")\nimport random\nrandom.seed(203)\nimport numpy as np\nnp.random.seed(345)\nfrom rich import print\n</code></pre>"},{"location":"tutorials/tutorial_scenarios/#visualizing-preference-profiles","title":"Visualizing preference profiles","text":"<p>In ANL 2024, different types of scenarios are used, see the detailed description of ANL 2024 for more information. In this notebook, we will provide some code to visualize the different types.</p> <pre><code>from negmas import SAOMechanism\nfrom anl.anl2024 import zerosum_pie_scenarios, monotonic_pie_scenarios, arbitrary_pie_scenarios\n\nscenario = monotonic_pie_scenarios(n_scenarios=2, n_outcomes=20)[0] #zerosum_pie_scenarios, arbitrary_pie_scenarios\nsession = SAOMechanism(issues=scenario.issues, n_steps=30)\nA_utility = scenario.ufuns[0]\nB_utility = scenario.ufuns[1]\n#visualize((session, A_utility, B_utility))\n</code></pre> <p>In the first line, we generate a <code>monotonic_pie_scenario</code> (or actually 2), which is one of three types of scenarios. The other two are  <code>zerosum_pie_scenarios</code> and <code>arbitrary_pie_scenarios</code>. After extracting the corresponding utility functions, we can visualize the session in the same way we did in the previous notebook (running a negotiation).</p> <p>For completeness, we provide the code for the <code>visualize</code> function below. By removing the <code>session.run</code> statement, it is possible to see the scenario plotted, without a negotiation taking place. This is useful to see the shape of the scenario, and to understand the preferences of the agents.</p> <pre><code>from anl.anl2024.negotiators import Conceder, Boulware\n\ndef visualize(negotiation_setup):\n    (session, A_utility, B_utility) = negotiation_setup\n\n    # create and add selller and buyer to the session\n    AgentA = Boulware(name=\"A\")\n    AgentB = Boulware(name=\"B\")\n    session.add(AgentA, ufun=A_utility)\n    session.add(AgentB, ufun=B_utility)\n\n    # run the negotiation and show the results\n    # session.run()\n\n    session.plot(ylimits=(0.0, 1.01), show_reserved=True)\n    plt.show()\n\nvisualize((session, A_utility, B_utility))\n</code></pre> <p></p> <p>Download Notebook</p>"}]}